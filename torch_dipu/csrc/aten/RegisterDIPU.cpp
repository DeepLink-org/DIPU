// required for old g++ to compile PRId64 macros, see
// https://github.com/pytorch/pytorch/issues/3571
// for context
#ifndef __STDC_FORMAT_MACROS
#define __STDC_FORMAT_MACROS
#endif

// an external backend might generate file within its code tree
// and check all the source files within the tree with clang-format.
// so, disable it since the backend might have a different config.
// clang-format off

// NOTE: This condition is true for all PyTorch internal libraries, it
//       just excludes external projects such as torch_xla which
//       re-use some of the PyTorch codegen machinery.
#if defined(CAFFE2_BUILD_MAIN_LIB)        || \
    defined(TORCH_CUDA_BUILD_MAIN_LIB)    || \
    defined(TORCH_HIP_BUILD_MAIN_LIB)     || \
    defined(TORCH_CUDA_CU_BUILD_MAIN_LIB) || \
    defined(TORCH_CUDA_CPP_BUILD_MAIN_LIB)
#define TORCH_ASSERT_ONLY_METHOD_OPERATORS
#endif

// @generated by torchgen/gen.py from RegisterDispatchKey.cpp

#include <c10/core/TensorImpl.h>
#include <c10/core/Allocator.h>
#include <ATen/DeviceGuard.h>
#include <ATen/NamedTensorUtils.h>
#include <ATen/Utils.h>
#include <ATen/WrapDimUtils.h>
#include <ATen/Dispatch.h>
#include <c10/util/ExclusivelyOwned.h>
#include <c10/util/Half.h>
#include <c10/core/UndefinedTensorImpl.h>
#include <c10/util/Optional.h>
#include <ATen/Tensor.h>
#include <ATen/native/Resize.h>

#include <cstddef>
#include <functional>
#include <memory>
#include <utility>

#include <ATen/Config.h>
#include <ATen/core/op_registration/adaption.h>
#include <torch/library.h>

#include "/mnt/petrelfs/zhongpu/dipu_poc/torch_dipu/csrc/aten/DIPUNativeFunctions.h"
#include "csrc_dipu/diopirt/diopi.h"
#include "csrc_dipu/aten/util/Log.h"
#include <ATen/NativeFunctions.h>
#include <ATen/Functions.h>
#include <ATen/Functions.h>

// See template file RegisterDispatchDefinitions.ini
namespace at {
// NB: TORCH_LIBRARY_IMPL must be in an anonymous namespace to avoid
// ambiguity with conflicting identifiers that may have been defined in
// at namespace already.
namespace {
void resize_out(const Tensor &out, IntArrayRef sizes, IntArrayRef strides, const TensorOptions &options) {
  TORCH_CHECK(options.dtype() == out.dtype(),
      "Expected out tensor to have dtype ", options.dtype(), ", but got ", out.dtype(), " instead");
  TORCH_CHECK(options.device() == out.device(),
      "Expected out tensor to have device ", options.device(), ", but got ", out.device(), " instead");
  const bool resized = at::native::resize_output(out, sizes);
  // Only restride if a resize occurred; otherwise we ignore the (advisory)
  // strides from the meta function and directly use the output tensor's
  // preexisting strides
  if (resized) {
    if (!strides.empty()) {
      TORCH_INTERNAL_ASSERT(!options.memory_format_opt().has_value());
      // TODO: avoid the redispatch here
      out.as_strided_(sizes, strides);
    } else if (options.memory_format_opt().has_value()) {
      out.unsafeGetTensorImpl()->empty_tensor_restride(*options.memory_format_opt());
    }
  }
}
void check_inplace(const Tensor &self, IntArrayRef sizes, const TensorOptions &options) {
  // These checks are needed on those operators that:
  //   1) don't use 'TensorIterator' (e.g. 'addmm' and 'baddbmm')
  //   2) have particular typing rules (e.g. 'cumsum' and 'cumprod')
  // For other operators (e.g. 'add'), 'TensorIterator' already checks
  // these things separately.
  TORCH_CHECK(options.dtype() == self.dtype(),
      "Bad in-place call: ",
      "input tensor dtype ", self.dtype(), " and output tensor dtype ", options.dtype(), " should match");
  TORCH_CHECK(options.device() == self.device(),
      "Bad in-place call: ",
      "input tensor device ", self.device(), " and output tensor device ", options.device(), " should match");
  TORCH_CHECK(sizes == self.sizes(),
      "Bad in-place call: ",
      "input tensor size ", self.sizes(), " and output tensor size ", sizes, " should match");
}
#define DIPU_LIBRARY_IMPL(opname, diopiFunc, wapperFunc) do {           \
    if (reinterpret_cast<void*>(diopiFunc) != nullptr) {                \
        m.impl(opname, TORCH_FN(wapperFunc));                           \
    }  else {                                                           \
        DIPU_LOG << #diopiFunc << " not implemented, do not register\n"; \
    }                                                                   \
} while (false);
namespace {
at::Tensor & wrapper_DIPU_out_add_out(const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return dipu::native::DIPUNativeFunctions::add_out(self, other, alpha, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_DIPU__conv2d(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, int64_t groups) {
    // No device check
  // DeviceGuard omitted
  return dipu::native::DIPUNativeFunctions::conv2d(input, weight, bias, stride, padding, dilation, groups);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_DIPU__copy_(at::Tensor & self, const at::Tensor & src, bool non_blocking) {
    // No device check
  // DeviceGuard omitted
  return dipu::native::DIPUNativeFunctions::copy_(self, src, non_blocking);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor,at::Tensor,at::Tensor> wrapper_DIPU__native_batch_norm(const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, bool training, double momentum, double eps) {
    // No device check
  // DeviceGuard omitted
  return dipu::native::DIPUNativeFunctions::native_batch_norm(input, weight, bias, running_mean, running_var, training, momentum, eps);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> wrapper_DIPU_out_native_batch_norm_out(const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, bool training, double momentum, double eps, at::Tensor & out, at::Tensor & save_mean, at::Tensor & save_invstd) {
    // No device check
  // DeviceGuard omitted
  return dipu::native::DIPUNativeFunctions::native_batch_norm_out(input, weight, bias, running_mean, running_var, training, momentum, eps, out, save_mean, save_invstd);
}
} // anonymous namespace
namespace {
::std::tuple<at::Tensor,at::Tensor,at::Tensor> wrapper_DIPU__native_batch_norm_backward(const at::Tensor & grad_out, const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, const c10::optional<at::Tensor> & save_mean, const c10::optional<at::Tensor> & save_invstd, bool train, double eps, ::std::array<bool,3> output_mask) {
    // No device check
  // DeviceGuard omitted
  return dipu::native::DIPUNativeFunctions::native_batch_norm_backward(grad_out, input, weight, running_mean, running_var, save_mean, save_invstd, train, eps, output_mask);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_DIPU_out_randperm_out(int64_t n, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return dipu::native::DIPUNativeFunctions::randperm_out(n, out);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_DIPU_generator_out_randperm_out(int64_t n, c10::optional<at::Generator> generator, at::Tensor & out) {
    // No device check
  // DeviceGuard omitted
  return dipu::native::DIPUNativeFunctions::randperm_out(n, generator, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_DIPU__relu(const at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return dipu::native::DIPUNativeFunctions::relu(self);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_DIPU__relu_(at::Tensor & self) {
    // No device check
  // DeviceGuard omitted
  return dipu::native::DIPUNativeFunctions::relu_(self);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_DIPU_from_random_(at::Tensor & self, int64_t from, c10::optional<int64_t> to, c10::optional<at::Generator> generator) {
    // No device check
  // DeviceGuard omitted
  return dipu::native::DIPUNativeFunctions::random_(self, from, to, generator);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_DIPU_to_random_(at::Tensor & self, int64_t to, c10::optional<at::Generator> generator) {
    // No device check
  // DeviceGuard omitted
  return dipu::native::DIPUNativeFunctions::random_(self, to, generator);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_DIPU__random_(at::Tensor & self, c10::optional<at::Generator> generator) {
    // No device check
  // DeviceGuard omitted
  return dipu::native::DIPUNativeFunctions::random_(self, generator);
}
} // anonymous namespace
TORCH_LIBRARY_IMPL(aten, DIPU, m) {
    DIPU_LIBRARY_IMPL("add.out", diopiAdd, wrapper_DIPU_out_add_out)
    DIPU_LIBRARY_IMPL("conv2d", diopiConvolution2d, wrapper_DIPU__conv2d)
    DIPU_LIBRARY_IMPL("copy_", diopiCopy, wrapper_DIPU__copy_)
    DIPU_LIBRARY_IMPL("native_batch_norm", diopiBatchNorm, wrapper_DIPU__native_batch_norm)
    DIPU_LIBRARY_IMPL("native_batch_norm.out", diopiBatchNorm, wrapper_DIPU_out_native_batch_norm_out)
    DIPU_LIBRARY_IMPL("native_batch_norm_backward", diopiBatchNormBackward, wrapper_DIPU__native_batch_norm_backward)
    DIPU_LIBRARY_IMPL("randperm.out", diopiRandperm, wrapper_DIPU_out_randperm_out)
    DIPU_LIBRARY_IMPL("randperm.generator_out", diopiRandperm, wrapper_DIPU_generator_out_randperm_out)
    DIPU_LIBRARY_IMPL("relu", diopiRelu, wrapper_DIPU__relu)
    DIPU_LIBRARY_IMPL("relu_", diopiReluInp, wrapper_DIPU__relu_)
    DIPU_LIBRARY_IMPL("random_.from", diopiRandomInp, wrapper_DIPU_from_random_)
    DIPU_LIBRARY_IMPL("random_.to", diopiRandomInp, wrapper_DIPU_to_random_)
    DIPU_LIBRARY_IMPL("random_", diopiRandomInp, wrapper_DIPU__random_)
};
} // anonymous namespace
namespace dipu {
} // namespace dipu
} // namespace at
