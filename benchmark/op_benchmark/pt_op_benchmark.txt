+ export PYTHONPATH=/nvme/share/share/platform/dep/DIOPI_pytorch/pytorch2.0_c263bd::/nvme/share/share/zhaoguochun/dipu/benchmark/op_benchmark:/nvme/share/share/platform/env/miniconda3.8/envs/pt2.0_diopi/pytorch2.0/benchmarks/operator_benchmark/
+ PYTHONPATH=/nvme/share/share/platform/dep/DIOPI_pytorch/pytorch2.0_c263bd::/nvme/share/share/zhaoguochun/dipu/benchmark/op_benchmark:/nvme/share/share/platform/env/miniconda3.8/envs/pt2.0_diopi/pytorch2.0/benchmarks/operator_benchmark/
+ options=' --num-runs 3 --iterations 100 --warmup-iterations 10 '
+ python -m benchmark_all_dipu_test --num-runs 3 --iterations 100 --warmup-iterations 10
not use dipu
# ----------------------------------------
# PyTorch/Caffe2 Operator Micro-benchmarks
# ----------------------------------------
# Tag : short

# Benchmarking PyTorch: add
# Mode: Eager
# Name: add_M1_N1_K1_cpu
# Input: M: 1, N: 1, K: 1, device: cpu
Run: 0, Forward Execution Time (us) : 6.674
Run: 1, Forward Execution Time (us) : 7.960
Run: 2, Forward Execution Time (us) : 7.899

# Benchmarking PyTorch: add
# Mode: Eager
# Name: add_M1_N1_K1_cuda
# Input: M: 1, N: 1, K: 1, device: cuda
Run: 0, Forward Execution Time (us) : 15.819
Run: 1, Forward Execution Time (us) : 15.910
Run: 2, Forward Execution Time (us) : 16.035

# Benchmarking PyTorch: add
# Mode: Eager
# Name: add_M64_N64_K64_cpu
# Input: M: 64, N: 64, K: 64, device: cpu
Run: 0, Forward Execution Time (us) : 132.676
Run: 1, Forward Execution Time (us) : 132.485
Run: 2, Forward Execution Time (us) : 132.376

# Benchmarking PyTorch: add
# Mode: Eager
# Name: add_M64_N64_K64_cuda
# Input: M: 64, N: 64, K: 64, device: cuda
Run: 0, Forward Execution Time (us) : 13.367
Run: 1, Forward Execution Time (us) : 15.776
Run: 2, Forward Execution Time (us) : 15.698

# Benchmarking PyTorch: add
# Mode: Eager
# Name: add_M64_N64_K128_cpu
# Input: M: 64, N: 64, K: 128, device: cpu
Run: 0, Forward Execution Time (us) : 255.039
Run: 1, Forward Execution Time (us) : 254.526
Run: 2, Forward Execution Time (us) : 254.664

# Benchmarking PyTorch: add
# Mode: Eager
# Name: add_M64_N64_K128_cuda
# Input: M: 64, N: 64, K: 128, device: cuda
Run: 0, Forward Execution Time (us) : 15.842
Run: 1, Forward Execution Time (us) : 16.101
Run: 2, Forward Execution Time (us) : 16.015

# Benchmarking PyTorch: add
# Mode: Eager
# Name: add_M1_N1_K1_cpu_bwdall
# Input: M: 1, N: 1, K: 1, device: cpu
Run: 0, Backward Execution Time (us) : 78.736
Run: 1, Backward Execution Time (us) : 93.427
Run: 2, Backward Execution Time (us) : 93.272

# Benchmarking PyTorch: add
# Mode: Eager
# Name: add_M1_N1_K1_cpu_bwd1
# Input: M: 1, N: 1, K: 1, device: cpu
Run: 0, Backward Execution Time (us) : 92.823
Run: 1, Backward Execution Time (us) : 93.639
Run: 2, Backward Execution Time (us) : 93.145

# Benchmarking PyTorch: add
# Mode: Eager
# Name: add_M1_N1_K1_cpu_bwd2
# Input: M: 1, N: 1, K: 1, device: cpu
Run: 0, Backward Execution Time (us) : 92.799
Run: 1, Backward Execution Time (us) : 93.010
Run: 2, Backward Execution Time (us) : 93.161

# Benchmarking PyTorch: add
# Mode: Eager
# Name: add_M1_N1_K1_cuda_bwdall
# Input: M: 1, N: 1, K: 1, device: cuda
Run: 0, Backward Execution Time (us) : 132.647
Run: 1, Backward Execution Time (us) : 130.079
Run: 2, Backward Execution Time (us) : 129.695

# Benchmarking PyTorch: add
# Mode: Eager
# Name: add_M1_N1_K1_cuda_bwd1
# Input: M: 1, N: 1, K: 1, device: cuda
Run: 0, Backward Execution Time (us) : 129.211
Run: 1, Backward Execution Time (us) : 132.443
Run: 2, Backward Execution Time (us) : 129.677

# Benchmarking PyTorch: add
# Mode: Eager
# Name: add_M1_N1_K1_cuda_bwd2
# Input: M: 1, N: 1, K: 1, device: cuda
Run: 0, Backward Execution Time (us) : 129.829
Run: 1, Backward Execution Time (us) : 132.496
Run: 2, Backward Execution Time (us) : 129.216

# Benchmarking PyTorch: add
# Mode: Eager
# Name: add_M64_N64_K64_cpu_bwdall
# Input: M: 64, N: 64, K: 64, device: cpu
Run: 0, Backward Execution Time (us) : 545.729
Run: 1, Backward Execution Time (us) : 550.139
Run: 2, Backward Execution Time (us) : 540.865

# Benchmarking PyTorch: add
# Mode: Eager
# Name: add_M64_N64_K64_cpu_bwd1
# Input: M: 64, N: 64, K: 64, device: cpu
Run: 0, Backward Execution Time (us) : 539.702
Run: 1, Backward Execution Time (us) : 542.304
Run: 2, Backward Execution Time (us) : 539.067

# Benchmarking PyTorch: add
# Mode: Eager
# Name: add_M64_N64_K64_cpu_bwd2
# Input: M: 64, N: 64, K: 64, device: cpu
Run: 0, Backward Execution Time (us) : 545.066
Run: 1, Backward Execution Time (us) : 542.589
Run: 2, Backward Execution Time (us) : 542.412

# Benchmarking PyTorch: add
# Mode: Eager
# Name: add_M64_N64_K64_cuda_bwdall
# Input: M: 64, N: 64, K: 64, device: cuda
Run: 0, Backward Execution Time (us) : 130.419
Run: 1, Backward Execution Time (us) : 129.968
Run: 2, Backward Execution Time (us) : 129.930

# Benchmarking PyTorch: add
# Mode: Eager
# Name: add_M64_N64_K64_cuda_bwd1
# Input: M: 64, N: 64, K: 64, device: cuda
Run: 0, Backward Execution Time (us) : 129.394
Run: 1, Backward Execution Time (us) : 129.905
Run: 2, Backward Execution Time (us) : 129.755

# Benchmarking PyTorch: add
# Mode: Eager
# Name: add_M64_N64_K64_cuda_bwd2
# Input: M: 64, N: 64, K: 64, device: cuda
Run: 0, Backward Execution Time (us) : 128.921
Run: 1, Backward Execution Time (us) : 130.497
Run: 2, Backward Execution Time (us) : 128.603

# Benchmarking PyTorch: add
# Mode: Eager
# Name: add_M64_N64_K128_cpu_bwdall
# Input: M: 64, N: 64, K: 128, device: cpu
Run: 0, Backward Execution Time (us) : 869.023
Run: 1, Backward Execution Time (us) : 961.699
Run: 2, Backward Execution Time (us) : 962.512

# Benchmarking PyTorch: add
# Mode: Eager
# Name: add_M64_N64_K128_cpu_bwd1
# Input: M: 64, N: 64, K: 128, device: cpu
Run: 0, Backward Execution Time (us) : 916.839
Run: 1, Backward Execution Time (us) : 966.656
Run: 2, Backward Execution Time (us) : 963.492

# Benchmarking PyTorch: add
# Mode: Eager
# Name: add_M64_N64_K128_cpu_bwd2
# Input: M: 64, N: 64, K: 128, device: cpu
Run: 0, Backward Execution Time (us) : 964.249
Run: 1, Backward Execution Time (us) : 971.210
Run: 2, Backward Execution Time (us) : 968.970

# Benchmarking PyTorch: add
# Mode: Eager
# Name: add_M64_N64_K128_cuda_bwdall
# Input: M: 64, N: 64, K: 128, device: cuda
Run: 0, Backward Execution Time (us) : 129.218
Run: 1, Backward Execution Time (us) : 130.080
Run: 2, Backward Execution Time (us) : 128.905

# Benchmarking PyTorch: add
# Mode: Eager
# Name: add_M64_N64_K128_cuda_bwd1
# Input: M: 64, N: 64, K: 128, device: cuda
Run: 0, Backward Execution Time (us) : 129.123
Run: 1, Backward Execution Time (us) : 129.727
Run: 2, Backward Execution Time (us) : 128.110

# Benchmarking PyTorch: add
# Mode: Eager
# Name: add_M64_N64_K128_cuda_bwd2
# Input: M: 64, N: 64, K: 128, device: cuda
Run: 0, Backward Execution Time (us) : 128.794
Run: 1, Backward Execution Time (us) : 130.073
Run: 2, Backward Execution Time (us) : 128.856

# Benchmarking PyTorch: addmm
# Mode: Eager
# Name: addmm_M1_N1_K1_cpu
# Input: M: 1, N: 1, K: 1, device: cpu
Run: 0, Forward Execution Time (us) : 8.614
Run: 1, Forward Execution Time (us) : 10.215
Run: 2, Forward Execution Time (us) : 10.183

# Benchmarking PyTorch: addmm
# Mode: Eager
# Name: addmm_M1_N1_K1_cuda
# Input: M: 1, N: 1, K: 1, device: cuda
Run: 0, Forward Execution Time (us) : 23.624
Run: 1, Forward Execution Time (us) : 27.721
Run: 2, Forward Execution Time (us) : 27.516

# Benchmarking PyTorch: addmm
# Mode: Eager
# Name: addmm_M64_N64_K64_cpu
# Input: M: 64, N: 64, K: 64, device: cpu
Run: 0, Forward Execution Time (us) : 22.863
Run: 1, Forward Execution Time (us) : 24.519
Run: 2, Forward Execution Time (us) : 24.945

# Benchmarking PyTorch: addmm
# Mode: Eager
# Name: addmm_M64_N64_K64_cuda
# Input: M: 64, N: 64, K: 64, device: cuda
Run: 0, Forward Execution Time (us) : 28.065
Run: 1, Forward Execution Time (us) : 28.457
Run: 2, Forward Execution Time (us) : 28.292

# Benchmarking PyTorch: addmm
# Mode: Eager
# Name: addmm_M64_N64_K128_cpu
# Input: M: 64, N: 64, K: 128, device: cpu
Run: 0, Forward Execution Time (us) : 31.950
Run: 1, Forward Execution Time (us) : 33.979
Run: 2, Forward Execution Time (us) : 32.021

# Benchmarking PyTorch: addmm
# Mode: Eager
# Name: addmm_M64_N64_K128_cuda
# Input: M: 64, N: 64, K: 128, device: cuda
Run: 0, Forward Execution Time (us) : 27.997
Run: 1, Forward Execution Time (us) : 28.375
Run: 2, Forward Execution Time (us) : 28.314

# Benchmarking PyTorch: addmm
# Mode: Eager
# Name: addmm_M1_N1_K1_cpu_bwdall
# Input: M: 1, N: 1, K: 1, device: cpu
Run: 0, Backward Execution Time (us) : 109.350
Run: 1, Backward Execution Time (us) : 129.710
Run: 2, Backward Execution Time (us) : 129.371

# Benchmarking PyTorch: addmm
# Mode: Eager
# Name: addmm_M1_N1_K1_cpu_bwd1
# Input: M: 1, N: 1, K: 1, device: cpu
Run: 0, Backward Execution Time (us) : 129.242
Run: 1, Backward Execution Time (us) : 129.672
Run: 2, Backward Execution Time (us) : 129.514

# Benchmarking PyTorch: addmm
# Mode: Eager
# Name: addmm_M1_N1_K1_cpu_bwd2
# Input: M: 1, N: 1, K: 1, device: cpu
Run: 0, Backward Execution Time (us) : 129.525
Run: 1, Backward Execution Time (us) : 129.781
Run: 2, Backward Execution Time (us) : 129.877

# Benchmarking PyTorch: addmm
# Mode: Eager
# Name: addmm_M1_N1_K1_cpu_bwd3
# Input: M: 1, N: 1, K: 1, device: cpu
Run: 0, Backward Execution Time (us) : 130.424
Run: 1, Backward Execution Time (us) : 129.316
Run: 2, Backward Execution Time (us) : 129.773

# Benchmarking PyTorch: addmm
# Mode: Eager
# Name: addmm_M1_N1_K1_cuda_bwdall
# Input: M: 1, N: 1, K: 1, device: cuda
Run: 0, Backward Execution Time (us) : 179.042
Run: 1, Backward Execution Time (us) : 184.164
Run: 2, Backward Execution Time (us) : 185.147

# Benchmarking PyTorch: addmm
# Mode: Eager
# Name: addmm_M1_N1_K1_cuda_bwd1
# Input: M: 1, N: 1, K: 1, device: cuda
Run: 0, Backward Execution Time (us) : 185.329
Run: 1, Backward Execution Time (us) : 187.302
Run: 2, Backward Execution Time (us) : 183.596

# Benchmarking PyTorch: addmm
# Mode: Eager
# Name: addmm_M1_N1_K1_cuda_bwd2
# Input: M: 1, N: 1, K: 1, device: cuda
Run: 0, Backward Execution Time (us) : 183.792
Run: 1, Backward Execution Time (us) : 186.077
Run: 2, Backward Execution Time (us) : 184.468

# Benchmarking PyTorch: addmm
# Mode: Eager
# Name: addmm_M1_N1_K1_cuda_bwd3
# Input: M: 1, N: 1, K: 1, device: cuda
Run: 0, Backward Execution Time (us) : 182.442
Run: 1, Backward Execution Time (us) : 183.485
Run: 2, Backward Execution Time (us) : 182.568

# Benchmarking PyTorch: addmm
# Mode: Eager
# Name: addmm_M64_N64_K64_cpu_bwdall
# Input: M: 64, N: 64, K: 64, device: cpu
Run: 0, Backward Execution Time (us) : 159.315
Run: 1, Backward Execution Time (us) : 160.386
Run: 2, Backward Execution Time (us) : 159.518

# Benchmarking PyTorch: addmm
# Mode: Eager
# Name: addmm_M64_N64_K64_cpu_bwd1
# Input: M: 64, N: 64, K: 64, device: cpu
Run: 0, Backward Execution Time (us) : 159.023
Run: 1, Backward Execution Time (us) : 159.692
Run: 2, Backward Execution Time (us) : 159.209

# Benchmarking PyTorch: addmm
# Mode: Eager
# Name: addmm_M64_N64_K64_cpu_bwd2
# Input: M: 64, N: 64, K: 64, device: cpu
Run: 0, Backward Execution Time (us) : 159.988
Run: 1, Backward Execution Time (us) : 159.859
Run: 2, Backward Execution Time (us) : 159.830

# Benchmarking PyTorch: addmm
# Mode: Eager
# Name: addmm_M64_N64_K64_cpu_bwd3
# Input: M: 64, N: 64, K: 64, device: cpu
Run: 0, Backward Execution Time (us) : 159.223
Run: 1, Backward Execution Time (us) : 159.494
Run: 2, Backward Execution Time (us) : 159.375

# Benchmarking PyTorch: addmm
# Mode: Eager
# Name: addmm_M64_N64_K64_cuda_bwdall
# Input: M: 64, N: 64, K: 64, device: cuda
Run: 0, Backward Execution Time (us) : 181.730
Run: 1, Backward Execution Time (us) : 180.189
Run: 2, Backward Execution Time (us) : 181.330

# Benchmarking PyTorch: addmm
# Mode: Eager
# Name: addmm_M64_N64_K64_cuda_bwd1
# Input: M: 64, N: 64, K: 64, device: cuda
Run: 0, Backward Execution Time (us) : 181.634
Run: 1, Backward Execution Time (us) : 180.993
Run: 2, Backward Execution Time (us) : 181.064

# Benchmarking PyTorch: addmm
# Mode: Eager
# Name: addmm_M64_N64_K64_cuda_bwd2
# Input: M: 64, N: 64, K: 64, device: cuda
Run: 0, Backward Execution Time (us) : 181.823
Run: 1, Backward Execution Time (us) : 180.334
Run: 2, Backward Execution Time (us) : 181.420

# Benchmarking PyTorch: addmm
# Mode: Eager
# Name: addmm_M64_N64_K64_cuda_bwd3
# Input: M: 64, N: 64, K: 64, device: cuda
Run: 0, Backward Execution Time (us) : 181.120
Run: 1, Backward Execution Time (us) : 180.069
Run: 2, Backward Execution Time (us) : 180.292

# Benchmarking PyTorch: addmm
# Mode: Eager
# Name: addmm_M64_N64_K128_cpu_bwdall
# Input: M: 64, N: 64, K: 128, device: cpu
Run: 0, Backward Execution Time (us) : 187.800
Run: 1, Backward Execution Time (us) : 188.565
Run: 2, Backward Execution Time (us) : 187.647

# Benchmarking PyTorch: addmm
# Mode: Eager
# Name: addmm_M64_N64_K128_cpu_bwd1
# Input: M: 64, N: 64, K: 128, device: cpu
Run: 0, Backward Execution Time (us) : 188.223
Run: 1, Backward Execution Time (us) : 188.992
Run: 2, Backward Execution Time (us) : 188.061

# Benchmarking PyTorch: addmm
# Mode: Eager
# Name: addmm_M64_N64_K128_cpu_bwd2
# Input: M: 64, N: 64, K: 128, device: cpu
Run: 0, Backward Execution Time (us) : 187.957
Run: 1, Backward Execution Time (us) : 188.105
Run: 2, Backward Execution Time (us) : 186.804

# Benchmarking PyTorch: addmm
# Mode: Eager
# Name: addmm_M64_N64_K128_cpu_bwd3
# Input: M: 64, N: 64, K: 128, device: cpu
Run: 0, Backward Execution Time (us) : 187.643
Run: 1, Backward Execution Time (us) : 189.219
Run: 2, Backward Execution Time (us) : 188.602

# Benchmarking PyTorch: addmm
# Mode: Eager
# Name: addmm_M64_N64_K128_cuda_bwdall
# Input: M: 64, N: 64, K: 128, device: cuda
Run: 0, Backward Execution Time (us) : 181.452
Run: 1, Backward Execution Time (us) : 180.877
Run: 2, Backward Execution Time (us) : 183.468

# Benchmarking PyTorch: addmm
# Mode: Eager
# Name: addmm_M64_N64_K128_cuda_bwd1
# Input: M: 64, N: 64, K: 128, device: cuda
Run: 0, Backward Execution Time (us) : 180.406
Run: 1, Backward Execution Time (us) : 180.471
Run: 2, Backward Execution Time (us) : 180.088

# Benchmarking PyTorch: addmm
# Mode: Eager
# Name: addmm_M64_N64_K128_cuda_bwd2
# Input: M: 64, N: 64, K: 128, device: cuda
Run: 0, Backward Execution Time (us) : 180.711
Run: 1, Backward Execution Time (us) : 180.375
Run: 2, Backward Execution Time (us) : 181.568

# Benchmarking PyTorch: addmm
# Mode: Eager
# Name: addmm_M64_N64_K128_cuda_bwd3
# Input: M: 64, N: 64, K: 128, device: cuda
Run: 0, Backward Execution Time (us) : 180.577
Run: 1, Backward Execution Time (us) : 181.034
Run: 2, Backward Execution Time (us) : 179.791

# Benchmarking PyTorch: bmm (actual op=bmm
# Mode: Eager
# Name: bmm(actualop=bmm_B2_M8_N256_K16_cpu_opbmm
# Input: B: 2, M: 8, N: 256, K: 16, device: cpu, op: bmm
Run: 0, Forward Execution Time (us) : 30.252
Run: 1, Forward Execution Time (us) : 30.655
Run: 2, Forward Execution Time (us) : 30.484

# Benchmarking PyTorch: bmm (actual op=matmul
# Mode: Eager
# Name: bmm(actualop=matmul_B2_M8_N256_K16_cpu_opmatmul
# Input: B: 2, M: 8, N: 256, K: 16, device: cpu, op: matmul
Run: 0, Forward Execution Time (us) : 47.717
Run: 1, Forward Execution Time (us) : 47.886
Run: 2, Forward Execution Time (us) : 47.953

# Benchmarking PyTorch: bmm (actual op=bmm
# Mode: Eager
# Name: bmm(actualop=bmm_B2_M8_N256_K32_cpu_opbmm
# Input: B: 2, M: 8, N: 256, K: 32, device: cpu, op: bmm
Run: 0, Forward Execution Time (us) : 37.654
Run: 1, Forward Execution Time (us) : 37.748
Run: 2, Forward Execution Time (us) : 37.773

# Benchmarking PyTorch: bmm (actual op=matmul
# Mode: Eager
# Name: bmm(actualop=matmul_B2_M8_N256_K32_cpu_opmatmul
# Input: B: 2, M: 8, N: 256, K: 32, device: cpu, op: matmul
Run: 0, Forward Execution Time (us) : 54.756
Run: 1, Forward Execution Time (us) : 54.905
Run: 2, Forward Execution Time (us) : 54.968

# Benchmarking PyTorch: bmm (actual op=bmm
# Mode: Eager
# Name: bmm(actualop=bmm_B2_M8_N16_K16_cpu_opbmm
# Input: B: 2, M: 8, N: 16, K: 16, device: cpu, op: bmm
Run: 0, Forward Execution Time (us) : 21.773
Run: 1, Forward Execution Time (us) : 21.893
Run: 2, Forward Execution Time (us) : 21.901

# Benchmarking PyTorch: bmm (actual op=matmul
# Mode: Eager
# Name: bmm(actualop=matmul_B2_M8_N16_K16_cpu_opmatmul
# Input: B: 2, M: 8, N: 16, K: 16, device: cpu, op: matmul
Run: 0, Forward Execution Time (us) : 38.881
Run: 1, Forward Execution Time (us) : 38.916
Run: 2, Forward Execution Time (us) : 38.971

# Benchmarking PyTorch: bmm (actual op=bmm
# Mode: Eager
# Name: bmm(actualop=bmm_B2_M8_N16_K32_cpu_opbmm
# Input: B: 2, M: 8, N: 16, K: 32, device: cpu, op: bmm
Run: 0, Forward Execution Time (us) : 22.404
Run: 1, Forward Execution Time (us) : 22.404
Run: 2, Forward Execution Time (us) : 22.536

# Benchmarking PyTorch: bmm (actual op=matmul
# Mode: Eager
# Name: bmm(actualop=matmul_B2_M8_N16_K32_cpu_opmatmul
# Input: B: 2, M: 8, N: 16, K: 32, device: cpu, op: matmul
Run: 0, Forward Execution Time (us) : 39.465
Run: 1, Forward Execution Time (us) : 39.671
Run: 2, Forward Execution Time (us) : 39.678

# Benchmarking PyTorch: bmm (actual op=bmm
# Mode: Eager
# Name: bmm(actualop=bmm_B2_M256_N256_K16_cpu_opbmm
# Input: B: 2, M: 256, N: 256, K: 16, device: cpu, op: bmm
Run: 0, Forward Execution Time (us) : 146.233
Run: 1, Forward Execution Time (us) : 146.251
Run: 2, Forward Execution Time (us) : 146.719

# Benchmarking PyTorch: bmm (actual op=matmul
# Mode: Eager
# Name: bmm(actualop=matmul_B2_M256_N256_K16_cpu_opmatmul
# Input: B: 2, M: 256, N: 256, K: 16, device: cpu, op: matmul
Run: 0, Forward Execution Time (us) : 165.417
Run: 1, Forward Execution Time (us) : 165.311
Run: 2, Forward Execution Time (us) : 165.398

# Benchmarking PyTorch: bmm (actual op=bmm
# Mode: Eager
# Name: bmm(actualop=bmm_B2_M256_N256_K32_cpu_opbmm
# Input: B: 2, M: 256, N: 256, K: 32, device: cpu, op: bmm
Run: 0, Forward Execution Time (us) : 208.918
Run: 1, Forward Execution Time (us) : 208.799
Run: 2, Forward Execution Time (us) : 209.221

# Benchmarking PyTorch: bmm (actual op=matmul
# Mode: Eager
# Name: bmm(actualop=matmul_B2_M256_N256_K32_cpu_opmatmul
# Input: B: 2, M: 256, N: 256, K: 32, device: cpu, op: matmul
Run: 0, Forward Execution Time (us) : 228.278
Run: 1, Forward Execution Time (us) : 229.335
Run: 2, Forward Execution Time (us) : 228.696

# Benchmarking PyTorch: bmm (actual op=bmm
# Mode: Eager
# Name: bmm(actualop=bmm_B2_M256_N16_K16_cpu_opbmm
# Input: B: 2, M: 256, N: 16, K: 16, device: cpu, op: bmm
Run: 0, Forward Execution Time (us) : 35.498
Run: 1, Forward Execution Time (us) : 35.425
Run: 2, Forward Execution Time (us) : 35.680

# Benchmarking PyTorch: bmm (actual op=matmul
# Mode: Eager
# Name: bmm(actualop=matmul_B2_M256_N16_K16_cpu_opmatmul
# Input: B: 2, M: 256, N: 16, K: 16, device: cpu, op: matmul
Run: 0, Forward Execution Time (us) : 52.661
Run: 1, Forward Execution Time (us) : 52.677
Run: 2, Forward Execution Time (us) : 52.823

# Benchmarking PyTorch: bmm (actual op=bmm
# Mode: Eager
# Name: bmm(actualop=bmm_B2_M256_N16_K32_cpu_opbmm
# Input: B: 2, M: 256, N: 16, K: 32, device: cpu, op: bmm
Run: 0, Forward Execution Time (us) : 43.287
Run: 1, Forward Execution Time (us) : 43.320
Run: 2, Forward Execution Time (us) : 43.339

# Benchmarking PyTorch: bmm (actual op=matmul
# Mode: Eager
# Name: bmm(actualop=matmul_B2_M256_N16_K32_cpu_opmatmul
# Input: B: 2, M: 256, N: 16, K: 32, device: cpu, op: matmul
Run: 0, Forward Execution Time (us) : 60.617
Run: 1, Forward Execution Time (us) : 60.613
Run: 2, Forward Execution Time (us) : 60.679

# Benchmarking PyTorch: bmm (actual op=bmm
# Mode: Eager
# Name: bmm(actualop=bmm_B100_M8_N256_K16_cpu_opbmm
# Input: B: 100, M: 8, N: 256, K: 16, device: cpu, op: bmm
Run: 0, Forward Execution Time (us) : 1172.317
Run: 1, Forward Execution Time (us) : 1166.831
Run: 2, Forward Execution Time (us) : 1177.192

# Benchmarking PyTorch: bmm (actual op=matmul
# Mode: Eager
# Name: bmm(actualop=matmul_B100_M8_N256_K16_cpu_opmatmul
# Input: B: 100, M: 8, N: 256, K: 16, device: cpu, op: matmul
Run: 0, Forward Execution Time (us) : 1186.879
Run: 1, Forward Execution Time (us) : 1191.192
Run: 2, Forward Execution Time (us) : 1189.001

# Benchmarking PyTorch: bmm (actual op=bmm
# Mode: Eager
# Name: bmm(actualop=bmm_B100_M8_N256_K32_cpu_opbmm
# Input: B: 100, M: 8, N: 256, K: 32, device: cpu, op: bmm
Run: 0, Forward Execution Time (us) : 1559.975
Run: 1, Forward Execution Time (us) : 1557.166
Run: 2, Forward Execution Time (us) : 1559.194

# Benchmarking PyTorch: bmm (actual op=matmul
# Mode: Eager
# Name: bmm(actualop=matmul_B100_M8_N256_K32_cpu_opmatmul
# Input: B: 100, M: 8, N: 256, K: 32, device: cpu, op: matmul
Run: 0, Forward Execution Time (us) : 1579.954
Run: 1, Forward Execution Time (us) : 1581.215
Run: 2, Forward Execution Time (us) : 1580.829

# Benchmarking PyTorch: bmm (actual op=bmm
# Mode: Eager
# Name: bmm(actualop=bmm_B100_M8_N16_K16_cpu_opbmm
# Input: B: 100, M: 8, N: 16, K: 16, device: cpu, op: bmm
Run: 0, Forward Execution Time (us) : 696.484
Run: 1, Forward Execution Time (us) : 692.620
Run: 2, Forward Execution Time (us) : 701.173

# Benchmarking PyTorch: bmm (actual op=matmul
# Mode: Eager
# Name: bmm(actualop=matmul_B100_M8_N16_K16_cpu_opmatmul
# Input: B: 100, M: 8, N: 16, K: 16, device: cpu, op: matmul
Run: 0, Forward Execution Time (us) : 716.472
Run: 1, Forward Execution Time (us) : 718.625
Run: 2, Forward Execution Time (us) : 717.142

# Benchmarking PyTorch: bmm (actual op=bmm
# Mode: Eager
# Name: bmm(actualop=bmm_B100_M8_N16_K32_cpu_opbmm
# Input: B: 100, M: 8, N: 16, K: 32, device: cpu, op: bmm
Run: 0, Forward Execution Time (us) : 722.609
Run: 1, Forward Execution Time (us) : 720.813
Run: 2, Forward Execution Time (us) : 729.199

# Benchmarking PyTorch: bmm (actual op=matmul
# Mode: Eager
# Name: bmm(actualop=matmul_B100_M8_N16_K32_cpu_opmatmul
# Input: B: 100, M: 8, N: 16, K: 32, device: cpu, op: matmul
Run: 0, Forward Execution Time (us) : 744.396
Run: 1, Forward Execution Time (us) : 742.696
Run: 2, Forward Execution Time (us) : 743.846

# Benchmarking PyTorch: bmm (actual op=bmm
# Mode: Eager
# Name: bmm(actualop=bmm_B100_M256_N256_K16_cpu_opbmm
# Input: B: 100, M: 256, N: 256, K: 16, device: cpu, op: bmm
Run: 0, Forward Execution Time (us) : 7479.469
Run: 1, Forward Execution Time (us) : 7489.112
Run: 2, Forward Execution Time (us) : 7464.787

# Benchmarking PyTorch: bmm (actual op=matmul
# Mode: Eager
# Name: bmm(actualop=matmul_B100_M256_N256_K16_cpu_opmatmul
# Input: B: 100, M: 256, N: 256, K: 16, device: cpu, op: matmul
Run: 0, Forward Execution Time (us) : 7460.906
Run: 1, Forward Execution Time (us) : 7394.715
Run: 2, Forward Execution Time (us) : 7430.567

# Benchmarking PyTorch: bmm (actual op=bmm
# Mode: Eager
# Name: bmm(actualop=bmm_B100_M256_N256_K32_cpu_opbmm
# Input: B: 100, M: 256, N: 256, K: 32, device: cpu, op: bmm
Run: 0, Forward Execution Time (us) : 10650.223
Run: 1, Forward Execution Time (us) : 10650.663
Run: 2, Forward Execution Time (us) : 11341.892

# Benchmarking PyTorch: bmm (actual op=matmul
# Mode: Eager
# Name: bmm(actualop=matmul_B100_M256_N256_K32_cpu_opmatmul
# Input: B: 100, M: 256, N: 256, K: 32, device: cpu, op: matmul
Run: 0, Forward Execution Time (us) : 10662.136
Run: 1, Forward Execution Time (us) : 10636.968
Run: 2, Forward Execution Time (us) : 10648.044

# Benchmarking PyTorch: bmm (actual op=bmm
# Mode: Eager
# Name: bmm(actualop=bmm_B100_M256_N16_K16_cpu_opbmm
# Input: B: 100, M: 256, N: 16, K: 16, device: cpu, op: bmm
Run: 0, Forward Execution Time (us) : 1398.881
Run: 1, Forward Execution Time (us) : 1395.843
Run: 2, Forward Execution Time (us) : 1397.431

# Benchmarking PyTorch: bmm (actual op=matmul
# Mode: Eager
# Name: bmm(actualop=matmul_B100_M256_N16_K16_cpu_opmatmul
# Input: B: 100, M: 256, N: 16, K: 16, device: cpu, op: matmul
Run: 0, Forward Execution Time (us) : 1425.328
Run: 1, Forward Execution Time (us) : 1421.811
Run: 2, Forward Execution Time (us) : 1420.523

# Benchmarking PyTorch: bmm (actual op=bmm
# Mode: Eager
# Name: bmm(actualop=bmm_B100_M256_N16_K32_cpu_opbmm
# Input: B: 100, M: 256, N: 16, K: 32, device: cpu, op: bmm
Run: 0, Forward Execution Time (us) : 1799.531
Run: 1, Forward Execution Time (us) : 1798.511
Run: 2, Forward Execution Time (us) : 1799.854

# Benchmarking PyTorch: bmm (actual op=matmul
# Mode: Eager
# Name: bmm(actualop=matmul_B100_M256_N16_K32_cpu_opmatmul
# Input: B: 100, M: 256, N: 16, K: 32, device: cpu, op: matmul
Run: 0, Forward Execution Time (us) : 1832.926
Run: 1, Forward Execution Time (us) : 1833.373
Run: 2, Forward Execution Time (us) : 1927.361

# Benchmarking PyTorch: fill_
# Mode: Eager
# Name: fill__N1_cpu_dtypetorch.int32
# Input: N: 1, device: cpu, dtype: torch.int32
Run: 0, Forward Execution Time (us) : 3.995
Run: 1, Forward Execution Time (us) : 3.910
Run: 2, Forward Execution Time (us) : 3.934

# Benchmarking PyTorch: fill_
# Mode: Eager
# Name: fill__N1_cuda_dtypetorch.int32
# Input: N: 1, device: cuda, dtype: torch.int32
Run: 0, Forward Execution Time (us) : 11.175
Run: 1, Forward Execution Time (us) : 11.185
Run: 2, Forward Execution Time (us) : 11.265

# Benchmarking PyTorch: fill_
# Mode: Eager
# Name: fill__N1024_cpu_dtypetorch.int32
# Input: N: 1024, device: cpu, dtype: torch.int32
Run: 0, Forward Execution Time (us) : 5.117
Run: 1, Forward Execution Time (us) : 5.041
Run: 2, Forward Execution Time (us) : 5.030

# Benchmarking PyTorch: fill_
# Mode: Eager
# Name: fill__N1024_cuda_dtypetorch.int32
# Input: N: 1024, device: cuda, dtype: torch.int32
Run: 0, Forward Execution Time (us) : 11.172
Run: 1, Forward Execution Time (us) : 11.355
Run: 2, Forward Execution Time (us) : 11.243

# Benchmarking PyTorch: fill_
# Mode: Eager
# Name: fill__N2048_cpu_dtypetorch.int32
# Input: N: 2048, device: cpu, dtype: torch.int32
Run: 0, Forward Execution Time (us) : 5.137
Run: 1, Forward Execution Time (us) : 5.157
Run: 2, Forward Execution Time (us) : 5.178

# Benchmarking PyTorch: fill_
# Mode: Eager
# Name: fill__N2048_cuda_dtypetorch.int32
# Input: N: 2048, device: cuda, dtype: torch.int32
Run: 0, Forward Execution Time (us) : 11.176
Run: 1, Forward Execution Time (us) : 11.296
Run: 2, Forward Execution Time (us) : 11.267

# Benchmarking PyTorch: Conv2d
# Mode: Eager
# Name: Conv2d_IC256_OC256_kernel3_stride1_N1_H16_W16_G1_pad0_cpu
# Input: IC: 256, OC: 256, kernel: 3, stride: 1, N: 1, H: 16, W: 16, G: 1, pad: 0, device: cpu
Run: 0, Forward Execution Time (us) : 4463.353
Run: 1, Forward Execution Time (us) : 4462.958
Run: 2, Forward Execution Time (us) : 4461.265

# Benchmarking PyTorch: Conv2d
# Mode: Eager
# Name: Conv2d_IC256_OC256_kernel3_stride1_N1_H16_W16_G1_pad0_cuda
# Input: IC: 256, OC: 256, kernel: 3, stride: 1, N: 1, H: 16, W: 16, G: 1, pad: 0, device: cuda
Run: 0, Forward Execution Time (us) : 77.520
Run: 1, Forward Execution Time (us) : 77.948
Run: 2, Forward Execution Time (us) : 77.811

# Benchmarking PyTorch: ConvTranspose2d
# Mode: Eager
# Name: ConvTranspose2d_IC256_OC256_kernel3_stride1_N1_H16_W16_G1_pad0_cpu
# Input: IC: 256, OC: 256, kernel: 3, stride: 1, N: 1, H: 16, W: 16, G: 1, pad: 0, device: cpu
Run: 0, Forward Execution Time (us) : 5648.042
Run: 1, Forward Execution Time (us) : 5615.833
Run: 2, Forward Execution Time (us) : 5617.437

# Benchmarking PyTorch: ConvTranspose2d
# Mode: Eager
# Name: ConvTranspose2d_IC256_OC256_kernel3_stride1_N1_H16_W16_G1_pad0_cuda
# Input: IC: 256, OC: 256, kernel: 3, stride: 1, N: 1, H: 16, W: 16, G: 1, pad: 0, device: cuda
Run: 0, Forward Execution Time (us) : 69.631
Run: 1, Forward Execution Time (us) : 70.394
Run: 2, Forward Execution Time (us) : 70.570

# Benchmarking PyTorch: Conv2dPointwise
# Mode: Eager
# Name: Conv2dPointwise_IC256_OC256_stride1_N1_H16_W16_G1_pad0_cpu
# Input: IC: 256, OC: 256, stride: 1, N: 1, H: 16, W: 16, G: 1, pad: 0, device: cpu
Run: 0, Forward Execution Time (us) : 533.613
Run: 1, Forward Execution Time (us) : 536.431
Run: 2, Forward Execution Time (us) : 537.003

# Benchmarking PyTorch: Conv2dPointwise
# Mode: Eager
# Name: Conv2dPointwise_IC256_OC256_stride1_N1_H16_W16_G1_pad0_cuda
# Input: IC: 256, OC: 256, stride: 1, N: 1, H: 16, W: 16, G: 1, pad: 0, device: cuda
Run: 0, Forward Execution Time (us) : 53.314
Run: 1, Forward Execution Time (us) : 62.830
Run: 2, Forward Execution Time (us) : 62.842

+ python -m pt.add_test --num-runs 3 --iterations 100 --warmup-iterations 10
not use dipu
# ----------------------------------------
# PyTorch/Caffe2 Operator Micro-benchmarks
# ----------------------------------------
# Tag : short

# Benchmarking PyTorch: add
# Mode: Eager
# Name: add_M1_N1_K1_cpu
# Input: M: 1, N: 1, K: 1, device: cpu
Run: 0, Forward Execution Time (us) : 6.550
Run: 1, Forward Execution Time (us) : 7.843
Run: 2, Forward Execution Time (us) : 7.738

# Benchmarking PyTorch: add
# Mode: Eager
# Name: add_M1_N1_K1_cuda
# Input: M: 1, N: 1, K: 1, device: cuda
Run: 0, Forward Execution Time (us) : 15.810
Run: 1, Forward Execution Time (us) : 16.066
Run: 2, Forward Execution Time (us) : 16.024

# Benchmarking PyTorch: add
# Mode: Eager
# Name: add_M64_N64_K64_cpu
# Input: M: 64, N: 64, K: 64, device: cpu
Run: 0, Forward Execution Time (us) : 132.828
Run: 1, Forward Execution Time (us) : 132.152
Run: 2, Forward Execution Time (us) : 132.205

# Benchmarking PyTorch: add
# Mode: Eager
# Name: add_M64_N64_K64_cuda
# Input: M: 64, N: 64, K: 64, device: cuda
Run: 0, Forward Execution Time (us) : 15.743
Run: 1, Forward Execution Time (us) : 15.934
Run: 2, Forward Execution Time (us) : 15.776

# Benchmarking PyTorch: add
# Mode: Eager
# Name: add_M64_N64_K128_cpu
# Input: M: 64, N: 64, K: 128, device: cpu
Run: 0, Forward Execution Time (us) : 255.945
Run: 1, Forward Execution Time (us) : 295.540
Run: 2, Forward Execution Time (us) : 254.951

# Benchmarking PyTorch: add
# Mode: Eager
# Name: add_M64_N64_K128_cuda
# Input: M: 64, N: 64, K: 128, device: cuda
Run: 0, Forward Execution Time (us) : 15.790
Run: 1, Forward Execution Time (us) : 15.948
Run: 2, Forward Execution Time (us) : 16.008

# Benchmarking PyTorch: add
# Mode: Eager
# Name: add_M1_N1_K1_cpu_bwdall
# Input: M: 1, N: 1, K: 1, device: cpu
Run: 0, Backward Execution Time (us) : 78.621
Run: 1, Backward Execution Time (us) : 92.761
Run: 2, Backward Execution Time (us) : 92.560

# Benchmarking PyTorch: add
# Mode: Eager
# Name: add_M1_N1_K1_cpu_bwd1
# Input: M: 1, N: 1, K: 1, device: cpu
Run: 0, Backward Execution Time (us) : 92.396
Run: 1, Backward Execution Time (us) : 92.474
Run: 2, Backward Execution Time (us) : 92.772

# Benchmarking PyTorch: add
# Mode: Eager
# Name: add_M1_N1_K1_cpu_bwd2
# Input: M: 1, N: 1, K: 1, device: cpu
Run: 0, Backward Execution Time (us) : 92.269
Run: 1, Backward Execution Time (us) : 92.565
Run: 2, Backward Execution Time (us) : 92.660

# Benchmarking PyTorch: add
# Mode: Eager
# Name: add_M1_N1_K1_cuda_bwdall
# Input: M: 1, N: 1, K: 1, device: cuda
Run: 0, Backward Execution Time (us) : 127.787
Run: 1, Backward Execution Time (us) : 127.495
Run: 2, Backward Execution Time (us) : 128.049

# Benchmarking PyTorch: add
# Mode: Eager
# Name: add_M1_N1_K1_cuda_bwd1
# Input: M: 1, N: 1, K: 1, device: cuda
Run: 0, Backward Execution Time (us) : 127.116
Run: 1, Backward Execution Time (us) : 130.903
Run: 2, Backward Execution Time (us) : 127.154

# Benchmarking PyTorch: add
# Mode: Eager
# Name: add_M1_N1_K1_cuda_bwd2
# Input: M: 1, N: 1, K: 1, device: cuda
Run: 0, Backward Execution Time (us) : 127.895
Run: 1, Backward Execution Time (us) : 130.817
Run: 2, Backward Execution Time (us) : 128.354

# Benchmarking PyTorch: add
# Mode: Eager
# Name: add_M64_N64_K64_cpu_bwdall
# Input: M: 64, N: 64, K: 64, device: cpu
Run: 0, Backward Execution Time (us) : 488.312
Run: 1, Backward Execution Time (us) : 541.523
Run: 2, Backward Execution Time (us) : 537.024

# Benchmarking PyTorch: add
# Mode: Eager
# Name: add_M64_N64_K64_cpu_bwd1
# Input: M: 64, N: 64, K: 64, device: cpu
Run: 0, Backward Execution Time (us) : 541.710
Run: 1, Backward Execution Time (us) : 538.633
Run: 2, Backward Execution Time (us) : 537.653

# Benchmarking PyTorch: add
# Mode: Eager
# Name: add_M64_N64_K64_cpu_bwd2
# Input: M: 64, N: 64, K: 64, device: cpu
Run: 0, Backward Execution Time (us) : 537.941
Run: 1, Backward Execution Time (us) : 540.131
Run: 2, Backward Execution Time (us) : 540.943

# Benchmarking PyTorch: add
# Mode: Eager
# Name: add_M64_N64_K64_cuda_bwdall
# Input: M: 64, N: 64, K: 64, device: cuda
Run: 0, Backward Execution Time (us) : 127.984
Run: 1, Backward Execution Time (us) : 127.997
Run: 2, Backward Execution Time (us) : 126.764

# Benchmarking PyTorch: add
# Mode: Eager
# Name: add_M64_N64_K64_cuda_bwd1
# Input: M: 64, N: 64, K: 64, device: cuda
Run: 0, Backward Execution Time (us) : 126.780
Run: 1, Backward Execution Time (us) : 128.141
Run: 2, Backward Execution Time (us) : 127.533

# Benchmarking PyTorch: add
# Mode: Eager
# Name: add_M64_N64_K64_cuda_bwd2
# Input: M: 64, N: 64, K: 64, device: cuda
Run: 0, Backward Execution Time (us) : 126.238
Run: 1, Backward Execution Time (us) : 128.025
Run: 2, Backward Execution Time (us) : 127.065

# Benchmarking PyTorch: add
# Mode: Eager
# Name: add_M64_N64_K128_cpu_bwdall
# Input: M: 64, N: 64, K: 128, device: cpu
Run: 0, Backward Execution Time (us) : 859.299
Run: 1, Backward Execution Time (us) : 958.057
Run: 2, Backward Execution Time (us) : 958.149

# Benchmarking PyTorch: add
# Mode: Eager
# Name: add_M64_N64_K128_cpu_bwd1
# Input: M: 64, N: 64, K: 128, device: cpu
Run: 0, Backward Execution Time (us) : 939.498
Run: 1, Backward Execution Time (us) : 963.283
Run: 2, Backward Execution Time (us) : 958.292

# Benchmarking PyTorch: add
# Mode: Eager
# Name: add_M64_N64_K128_cpu_bwd2
# Input: M: 64, N: 64, K: 128, device: cpu
Run: 0, Backward Execution Time (us) : 964.963
Run: 1, Backward Execution Time (us) : 968.092
Run: 2, Backward Execution Time (us) : 966.207

# Benchmarking PyTorch: add
# Mode: Eager
# Name: add_M64_N64_K128_cuda_bwdall
# Input: M: 64, N: 64, K: 128, device: cuda
Run: 0, Backward Execution Time (us) : 127.341
Run: 1, Backward Execution Time (us) : 128.329
Run: 2, Backward Execution Time (us) : 127.344

# Benchmarking PyTorch: add
# Mode: Eager
# Name: add_M64_N64_K128_cuda_bwd1
# Input: M: 64, N: 64, K: 128, device: cuda
Run: 0, Backward Execution Time (us) : 127.513
Run: 1, Backward Execution Time (us) : 120.586
Run: 2, Backward Execution Time (us) : 114.361

# Benchmarking PyTorch: add
# Mode: Eager
# Name: add_M64_N64_K128_cuda_bwd2
# Input: M: 64, N: 64, K: 128, device: cuda
Run: 0, Backward Execution Time (us) : 113.450
Run: 1, Backward Execution Time (us) : 115.277
Run: 2, Backward Execution Time (us) : 113.446

# Benchmarking PyTorch: addmm
# Mode: Eager
# Name: addmm_M1_N1_K1_cpu
# Input: M: 1, N: 1, K: 1, device: cpu
Run: 0, Forward Execution Time (us) : 8.561
Run: 1, Forward Execution Time (us) : 10.184
Run: 2, Forward Execution Time (us) : 10.159

# Benchmarking PyTorch: addmm
# Mode: Eager
# Name: addmm_M1_N1_K1_cuda
# Input: M: 1, N: 1, K: 1, device: cuda
Run: 0, Forward Execution Time (us) : 23.462
Run: 1, Forward Execution Time (us) : 27.414
Run: 2, Forward Execution Time (us) : 27.296

# Benchmarking PyTorch: addmm
# Mode: Eager
# Name: addmm_M64_N64_K64_cpu
# Input: M: 64, N: 64, K: 64, device: cpu
Run: 0, Forward Execution Time (us) : 22.891
Run: 1, Forward Execution Time (us) : 23.100
Run: 2, Forward Execution Time (us) : 22.976

# Benchmarking PyTorch: addmm
# Mode: Eager
# Name: addmm_M64_N64_K64_cuda
# Input: M: 64, N: 64, K: 64, device: cuda
Run: 0, Forward Execution Time (us) : 27.896
Run: 1, Forward Execution Time (us) : 28.065
Run: 2, Forward Execution Time (us) : 28.074

# Benchmarking PyTorch: addmm
# Mode: Eager
# Name: addmm_M64_N64_K128_cpu
# Input: M: 64, N: 64, K: 128, device: cpu
Run: 0, Forward Execution Time (us) : 32.029
Run: 1, Forward Execution Time (us) : 32.219
Run: 2, Forward Execution Time (us) : 32.120

# Benchmarking PyTorch: addmm
# Mode: Eager
# Name: addmm_M64_N64_K128_cuda
# Input: M: 64, N: 64, K: 128, device: cuda
Run: 0, Forward Execution Time (us) : 27.770
Run: 1, Forward Execution Time (us) : 27.909
Run: 2, Forward Execution Time (us) : 27.878

# Benchmarking PyTorch: addmm
# Mode: Eager
# Name: addmm_M1_N1_K1_cpu_bwdall
# Input: M: 1, N: 1, K: 1, device: cpu
Run: 0, Backward Execution Time (us) : 109.082
Run: 1, Backward Execution Time (us) : 129.385
Run: 2, Backward Execution Time (us) : 129.416

# Benchmarking PyTorch: addmm
# Mode: Eager
# Name: addmm_M1_N1_K1_cpu_bwd1
# Input: M: 1, N: 1, K: 1, device: cpu
Run: 0, Backward Execution Time (us) : 129.152
Run: 1, Backward Execution Time (us) : 129.438
Run: 2, Backward Execution Time (us) : 128.944

# Benchmarking PyTorch: addmm
# Mode: Eager
# Name: addmm_M1_N1_K1_cpu_bwd2
# Input: M: 1, N: 1, K: 1, device: cpu
Run: 0, Backward Execution Time (us) : 129.151
Run: 1, Backward Execution Time (us) : 129.132
Run: 2, Backward Execution Time (us) : 130.871

# Benchmarking PyTorch: addmm
# Mode: Eager
# Name: addmm_M1_N1_K1_cpu_bwd3
# Input: M: 1, N: 1, K: 1, device: cpu
Run: 0, Backward Execution Time (us) : 129.166
Run: 1, Backward Execution Time (us) : 129.421
Run: 2, Backward Execution Time (us) : 129.385

# Benchmarking PyTorch: addmm
# Mode: Eager
# Name: addmm_M1_N1_K1_cuda_bwdall
# Input: M: 1, N: 1, K: 1, device: cuda
Run: 0, Backward Execution Time (us) : 160.647
Run: 1, Backward Execution Time (us) : 165.974
Run: 2, Backward Execution Time (us) : 166.007

# Benchmarking PyTorch: addmm
# Mode: Eager
# Name: addmm_M1_N1_K1_cuda_bwd1
# Input: M: 1, N: 1, K: 1, device: cuda
Run: 0, Backward Execution Time (us) : 165.245
Run: 1, Backward Execution Time (us) : 166.944
Run: 2, Backward Execution Time (us) : 165.646

# Benchmarking PyTorch: addmm
# Mode: Eager
# Name: addmm_M1_N1_K1_cuda_bwd2
# Input: M: 1, N: 1, K: 1, device: cuda
Run: 0, Backward Execution Time (us) : 165.781
Run: 1, Backward Execution Time (us) : 166.756
Run: 2, Backward Execution Time (us) : 166.120

# Benchmarking PyTorch: addmm
# Mode: Eager
# Name: addmm_M1_N1_K1_cuda_bwd3
# Input: M: 1, N: 1, K: 1, device: cuda
Run: 0, Backward Execution Time (us) : 164.973
Run: 1, Backward Execution Time (us) : 165.940
Run: 2, Backward Execution Time (us) : 165.345

# Benchmarking PyTorch: addmm
# Mode: Eager
# Name: addmm_M64_N64_K64_cpu_bwdall
# Input: M: 64, N: 64, K: 64, device: cpu
Run: 0, Backward Execution Time (us) : 158.504
Run: 1, Backward Execution Time (us) : 159.243
Run: 2, Backward Execution Time (us) : 158.680

# Benchmarking PyTorch: addmm
# Mode: Eager
# Name: addmm_M64_N64_K64_cpu_bwd1
# Input: M: 64, N: 64, K: 64, device: cpu
Run: 0, Backward Execution Time (us) : 158.984
Run: 1, Backward Execution Time (us) : 159.307
Run: 2, Backward Execution Time (us) : 159.189

# Benchmarking PyTorch: addmm
# Mode: Eager
# Name: addmm_M64_N64_K64_cpu_bwd2
# Input: M: 64, N: 64, K: 64, device: cpu
Run: 0, Backward Execution Time (us) : 158.701
Run: 1, Backward Execution Time (us) : 158.992
Run: 2, Backward Execution Time (us) : 158.824

# Benchmarking PyTorch: addmm
# Mode: Eager
# Name: addmm_M64_N64_K64_cpu_bwd3
# Input: M: 64, N: 64, K: 64, device: cpu
Run: 0, Backward Execution Time (us) : 159.174
Run: 1, Backward Execution Time (us) : 159.283
Run: 2, Backward Execution Time (us) : 159.070

# Benchmarking PyTorch: addmm
# Mode: Eager
# Name: addmm_M64_N64_K64_cuda_bwdall
# Input: M: 64, N: 64, K: 64, device: cuda
Run: 0, Backward Execution Time (us) : 162.936
Run: 1, Backward Execution Time (us) : 162.575
Run: 2, Backward Execution Time (us) : 162.763

# Benchmarking PyTorch: addmm
# Mode: Eager
# Name: addmm_M64_N64_K64_cuda_bwd1
# Input: M: 64, N: 64, K: 64, device: cuda
Run: 0, Backward Execution Time (us) : 161.483
Run: 1, Backward Execution Time (us) : 162.151
Run: 2, Backward Execution Time (us) : 161.787

# Benchmarking PyTorch: addmm
# Mode: Eager
# Name: addmm_M64_N64_K64_cuda_bwd2
# Input: M: 64, N: 64, K: 64, device: cuda
Run: 0, Backward Execution Time (us) : 163.051
Run: 1, Backward Execution Time (us) : 162.252
Run: 2, Backward Execution Time (us) : 162.942

# Benchmarking PyTorch: addmm
# Mode: Eager
# Name: addmm_M64_N64_K64_cuda_bwd3
# Input: M: 64, N: 64, K: 64, device: cuda
Run: 0, Backward Execution Time (us) : 161.487
Run: 1, Backward Execution Time (us) : 161.940
Run: 2, Backward Execution Time (us) : 161.850

# Benchmarking PyTorch: addmm
# Mode: Eager
# Name: addmm_M64_N64_K128_cpu_bwdall
# Input: M: 64, N: 64, K: 128, device: cpu
Run: 0, Backward Execution Time (us) : 186.729
Run: 1, Backward Execution Time (us) : 188.291
Run: 2, Backward Execution Time (us) : 186.555

# Benchmarking PyTorch: addmm
# Mode: Eager
# Name: addmm_M64_N64_K128_cpu_bwd1
# Input: M: 64, N: 64, K: 128, device: cpu
Run: 0, Backward Execution Time (us) : 187.934
Run: 1, Backward Execution Time (us) : 187.403
Run: 2, Backward Execution Time (us) : 187.523

# Benchmarking PyTorch: addmm
# Mode: Eager
# Name: addmm_M64_N64_K128_cpu_bwd2
# Input: M: 64, N: 64, K: 128, device: cpu
Run: 0, Backward Execution Time (us) : 186.243
Run: 1, Backward Execution Time (us) : 186.846
Run: 2, Backward Execution Time (us) : 186.524

# Benchmarking PyTorch: addmm
# Mode: Eager
# Name: addmm_M64_N64_K128_cpu_bwd3
# Input: M: 64, N: 64, K: 128, device: cpu
Run: 0, Backward Execution Time (us) : 186.486
Run: 1, Backward Execution Time (us) : 187.822
Run: 2, Backward Execution Time (us) : 186.936

# Benchmarking PyTorch: addmm
# Mode: Eager
# Name: addmm_M64_N64_K128_cuda_bwdall
# Input: M: 64, N: 64, K: 128, device: cuda
Run: 0, Backward Execution Time (us) : 163.687
Run: 1, Backward Execution Time (us) : 162.589
Run: 2, Backward Execution Time (us) : 163.409

# Benchmarking PyTorch: addmm
# Mode: Eager
# Name: addmm_M64_N64_K128_cuda_bwd1
# Input: M: 64, N: 64, K: 128, device: cuda
Run: 0, Backward Execution Time (us) : 162.833
Run: 1, Backward Execution Time (us) : 161.654
Run: 2, Backward Execution Time (us) : 163.414

# Benchmarking PyTorch: addmm
# Mode: Eager
# Name: addmm_M64_N64_K128_cuda_bwd2
# Input: M: 64, N: 64, K: 128, device: cuda
Run: 0, Backward Execution Time (us) : 164.206
Run: 1, Backward Execution Time (us) : 161.265
Run: 2, Backward Execution Time (us) : 164.127

# Benchmarking PyTorch: addmm
# Mode: Eager
# Name: addmm_M64_N64_K128_cuda_bwd3
# Input: M: 64, N: 64, K: 128, device: cuda
Run: 0, Backward Execution Time (us) : 162.922
Run: 1, Backward Execution Time (us) : 162.519
Run: 2, Backward Execution Time (us) : 163.375

+ python -m pt.layernorm_test --num-runs 3 --iterations 100 --warmup-iterations 10
not use dipu
# ----------------------------------------
# PyTorch/Caffe2 Operator Micro-benchmarks
# ----------------------------------------
# Tag : short

# Benchmarking PyTorch: LayerNormBenchmark
# Mode: Eager
# Name: LayerNormBenchmark_dims(1,8,16)
# Input: dims: (1, 8, 16)
Run: 0, Forward Execution Time (us) : 18.679
Run: 1, Forward Execution Time (us) : 19.515
Run: 2, Forward Execution Time (us) : 19.333

# Benchmarking PyTorch: LayerNormBenchmark
# Mode: Eager
# Name: LayerNormBenchmark_dims(8,8,16)
# Input: dims: (8, 8, 16)
Run: 0, Forward Execution Time (us) : 20.488
Run: 1, Forward Execution Time (us) : 20.538
Run: 2, Forward Execution Time (us) : 20.681

# Benchmarking PyTorch: LayerNormBenchmark
# Mode: Eager
# Name: LayerNormBenchmark_dims(32,8,16)
# Input: dims: (32, 8, 16)
Run: 0, Forward Execution Time (us) : 29.426
Run: 1, Forward Execution Time (us) : 29.526
Run: 2, Forward Execution Time (us) : 29.511

# Benchmarking PyTorch: LayerNormBenchmark
# Mode: Eager
# Name: LayerNormBenchmark_dims(64,128,56,56)
# Input: dims: (64, 128, 56, 56)
Run: 0, Forward Execution Time (us) : 98821.746
Run: 1, Forward Execution Time (us) : 98749.171
Run: 2, Forward Execution Time (us) : 98316.360

+ python -m pt.qcat_test --num-runs 3 --iterations 100 --warmup-iterations 10
not use dipu
# ----------------------------------------
# PyTorch/Caffe2 Operator Micro-benchmarks
# ----------------------------------------
# Tag : short

# Benchmarking PyTorch: qcat
# Mode: Eager
# Name: qcat_M256_N512_K1_L2_dim0_contigall_dtypetorch.quint8
# Input: M: 256, N: 512, K: 1, L: 2, dim: 0, contig: all, dtype: torch.quint8
Run: 0, Forward Execution Time (us) : 8294.126
Run: 1, Forward Execution Time (us) : 8358.772
Run: 2, Forward Execution Time (us) : 8302.177

# Benchmarking PyTorch: qcat
# Mode: Eager
# Name: qcat_M256_N512_K1_L2_dim0_contigall_dtypetorch.qint8
# Input: M: 256, N: 512, K: 1, L: 2, dim: 0, contig: all, dtype: torch.qint8
Run: 0, Forward Execution Time (us) : 8354.896
Run: 1, Forward Execution Time (us) : 8397.632
Run: 2, Forward Execution Time (us) : 8349.553

# Benchmarking PyTorch: qcat
# Mode: Eager
# Name: qcat_M256_N512_K1_L2_dim0_contigall_dtypetorch.qint32
# Input: M: 256, N: 512, K: 1, L: 2, dim: 0, contig: all, dtype: torch.qint32
Run: 0, Forward Execution Time (us) : 8404.075
Run: 1, Forward Execution Time (us) : 8459.769
Run: 2, Forward Execution Time (us) : 8402.107

# Benchmarking PyTorch: qcat
# Mode: Eager
# Name: qcat_M256_N512_K1_L2_dim0_contigone_dtypetorch.quint8
# Input: M: 256, N: 512, K: 1, L: 2, dim: 0, contig: one, dtype: torch.quint8
Run: 0, Forward Execution Time (us) : 8402.448
Run: 1, Forward Execution Time (us) : 8438.442
Run: 2, Forward Execution Time (us) : 8408.281

# Benchmarking PyTorch: qcat
# Mode: Eager
# Name: qcat_M256_N512_K1_L2_dim0_contigone_dtypetorch.qint8
# Input: M: 256, N: 512, K: 1, L: 2, dim: 0, contig: one, dtype: torch.qint8
Run: 0, Forward Execution Time (us) : 8464.740
Run: 1, Forward Execution Time (us) : 8464.007
Run: 2, Forward Execution Time (us) : 8465.696

# Benchmarking PyTorch: qcat
# Mode: Eager
# Name: qcat_M256_N512_K1_L2_dim0_contigone_dtypetorch.qint32
# Input: M: 256, N: 512, K: 1, L: 2, dim: 0, contig: one, dtype: torch.qint32
Run: 0, Forward Execution Time (us) : 9040.136
Run: 1, Forward Execution Time (us) : 9016.732
Run: 2, Forward Execution Time (us) : 9009.922

# Benchmarking PyTorch: qcat
# Mode: Eager
# Name: qcat_M256_N512_K1_L2_dim0_contignone_dtypetorch.quint8
# Input: M: 256, N: 512, K: 1, L: 2, dim: 0, contig: none, dtype: torch.quint8
Run: 0, Forward Execution Time (us) : 8513.612
Run: 1, Forward Execution Time (us) : 8511.502
Run: 2, Forward Execution Time (us) : 8511.280

# Benchmarking PyTorch: qcat
# Mode: Eager
# Name: qcat_M256_N512_K1_L2_dim0_contignone_dtypetorch.qint8
# Input: M: 256, N: 512, K: 1, L: 2, dim: 0, contig: none, dtype: torch.qint8
Run: 0, Forward Execution Time (us) : 8575.384
Run: 1, Forward Execution Time (us) : 8613.820
Run: 2, Forward Execution Time (us) : 8576.595

# Benchmarking PyTorch: qcat
# Mode: Eager
# Name: qcat_M256_N512_K1_L2_dim0_contignone_dtypetorch.qint32
# Input: M: 256, N: 512, K: 1, L: 2, dim: 0, contig: none, dtype: torch.qint32
Run: 0, Forward Execution Time (us) : 9137.285
Run: 1, Forward Execution Time (us) : 9145.989
Run: 2, Forward Execution Time (us) : 9140.358

# Benchmarking PyTorch: qcat
# Mode: Eager
# Name: qcat_M512_N512_K2_L1_dim1_contigall_dtypetorch.quint8
# Input: M: 512, N: 512, K: 2, L: 1, dim: 1, contig: all, dtype: torch.quint8
Run: 0, Forward Execution Time (us) : 33018.148
Run: 1, Forward Execution Time (us) : 33095.202
Run: 2, Forward Execution Time (us) : 33026.293

# Benchmarking PyTorch: qcat
# Mode: Eager
# Name: qcat_M512_N512_K2_L1_dim1_contigall_dtypetorch.qint8
# Input: M: 512, N: 512, K: 2, L: 1, dim: 1, contig: all, dtype: torch.qint8
Run: 0, Forward Execution Time (us) : 33266.144
Run: 1, Forward Execution Time (us) : 33262.785
Run: 2, Forward Execution Time (us) : 33265.642

# Benchmarking PyTorch: qcat
# Mode: Eager
# Name: qcat_M512_N512_K2_L1_dim1_contigall_dtypetorch.qint32
# Input: M: 512, N: 512, K: 2, L: 1, dim: 1, contig: all, dtype: torch.qint32
Run: 0, Forward Execution Time (us) : 35330.561
Run: 1, Forward Execution Time (us) : 35296.813
Run: 2, Forward Execution Time (us) : 35284.038

# Benchmarking PyTorch: qcat
# Mode: Eager
# Name: qcat_M512_N512_K2_L1_dim1_contigone_dtypetorch.quint8
# Input: M: 512, N: 512, K: 2, L: 1, dim: 1, contig: one, dtype: torch.quint8
Run: 0, Forward Execution Time (us) : 33728.108
Run: 1, Forward Execution Time (us) : 33730.969
Run: 2, Forward Execution Time (us) : 33735.896

# Benchmarking PyTorch: qcat
# Mode: Eager
# Name: qcat_M512_N512_K2_L1_dim1_contigone_dtypetorch.qint8
# Input: M: 512, N: 512, K: 2, L: 1, dim: 1, contig: one, dtype: torch.qint8
Run: 0, Forward Execution Time (us) : 33919.462
Run: 1, Forward Execution Time (us) : 31892.270
Run: 2, Forward Execution Time (us) : 30765.120

# Benchmarking PyTorch: qcat
# Mode: Eager
# Name: qcat_M512_N512_K2_L1_dim1_contigone_dtypetorch.qint32
# Input: M: 512, N: 512, K: 2, L: 1, dim: 1, contig: one, dtype: torch.qint32
Run: 0, Forward Execution Time (us) : 35777.195
Run: 1, Forward Execution Time (us) : 35909.354
Run: 2, Forward Execution Time (us) : 37061.226

# Benchmarking PyTorch: qcat
# Mode: Eager
# Name: qcat_M512_N512_K2_L1_dim1_contignone_dtypetorch.quint8
# Input: M: 512, N: 512, K: 2, L: 1, dim: 1, contig: none, dtype: torch.quint8
Run: 0, Forward Execution Time (us) : 34402.834
Run: 1, Forward Execution Time (us) : 34401.597
Run: 2, Forward Execution Time (us) : 34407.123

# Benchmarking PyTorch: qcat
# Mode: Eager
# Name: qcat_M512_N512_K2_L1_dim1_contignone_dtypetorch.qint8
# Input: M: 512, N: 512, K: 2, L: 1, dim: 1, contig: none, dtype: torch.qint8
Run: 0, Forward Execution Time (us) : 34592.902
Run: 1, Forward Execution Time (us) : 34575.354
Run: 2, Forward Execution Time (us) : 34575.022

# Benchmarking PyTorch: qcat
# Mode: Eager
# Name: qcat_M512_N512_K2_L1_dim1_contignone_dtypetorch.qint32
# Input: M: 512, N: 512, K: 2, L: 1, dim: 1, contig: none, dtype: torch.qint32
Run: 0, Forward Execution Time (us) : 39089.686
Run: 1, Forward Execution Time (us) : 39185.924
Run: 2, Forward Execution Time (us) : 39095.685

+ thon -m pt.qpool_test --num-runs 3 --iterations 100 --warmup-iterations 10
run_benchmark.sh: line 15: thon: command not found
+ python -m pt.ao_sparsifier_test --num-runs 3 --iterations 100 --warmup-iterations 10
not use dipu
# ----------------------------------------
# PyTorch/Caffe2 Operator Micro-benchmarks
# ----------------------------------------
# Tag : short

# Benchmarking PyTorch: weight_norm_sparsifier_step
# Mode: Eager
# Name: weight_norm_sparsifier_step_M(32,16)_SL0.3_SBS(4,1)_ZPB2
# Input: M: (32, 16), SL: 0.3, SBS: (4, 1), ZPB: 2
Run: 0, Forward Execution Time (us) : 410.939
Run: 1, Forward Execution Time (us) : 414.730
Run: 2, Forward Execution Time (us) : 412.420

# Benchmarking PyTorch: weight_norm_sparsifier_step
# Mode: Eager
# Name: weight_norm_sparsifier_step_M(32,16)_SL0.6_SBS(1,4)_ZPB4
# Input: M: (32, 16), SL: 0.6, SBS: (1, 4), ZPB: 4
Run: 0, Forward Execution Time (us) : 230.620
Run: 1, Forward Execution Time (us) : 230.790
Run: 2, Forward Execution Time (us) : 230.457

# Benchmarking PyTorch: weight_norm_sparsifier_step
# Mode: Eager
# Name: weight_norm_sparsifier_step_M(17,23)_SL0.9_SBS(1,1)_ZPB1
# Input: M: (17, 23), SL: 0.9, SBS: (1, 1), ZPB: 1
Run: 0, Forward Execution Time (us) : 197.297
Run: 1, Forward Execution Time (us) : 198.051
Run: 2, Forward Execution Time (us) : 198.453

+ python -m pt.embeddingbag_test --num-runs 3 --iterations 100 --warmup-iterations 10
not use dipu
# ----------------------------------------
# PyTorch/Caffe2 Operator Micro-benchmarks
# ----------------------------------------
# Tag : short

# Benchmarking PyTorch: embeddingbag
# Mode: Eager
# Name: embeddingbag_embeddingbags10_dim64_modesum_input_size8_offset0_sparseTrue_include_last_offsetTrue_cpu
# Input: embeddingbags: 10, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: True, include_last_offset: True, device: cpu
Run: 0, Forward Execution Time (us) : 39.690
Run: 1, Forward Execution Time (us) : 47.361
Run: 2, Forward Execution Time (us) : 47.198

# Benchmarking PyTorch: embeddingbag
# Mode: Eager
# Name: embeddingbag_embeddingbags10_dim64_modesum_input_size8_offset0_sparseTrue_include_last_offsetFalse_cpu
# Input: embeddingbags: 10, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: True, include_last_offset: False, device: cpu
Run: 0, Forward Execution Time (us) : 57.212
Run: 1, Forward Execution Time (us) : 57.517
Run: 2, Forward Execution Time (us) : 57.827

# Benchmarking PyTorch: embeddingbag
# Mode: Eager
# Name: embeddingbag_embeddingbags10_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetTrue_cpu
# Input: embeddingbags: 10, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: True, device: cpu
Run: 0, Forward Execution Time (us) : 46.847
Run: 1, Forward Execution Time (us) : 46.960
Run: 2, Forward Execution Time (us) : 46.962

# Benchmarking PyTorch: embeddingbag
# Mode: Eager
# Name: embeddingbag_embeddingbags10_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetFalse_cpu
# Input: embeddingbags: 10, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: False, device: cpu
Run: 0, Forward Execution Time (us) : 57.459
Run: 1, Forward Execution Time (us) : 57.584
Run: 2, Forward Execution Time (us) : 57.556

# Benchmarking PyTorch: embeddingbag
# Mode: Eager
# Name: embeddingbag_embeddingbags10_dim64_modesum_input_size16_offset0_sparseTrue_include_last_offsetTrue_cpu
# Input: embeddingbags: 10, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: True, include_last_offset: True, device: cpu
Run: 0, Forward Execution Time (us) : 46.669
Run: 1, Forward Execution Time (us) : 47.036
Run: 2, Forward Execution Time (us) : 46.983

# Benchmarking PyTorch: embeddingbag
# Mode: Eager
# Name: embeddingbag_embeddingbags10_dim64_modesum_input_size16_offset0_sparseTrue_include_last_offsetFalse_cpu
# Input: embeddingbags: 10, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: True, include_last_offset: False, device: cpu
Run: 0, Forward Execution Time (us) : 57.407
Run: 1, Forward Execution Time (us) : 57.654
Run: 2, Forward Execution Time (us) : 57.585

# Benchmarking PyTorch: embeddingbag
# Mode: Eager
# Name: embeddingbag_embeddingbags10_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetTrue_cpu
# Input: embeddingbags: 10, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: True, device: cpu
Run: 0, Forward Execution Time (us) : 46.868
Run: 1, Forward Execution Time (us) : 46.993
Run: 2, Forward Execution Time (us) : 46.973

# Benchmarking PyTorch: embeddingbag
# Mode: Eager
# Name: embeddingbag_embeddingbags10_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetFalse_cpu
# Input: embeddingbags: 10, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: False, device: cpu
Run: 0, Forward Execution Time (us) : 57.455
Run: 1, Forward Execution Time (us) : 57.492
Run: 2, Forward Execution Time (us) : 57.536

# Benchmarking PyTorch: embeddingbag
# Mode: Eager
# Name: embeddingbag_embeddingbags10_dim64_modesum_input_size64_offset0_sparseTrue_include_last_offsetTrue_cpu
# Input: embeddingbags: 10, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: True, include_last_offset: True, device: cpu
Run: 0, Forward Execution Time (us) : 47.172
Run: 1, Forward Execution Time (us) : 47.422
Run: 2, Forward Execution Time (us) : 47.324

# Benchmarking PyTorch: embeddingbag
# Mode: Eager
# Name: embeddingbag_embeddingbags10_dim64_modesum_input_size64_offset0_sparseTrue_include_last_offsetFalse_cpu
# Input: embeddingbags: 10, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: True, include_last_offset: False, device: cpu
Run: 0, Forward Execution Time (us) : 57.517
Run: 1, Forward Execution Time (us) : 57.795
Run: 2, Forward Execution Time (us) : 57.840

# Benchmarking PyTorch: embeddingbag
# Mode: Eager
# Name: embeddingbag_embeddingbags10_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetTrue_cpu
# Input: embeddingbags: 10, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: True, device: cpu
Run: 0, Forward Execution Time (us) : 47.099
Run: 1, Forward Execution Time (us) : 47.221
Run: 2, Forward Execution Time (us) : 47.169

# Benchmarking PyTorch: embeddingbag
# Mode: Eager
# Name: embeddingbag_embeddingbags10_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetFalse_cpu
# Input: embeddingbags: 10, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: False, device: cpu
Run: 0, Forward Execution Time (us) : 57.863
Run: 1, Forward Execution Time (us) : 58.001
Run: 2, Forward Execution Time (us) : 57.934

# Benchmarking PyTorch: embeddingbag
# Mode: Eager
# Name: embeddingbag_embeddingbags120_dim64_modesum_input_size8_offset0_sparseTrue_include_last_offsetTrue_cpu
# Input: embeddingbags: 120, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: True, include_last_offset: True, device: cpu
Run: 0, Forward Execution Time (us) : 46.742
Run: 1, Forward Execution Time (us) : 46.930
Run: 2, Forward Execution Time (us) : 46.908

# Benchmarking PyTorch: embeddingbag
# Mode: Eager
# Name: embeddingbag_embeddingbags120_dim64_modesum_input_size8_offset0_sparseTrue_include_last_offsetFalse_cpu
# Input: embeddingbags: 120, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: True, include_last_offset: False, device: cpu
Run: 0, Forward Execution Time (us) : 57.135
Run: 1, Forward Execution Time (us) : 57.571
Run: 2, Forward Execution Time (us) : 57.643

# Benchmarking PyTorch: embeddingbag
# Mode: Eager
# Name: embeddingbag_embeddingbags120_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetTrue_cpu
# Input: embeddingbags: 120, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: True, device: cpu
Run: 0, Forward Execution Time (us) : 46.613
Run: 1, Forward Execution Time (us) : 46.980
Run: 2, Forward Execution Time (us) : 46.836

# Benchmarking PyTorch: embeddingbag
# Mode: Eager
# Name: embeddingbag_embeddingbags120_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetFalse_cpu
# Input: embeddingbags: 120, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: False, device: cpu
Run: 0, Forward Execution Time (us) : 57.573
Run: 1, Forward Execution Time (us) : 57.753
Run: 2, Forward Execution Time (us) : 57.673

# Benchmarking PyTorch: embeddingbag
# Mode: Eager
# Name: embeddingbag_embeddingbags120_dim64_modesum_input_size16_offset0_sparseTrue_include_last_offsetTrue_cpu
# Input: embeddingbags: 120, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: True, include_last_offset: True, device: cpu
Run: 0, Forward Execution Time (us) : 46.893
Run: 1, Forward Execution Time (us) : 47.038
Run: 2, Forward Execution Time (us) : 46.985

# Benchmarking PyTorch: embeddingbag
# Mode: Eager
# Name: embeddingbag_embeddingbags120_dim64_modesum_input_size16_offset0_sparseTrue_include_last_offsetFalse_cpu
# Input: embeddingbags: 120, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: True, include_last_offset: False, device: cpu
Run: 0, Forward Execution Time (us) : 57.617
Run: 1, Forward Execution Time (us) : 57.820
Run: 2, Forward Execution Time (us) : 57.926

# Benchmarking PyTorch: embeddingbag
# Mode: Eager
# Name: embeddingbag_embeddingbags120_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetTrue_cpu
# Input: embeddingbags: 120, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: True, device: cpu
Run: 0, Forward Execution Time (us) : 46.806
Run: 1, Forward Execution Time (us) : 47.116
Run: 2, Forward Execution Time (us) : 46.987

# Benchmarking PyTorch: embeddingbag
# Mode: Eager
# Name: embeddingbag_embeddingbags120_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetFalse_cpu
# Input: embeddingbags: 120, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: False, device: cpu
Run: 0, Forward Execution Time (us) : 57.602
Run: 1, Forward Execution Time (us) : 57.891
Run: 2, Forward Execution Time (us) : 57.620

# Benchmarking PyTorch: embeddingbag
# Mode: Eager
# Name: embeddingbag_embeddingbags120_dim64_modesum_input_size64_offset0_sparseTrue_include_last_offsetTrue_cpu
# Input: embeddingbags: 120, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: True, include_last_offset: True, device: cpu
Run: 0, Forward Execution Time (us) : 47.069
Run: 1, Forward Execution Time (us) : 47.494
Run: 2, Forward Execution Time (us) : 47.450

# Benchmarking PyTorch: embeddingbag
# Mode: Eager
# Name: embeddingbag_embeddingbags120_dim64_modesum_input_size64_offset0_sparseTrue_include_last_offsetFalse_cpu
# Input: embeddingbags: 120, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: True, include_last_offset: False, device: cpu
Run: 0, Forward Execution Time (us) : 58.068
Run: 1, Forward Execution Time (us) : 58.285
Run: 2, Forward Execution Time (us) : 58.255

# Benchmarking PyTorch: embeddingbag
# Mode: Eager
# Name: embeddingbag_embeddingbags120_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetTrue_cpu
# Input: embeddingbags: 120, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: True, device: cpu
Run: 0, Forward Execution Time (us) : 47.050
Run: 1, Forward Execution Time (us) : 47.346
Run: 2, Forward Execution Time (us) : 47.317

# Benchmarking PyTorch: embeddingbag
# Mode: Eager
# Name: embeddingbag_embeddingbags120_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetFalse_cpu
# Input: embeddingbags: 120, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: False, device: cpu
Run: 0, Forward Execution Time (us) : 57.851
Run: 1, Forward Execution Time (us) : 58.058
Run: 2, Forward Execution Time (us) : 58.028

# Benchmarking PyTorch: embeddingbag
# Mode: Eager
# Name: embeddingbag_embeddingbags1000_dim64_modesum_input_size8_offset0_sparseTrue_include_last_offsetTrue_cpu
# Input: embeddingbags: 1000, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: True, include_last_offset: True, device: cpu
Run: 0, Forward Execution Time (us) : 46.653
Run: 1, Forward Execution Time (us) : 47.109
Run: 2, Forward Execution Time (us) : 47.189

# Benchmarking PyTorch: embeddingbag
# Mode: Eager
# Name: embeddingbag_embeddingbags1000_dim64_modesum_input_size8_offset0_sparseTrue_include_last_offsetFalse_cpu
# Input: embeddingbags: 1000, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: True, include_last_offset: False, device: cpu
Run: 0, Forward Execution Time (us) : 57.594
Run: 1, Forward Execution Time (us) : 58.064
Run: 2, Forward Execution Time (us) : 58.182

# Benchmarking PyTorch: embeddingbag
# Mode: Eager
# Name: embeddingbag_embeddingbags1000_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetTrue_cpu
# Input: embeddingbags: 1000, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: True, device: cpu
Run: 0, Forward Execution Time (us) : 47.056
Run: 1, Forward Execution Time (us) : 47.070
Run: 2, Forward Execution Time (us) : 46.907

# Benchmarking PyTorch: embeddingbag
# Mode: Eager
# Name: embeddingbag_embeddingbags1000_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetFalse_cpu
# Input: embeddingbags: 1000, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: False, device: cpu
Run: 0, Forward Execution Time (us) : 48.550
Run: 1, Forward Execution Time (us) : 57.487
Run: 2, Forward Execution Time (us) : 57.668

# Benchmarking PyTorch: embeddingbag
# Mode: Eager
# Name: embeddingbag_embeddingbags1000_dim64_modesum_input_size16_offset0_sparseTrue_include_last_offsetTrue_cpu
# Input: embeddingbags: 1000, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: True, include_last_offset: True, device: cpu
Run: 0, Forward Execution Time (us) : 39.679
Run: 1, Forward Execution Time (us) : 47.031
Run: 2, Forward Execution Time (us) : 47.014

# Benchmarking PyTorch: embeddingbag
# Mode: Eager
# Name: embeddingbag_embeddingbags1000_dim64_modesum_input_size16_offset0_sparseTrue_include_last_offsetFalse_cpu
# Input: embeddingbags: 1000, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: True, include_last_offset: False, device: cpu
Run: 0, Forward Execution Time (us) : 48.581
Run: 1, Forward Execution Time (us) : 57.794
Run: 2, Forward Execution Time (us) : 57.942

# Benchmarking PyTorch: embeddingbag
# Mode: Eager
# Name: embeddingbag_embeddingbags1000_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetTrue_cpu
# Input: embeddingbags: 1000, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: True, device: cpu
Run: 0, Forward Execution Time (us) : 47.074
Run: 1, Forward Execution Time (us) : 47.212
Run: 2, Forward Execution Time (us) : 47.185

# Benchmarking PyTorch: embeddingbag
# Mode: Eager
# Name: embeddingbag_embeddingbags1000_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetFalse_cpu
# Input: embeddingbags: 1000, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: False, device: cpu
Run: 0, Forward Execution Time (us) : 48.679
Run: 1, Forward Execution Time (us) : 57.646
Run: 2, Forward Execution Time (us) : 57.666

# Benchmarking PyTorch: embeddingbag
# Mode: Eager
# Name: embeddingbag_embeddingbags1000_dim64_modesum_input_size64_offset0_sparseTrue_include_last_offsetTrue_cpu
# Input: embeddingbags: 1000, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: True, include_last_offset: True, device: cpu
Run: 0, Forward Execution Time (us) : 40.134
Run: 1, Forward Execution Time (us) : 47.552
Run: 2, Forward Execution Time (us) : 47.617

# Benchmarking PyTorch: embeddingbag
# Mode: Eager
# Name: embeddingbag_embeddingbags1000_dim64_modesum_input_size64_offset0_sparseTrue_include_last_offsetFalse_cpu
# Input: embeddingbags: 1000, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: True, include_last_offset: False, device: cpu
Run: 0, Forward Execution Time (us) : 49.216
Run: 1, Forward Execution Time (us) : 58.570
Run: 2, Forward Execution Time (us) : 58.452

# Benchmarking PyTorch: embeddingbag
# Mode: Eager
# Name: embeddingbag_embeddingbags1000_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetTrue_cpu
# Input: embeddingbags: 1000, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: True, device: cpu
Run: 0, Forward Execution Time (us) : 39.963
Run: 1, Forward Execution Time (us) : 47.435
Run: 2, Forward Execution Time (us) : 47.525

# Benchmarking PyTorch: embeddingbag
# Mode: Eager
# Name: embeddingbag_embeddingbags1000_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetFalse_cpu
# Input: embeddingbags: 1000, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: False, device: cpu
Run: 0, Forward Execution Time (us) : 49.235
Run: 1, Forward Execution Time (us) : 58.355
Run: 2, Forward Execution Time (us) : 58.205

# Benchmarking PyTorch: embeddingbag
# Mode: Eager
# Name: embeddingbag_embeddingbags2300_dim64_modesum_input_size8_offset0_sparseTrue_include_last_offsetTrue_cpu
# Input: embeddingbags: 2300, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: True, include_last_offset: True, device: cpu
Run: 0, Forward Execution Time (us) : 46.842
Run: 1, Forward Execution Time (us) : 47.120
Run: 2, Forward Execution Time (us) : 47.064

# Benchmarking PyTorch: embeddingbag
# Mode: Eager
# Name: embeddingbag_embeddingbags2300_dim64_modesum_input_size8_offset0_sparseTrue_include_last_offsetFalse_cpu
# Input: embeddingbags: 2300, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: True, include_last_offset: False, device: cpu
Run: 0, Forward Execution Time (us) : 57.288
Run: 1, Forward Execution Time (us) : 57.582
Run: 2, Forward Execution Time (us) : 57.704

# Benchmarking PyTorch: embeddingbag
# Mode: Eager
# Name: embeddingbag_embeddingbags2300_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetTrue_cpu
# Input: embeddingbags: 2300, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: True, device: cpu
Run: 0, Forward Execution Time (us) : 39.489
Run: 1, Forward Execution Time (us) : 47.068
Run: 2, Forward Execution Time (us) : 46.929

# Benchmarking PyTorch: embeddingbag
# Mode: Eager
# Name: embeddingbag_embeddingbags2300_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetFalse_cpu
# Input: embeddingbags: 2300, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: False, device: cpu
Run: 0, Forward Execution Time (us) : 48.550
Run: 1, Forward Execution Time (us) : 57.489
Run: 2, Forward Execution Time (us) : 57.527

# Benchmarking PyTorch: embeddingbag
# Mode: Eager
# Name: embeddingbag_embeddingbags2300_dim64_modesum_input_size16_offset0_sparseTrue_include_last_offsetTrue_cpu
# Input: embeddingbags: 2300, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: True, include_last_offset: True, device: cpu
Run: 0, Forward Execution Time (us) : 39.545
Run: 1, Forward Execution Time (us) : 47.007
Run: 2, Forward Execution Time (us) : 46.958

# Benchmarking PyTorch: embeddingbag
# Mode: Eager
# Name: embeddingbag_embeddingbags2300_dim64_modesum_input_size16_offset0_sparseTrue_include_last_offsetFalse_cpu
# Input: embeddingbags: 2300, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: True, include_last_offset: False, device: cpu
Run: 0, Forward Execution Time (us) : 48.479
Run: 1, Forward Execution Time (us) : 57.954
Run: 2, Forward Execution Time (us) : 57.677

# Benchmarking PyTorch: embeddingbag
# Mode: Eager
# Name: embeddingbag_embeddingbags2300_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetTrue_cpu
# Input: embeddingbags: 2300, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: True, device: cpu
Run: 0, Forward Execution Time (us) : 39.743
Run: 1, Forward Execution Time (us) : 47.268
Run: 2, Forward Execution Time (us) : 47.155

# Benchmarking PyTorch: embeddingbag
# Mode: Eager
# Name: embeddingbag_embeddingbags2300_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetFalse_cpu
# Input: embeddingbags: 2300, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: False, device: cpu
Run: 0, Forward Execution Time (us) : 48.659
Run: 1, Forward Execution Time (us) : 57.587
Run: 2, Forward Execution Time (us) : 57.605

# Benchmarking PyTorch: embeddingbag
# Mode: Eager
# Name: embeddingbag_embeddingbags2300_dim64_modesum_input_size64_offset0_sparseTrue_include_last_offsetTrue_cpu
# Input: embeddingbags: 2300, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: True, include_last_offset: True, device: cpu
Run: 0, Forward Execution Time (us) : 40.055
Run: 1, Forward Execution Time (us) : 47.694
Run: 2, Forward Execution Time (us) : 47.678

# Benchmarking PyTorch: embeddingbag
# Mode: Eager
# Name: embeddingbag_embeddingbags2300_dim64_modesum_input_size64_offset0_sparseTrue_include_last_offsetFalse_cpu
# Input: embeddingbags: 2300, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: True, include_last_offset: False, device: cpu
Run: 0, Forward Execution Time (us) : 49.098
Run: 1, Forward Execution Time (us) : 58.418
Run: 2, Forward Execution Time (us) : 58.329

# Benchmarking PyTorch: embeddingbag
# Mode: Eager
# Name: embeddingbag_embeddingbags2300_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetTrue_cpu
# Input: embeddingbags: 2300, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: True, device: cpu
Run: 0, Forward Execution Time (us) : 40.066
Run: 1, Forward Execution Time (us) : 47.598
Run: 2, Forward Execution Time (us) : 47.534

# Benchmarking PyTorch: embeddingbag
# Mode: Eager
# Name: embeddingbag_embeddingbags2300_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetFalse_cpu
# Input: embeddingbags: 2300, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: False, device: cpu
Run: 0, Forward Execution Time (us) : 49.041
Run: 1, Forward Execution Time (us) : 58.341
Run: 2, Forward Execution Time (us) : 58.146

# Benchmarking PyTorch: embeddingbag
# Mode: Eager
# Name: embeddingbag_embeddingbags10_dim64_modesum_input_size8_offset0_sparseTrue_include_last_offsetTrue_cpu
# Input: embeddingbags: 10, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: True, include_last_offset: True, device: cpu
Run: 0, Backward Execution Time (us) : 157.656
Run: 1, Backward Execution Time (us) : 157.785
Run: 2, Backward Execution Time (us) : 157.639

# Benchmarking PyTorch: embeddingbag
# Mode: Eager
# Name: embeddingbag_embeddingbags10_dim64_modesum_input_size8_offset0_sparseTrue_include_last_offsetFalse_cpu
# Input: embeddingbags: 10, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: True, include_last_offset: False, device: cpu
Run: 0, Backward Execution Time (us) : 157.121
Run: 1, Backward Execution Time (us) : 158.239
Run: 2, Backward Execution Time (us) : 157.292

# Benchmarking PyTorch: embeddingbag
# Mode: Eager
# Name: embeddingbag_embeddingbags10_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetTrue_cpu
# Input: embeddingbags: 10, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: True, device: cpu
Run: 0, Backward Execution Time (us) : 134.734
Run: 1, Backward Execution Time (us) : 135.143
Run: 2, Backward Execution Time (us) : 135.243

# Benchmarking PyTorch: embeddingbag
# Mode: Eager
# Name: embeddingbag_embeddingbags10_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetFalse_cpu
# Input: embeddingbags: 10, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: False, device: cpu
Run: 0, Backward Execution Time (us) : 134.778
Run: 1, Backward Execution Time (us) : 134.811
Run: 2, Backward Execution Time (us) : 134.775

# Benchmarking PyTorch: embeddingbag
# Mode: Eager
# Name: embeddingbag_embeddingbags10_dim64_modesum_input_size16_offset0_sparseTrue_include_last_offsetTrue_cpu
# Input: embeddingbags: 10, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: True, include_last_offset: True, device: cpu
Run: 0, Backward Execution Time (us) : 159.917
Run: 1, Backward Execution Time (us) : 160.481
Run: 2, Backward Execution Time (us) : 159.336

# Benchmarking PyTorch: embeddingbag
# Mode: Eager
# Name: embeddingbag_embeddingbags10_dim64_modesum_input_size16_offset0_sparseTrue_include_last_offsetFalse_cpu
# Input: embeddingbags: 10, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: True, include_last_offset: False, device: cpu
Run: 0, Backward Execution Time (us) : 159.513
Run: 1, Backward Execution Time (us) : 160.028
Run: 2, Backward Execution Time (us) : 159.527

# Benchmarking PyTorch: embeddingbag
# Mode: Eager
# Name: embeddingbag_embeddingbags10_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetTrue_cpu
# Input: embeddingbags: 10, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: True, device: cpu
Run: 0, Backward Execution Time (us) : 135.835
Run: 1, Backward Execution Time (us) : 136.296
Run: 2, Backward Execution Time (us) : 135.895

# Benchmarking PyTorch: embeddingbag
# Mode: Eager
# Name: embeddingbag_embeddingbags10_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetFalse_cpu
# Input: embeddingbags: 10, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: False, device: cpu
Run: 0, Backward Execution Time (us) : 135.946
Run: 1, Backward Execution Time (us) : 136.491
Run: 2, Backward Execution Time (us) : 135.807

# Benchmarking PyTorch: embeddingbag
# Mode: Eager
# Name: embeddingbag_embeddingbags10_dim64_modesum_input_size64_offset0_sparseTrue_include_last_offsetTrue_cpu
# Input: embeddingbags: 10, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: True, include_last_offset: True, device: cpu
Run: 0, Backward Execution Time (us) : 174.661
Run: 1, Backward Execution Time (us) : 174.938
Run: 2, Backward Execution Time (us) : 174.921

# Benchmarking PyTorch: embeddingbag
# Mode: Eager
# Name: embeddingbag_embeddingbags10_dim64_modesum_input_size64_offset0_sparseTrue_include_last_offsetFalse_cpu
# Input: embeddingbags: 10, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: True, include_last_offset: False, device: cpu
Run: 0, Backward Execution Time (us) : 175.167
Run: 1, Backward Execution Time (us) : 175.081
Run: 2, Backward Execution Time (us) : 175.005

# Benchmarking PyTorch: embeddingbag
# Mode: Eager
# Name: embeddingbag_embeddingbags10_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetTrue_cpu
# Input: embeddingbags: 10, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: True, device: cpu
Run: 0, Backward Execution Time (us) : 146.130
Run: 1, Backward Execution Time (us) : 146.108
Run: 2, Backward Execution Time (us) : 184.246

# Benchmarking PyTorch: embeddingbag
# Mode: Eager
# Name: embeddingbag_embeddingbags10_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetFalse_cpu
# Input: embeddingbags: 10, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: False, device: cpu
Run: 0, Backward Execution Time (us) : 146.189
Run: 1, Backward Execution Time (us) : 146.441
Run: 2, Backward Execution Time (us) : 146.051

# Benchmarking PyTorch: embeddingbag
# Mode: Eager
# Name: embeddingbag_embeddingbags120_dim64_modesum_input_size8_offset0_sparseTrue_include_last_offsetTrue_cpu
# Input: embeddingbags: 120, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: True, include_last_offset: True, device: cpu
Run: 0, Backward Execution Time (us) : 156.751
Run: 1, Backward Execution Time (us) : 157.520
Run: 2, Backward Execution Time (us) : 156.618

# Benchmarking PyTorch: embeddingbag
# Mode: Eager
# Name: embeddingbag_embeddingbags120_dim64_modesum_input_size8_offset0_sparseTrue_include_last_offsetFalse_cpu
# Input: embeddingbags: 120, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: True, include_last_offset: False, device: cpu
Run: 0, Backward Execution Time (us) : 156.712
Run: 1, Backward Execution Time (us) : 156.927
Run: 2, Backward Execution Time (us) : 156.683

# Benchmarking PyTorch: embeddingbag
# Mode: Eager
# Name: embeddingbag_embeddingbags120_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetTrue_cpu
# Input: embeddingbags: 120, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: True, device: cpu
Run: 0, Backward Execution Time (us) : 137.192
Run: 1, Backward Execution Time (us) : 137.471
Run: 2, Backward Execution Time (us) : 137.558

# Benchmarking PyTorch: embeddingbag
# Mode: Eager
# Name: embeddingbag_embeddingbags120_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetFalse_cpu
# Input: embeddingbags: 120, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: False, device: cpu
Run: 0, Backward Execution Time (us) : 137.250
Run: 1, Backward Execution Time (us) : 137.843
Run: 2, Backward Execution Time (us) : 137.395

# Benchmarking PyTorch: embeddingbag
# Mode: Eager
# Name: embeddingbag_embeddingbags120_dim64_modesum_input_size16_offset0_sparseTrue_include_last_offsetTrue_cpu
# Input: embeddingbags: 120, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: True, include_last_offset: True, device: cpu
Run: 0, Backward Execution Time (us) : 159.413
Run: 1, Backward Execution Time (us) : 159.948
Run: 2, Backward Execution Time (us) : 159.697

# Benchmarking PyTorch: embeddingbag
# Mode: Eager
# Name: embeddingbag_embeddingbags120_dim64_modesum_input_size16_offset0_sparseTrue_include_last_offsetFalse_cpu
# Input: embeddingbags: 120, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: True, include_last_offset: False, device: cpu
Run: 0, Backward Execution Time (us) : 158.859
Run: 1, Backward Execution Time (us) : 158.882
Run: 2, Backward Execution Time (us) : 159.179

# Benchmarking PyTorch: embeddingbag
# Mode: Eager
# Name: embeddingbag_embeddingbags120_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetTrue_cpu
# Input: embeddingbags: 120, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: True, device: cpu
Run: 0, Backward Execution Time (us) : 138.837
Run: 1, Backward Execution Time (us) : 138.750
Run: 2, Backward Execution Time (us) : 139.072

# Benchmarking PyTorch: embeddingbag
# Mode: Eager
# Name: embeddingbag_embeddingbags120_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetFalse_cpu
# Input: embeddingbags: 120, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: False, device: cpu
Run: 0, Backward Execution Time (us) : 138.736
Run: 1, Backward Execution Time (us) : 139.071
Run: 2, Backward Execution Time (us) : 138.746

# Benchmarking PyTorch: embeddingbag
# Mode: Eager
# Name: embeddingbag_embeddingbags120_dim64_modesum_input_size64_offset0_sparseTrue_include_last_offsetTrue_cpu
# Input: embeddingbags: 120, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: True, include_last_offset: True, device: cpu
Run: 0, Backward Execution Time (us) : 174.115
Run: 1, Backward Execution Time (us) : 174.147
Run: 2, Backward Execution Time (us) : 173.812

# Benchmarking PyTorch: embeddingbag
# Mode: Eager
# Name: embeddingbag_embeddingbags120_dim64_modesum_input_size64_offset0_sparseTrue_include_last_offsetFalse_cpu
# Input: embeddingbags: 120, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: True, include_last_offset: False, device: cpu
Run: 0, Backward Execution Time (us) : 174.550
Run: 1, Backward Execution Time (us) : 174.547
Run: 2, Backward Execution Time (us) : 174.167

# Benchmarking PyTorch: embeddingbag
# Mode: Eager
# Name: embeddingbag_embeddingbags120_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetTrue_cpu
# Input: embeddingbags: 120, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: True, device: cpu
Run: 0, Backward Execution Time (us) : 149.002
Run: 1, Backward Execution Time (us) : 149.599
Run: 2, Backward Execution Time (us) : 149.576

# Benchmarking PyTorch: embeddingbag
# Mode: Eager
# Name: embeddingbag_embeddingbags120_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetFalse_cpu
# Input: embeddingbags: 120, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: False, device: cpu
Run: 0, Backward Execution Time (us) : 148.757
Run: 1, Backward Execution Time (us) : 148.808
Run: 2, Backward Execution Time (us) : 148.975

# Benchmarking PyTorch: embeddingbag
# Mode: Eager
# Name: embeddingbag_embeddingbags1000_dim64_modesum_input_size8_offset0_sparseTrue_include_last_offsetTrue_cpu
# Input: embeddingbags: 1000, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: True, include_last_offset: True, device: cpu
Run: 0, Backward Execution Time (us) : 156.958
Run: 1, Backward Execution Time (us) : 157.686
Run: 2, Backward Execution Time (us) : 157.213

# Benchmarking PyTorch: embeddingbag
# Mode: Eager
# Name: embeddingbag_embeddingbags1000_dim64_modesum_input_size8_offset0_sparseTrue_include_last_offsetFalse_cpu
# Input: embeddingbags: 1000, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: True, include_last_offset: False, device: cpu
Run: 0, Backward Execution Time (us) : 156.902
Run: 1, Backward Execution Time (us) : 156.936
Run: 2, Backward Execution Time (us) : 156.662

# Benchmarking PyTorch: embeddingbag
# Mode: Eager
# Name: embeddingbag_embeddingbags1000_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetTrue_cpu
# Input: embeddingbags: 1000, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: True, device: cpu
Run: 0, Backward Execution Time (us) : 169.146
Run: 1, Backward Execution Time (us) : 169.328
Run: 2, Backward Execution Time (us) : 169.549

# Benchmarking PyTorch: embeddingbag
# Mode: Eager
# Name: embeddingbag_embeddingbags1000_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetFalse_cpu
# Input: embeddingbags: 1000, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: False, device: cpu
Run: 0, Backward Execution Time (us) : 168.559
Run: 1, Backward Execution Time (us) : 168.803
Run: 2, Backward Execution Time (us) : 168.675

# Benchmarking PyTorch: embeddingbag
# Mode: Eager
# Name: embeddingbag_embeddingbags1000_dim64_modesum_input_size16_offset0_sparseTrue_include_last_offsetTrue_cpu
# Input: embeddingbags: 1000, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: True, include_last_offset: True, device: cpu
Run: 0, Backward Execution Time (us) : 134.816
Run: 1, Backward Execution Time (us) : 159.754
Run: 2, Backward Execution Time (us) : 159.257

# Benchmarking PyTorch: embeddingbag
# Mode: Eager
# Name: embeddingbag_embeddingbags1000_dim64_modesum_input_size16_offset0_sparseTrue_include_last_offsetFalse_cpu
# Input: embeddingbags: 1000, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: True, include_last_offset: False, device: cpu
Run: 0, Backward Execution Time (us) : 134.832
Run: 1, Backward Execution Time (us) : 159.255
Run: 2, Backward Execution Time (us) : 159.036

# Benchmarking PyTorch: embeddingbag
# Mode: Eager
# Name: embeddingbag_embeddingbags1000_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetTrue_cpu
# Input: embeddingbags: 1000, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: True, device: cpu
Run: 0, Backward Execution Time (us) : 171.150
Run: 1, Backward Execution Time (us) : 171.439
Run: 2, Backward Execution Time (us) : 171.508

# Benchmarking PyTorch: embeddingbag
# Mode: Eager
# Name: embeddingbag_embeddingbags1000_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetFalse_cpu
# Input: embeddingbags: 1000, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: False, device: cpu
Run: 0, Backward Execution Time (us) : 170.078
Run: 1, Backward Execution Time (us) : 170.666
Run: 2, Backward Execution Time (us) : 170.523

# Benchmarking PyTorch: embeddingbag
# Mode: Eager
# Name: embeddingbag_embeddingbags1000_dim64_modesum_input_size64_offset0_sparseTrue_include_last_offsetTrue_cpu
# Input: embeddingbags: 1000, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: True, include_last_offset: True, device: cpu
Run: 0, Backward Execution Time (us) : 148.097
Run: 1, Backward Execution Time (us) : 174.481
Run: 2, Backward Execution Time (us) : 174.690

# Benchmarking PyTorch: embeddingbag
# Mode: Eager
# Name: embeddingbag_embeddingbags1000_dim64_modesum_input_size64_offset0_sparseTrue_include_last_offsetFalse_cpu
# Input: embeddingbags: 1000, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: True, include_last_offset: False, device: cpu
Run: 0, Backward Execution Time (us) : 148.060
Run: 1, Backward Execution Time (us) : 174.888
Run: 2, Backward Execution Time (us) : 174.956

# Benchmarking PyTorch: embeddingbag
# Mode: Eager
# Name: embeddingbag_embeddingbags1000_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetTrue_cpu
# Input: embeddingbags: 1000, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: True, device: cpu
Run: 0, Backward Execution Time (us) : 181.823
Run: 1, Backward Execution Time (us) : 182.973
Run: 2, Backward Execution Time (us) : 182.422

# Benchmarking PyTorch: embeddingbag
# Mode: Eager
# Name: embeddingbag_embeddingbags1000_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetFalse_cpu
# Input: embeddingbags: 1000, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: False, device: cpu
Run: 0, Backward Execution Time (us) : 180.495
Run: 1, Backward Execution Time (us) : 180.889
Run: 2, Backward Execution Time (us) : 180.476

# Benchmarking PyTorch: embeddingbag
# Mode: Eager
# Name: embeddingbag_embeddingbags2300_dim64_modesum_input_size8_offset0_sparseTrue_include_last_offsetTrue_cpu
# Input: embeddingbags: 2300, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: True, include_last_offset: True, device: cpu
Run: 0, Backward Execution Time (us) : 157.178
Run: 1, Backward Execution Time (us) : 157.303
Run: 2, Backward Execution Time (us) : 157.252

# Benchmarking PyTorch: embeddingbag
# Mode: Eager
# Name: embeddingbag_embeddingbags2300_dim64_modesum_input_size8_offset0_sparseTrue_include_last_offsetFalse_cpu
# Input: embeddingbags: 2300, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: True, include_last_offset: False, device: cpu
Run: 0, Backward Execution Time (us) : 132.591
Run: 1, Backward Execution Time (us) : 156.657
Run: 2, Backward Execution Time (us) : 156.704

# Benchmarking PyTorch: embeddingbag
# Mode: Eager
# Name: embeddingbag_embeddingbags2300_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetTrue_cpu
# Input: embeddingbags: 2300, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: True, device: cpu
Run: 0, Backward Execution Time (us) : 215.787
Run: 1, Backward Execution Time (us) : 231.495
Run: 2, Backward Execution Time (us) : 232.224

# Benchmarking PyTorch: embeddingbag
# Mode: Eager
# Name: embeddingbag_embeddingbags2300_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetFalse_cpu
# Input: embeddingbags: 2300, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: False, device: cpu
Run: 0, Backward Execution Time (us) : 231.748
Run: 1, Backward Execution Time (us) : 232.862
Run: 2, Backward Execution Time (us) : 233.020

# Benchmarking PyTorch: embeddingbag
# Mode: Eager
# Name: embeddingbag_embeddingbags2300_dim64_modesum_input_size16_offset0_sparseTrue_include_last_offsetTrue_cpu
# Input: embeddingbags: 2300, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: True, include_last_offset: True, device: cpu
Run: 0, Backward Execution Time (us) : 135.022
Run: 1, Backward Execution Time (us) : 159.529
Run: 2, Backward Execution Time (us) : 159.536

# Benchmarking PyTorch: embeddingbag
# Mode: Eager
# Name: embeddingbag_embeddingbags2300_dim64_modesum_input_size16_offset0_sparseTrue_include_last_offsetFalse_cpu
# Input: embeddingbags: 2300, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: True, include_last_offset: False, device: cpu
Run: 0, Backward Execution Time (us) : 134.831
Run: 1, Backward Execution Time (us) : 159.500
Run: 2, Backward Execution Time (us) : 159.487

# Benchmarking PyTorch: embeddingbag
# Mode: Eager
# Name: embeddingbag_embeddingbags2300_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetTrue_cpu
# Input: embeddingbags: 2300, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: True, device: cpu
Run: 0, Backward Execution Time (us) : 213.690
Run: 1, Backward Execution Time (us) : 232.696
Run: 2, Backward Execution Time (us) : 233.182

# Benchmarking PyTorch: embeddingbag
# Mode: Eager
# Name: embeddingbag_embeddingbags2300_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetFalse_cpu
# Input: embeddingbags: 2300, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: False, device: cpu
Run: 0, Backward Execution Time (us) : 233.644
Run: 1, Backward Execution Time (us) : 234.317
Run: 2, Backward Execution Time (us) : 234.461

# Benchmarking PyTorch: embeddingbag
# Mode: Eager
# Name: embeddingbag_embeddingbags2300_dim64_modesum_input_size64_offset0_sparseTrue_include_last_offsetTrue_cpu
# Input: embeddingbags: 2300, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: True, include_last_offset: True, device: cpu
Run: 0, Backward Execution Time (us) : 148.049
Run: 1, Backward Execution Time (us) : 175.889
Run: 2, Backward Execution Time (us) : 174.670

# Benchmarking PyTorch: embeddingbag
# Mode: Eager
# Name: embeddingbag_embeddingbags2300_dim64_modesum_input_size64_offset0_sparseTrue_include_last_offsetFalse_cpu
# Input: embeddingbags: 2300, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: True, include_last_offset: False, device: cpu
Run: 0, Backward Execution Time (us) : 148.311
Run: 1, Backward Execution Time (us) : 174.833
Run: 2, Backward Execution Time (us) : 174.848

# Benchmarking PyTorch: embeddingbag
# Mode: Eager
# Name: embeddingbag_embeddingbags2300_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetTrue_cpu
# Input: embeddingbags: 2300, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: True, device: cpu
Run: 0, Backward Execution Time (us) : 226.946
Run: 1, Backward Execution Time (us) : 243.275
Run: 2, Backward Execution Time (us) : 242.546

# Benchmarking PyTorch: embeddingbag
# Mode: Eager
# Name: embeddingbag_embeddingbags2300_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetFalse_cpu
# Input: embeddingbags: 2300, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: False, device: cpu
Run: 0, Backward Execution Time (us) : 244.365
Run: 1, Backward Execution Time (us) : 271.184
Run: 2, Backward Execution Time (us) : 247.247

# Benchmarking PyTorch: embedding
# Mode: Eager
# Name: embedding_num_embeddings10_embedding_dim64_input_size8_cpu
# Input: num_embeddings: 10, embedding_dim: 64, input_size: 8, device: cpu
Run: 0, Forward Execution Time (us) : 25.136
Run: 1, Forward Execution Time (us) : 25.300
Run: 2, Forward Execution Time (us) : 25.357

# Benchmarking PyTorch: embedding
# Mode: Eager
# Name: embedding_num_embeddings10_embedding_dim64_input_size16_cpu
# Input: num_embeddings: 10, embedding_dim: 64, input_size: 16, device: cpu
Run: 0, Forward Execution Time (us) : 25.315
Run: 1, Forward Execution Time (us) : 25.496
Run: 2, Forward Execution Time (us) : 25.433

# Benchmarking PyTorch: embedding
# Mode: Eager
# Name: embedding_num_embeddings10_embedding_dim64_input_size64_cpu
# Input: num_embeddings: 10, embedding_dim: 64, input_size: 64, device: cpu
Run: 0, Forward Execution Time (us) : 26.794
Run: 1, Forward Execution Time (us) : 27.016
Run: 2, Forward Execution Time (us) : 26.940

# Benchmarking PyTorch: embedding
# Mode: Eager
# Name: embedding_num_embeddings120_embedding_dim64_input_size8_cpu
# Input: num_embeddings: 120, embedding_dim: 64, input_size: 8, device: cpu
Run: 0, Forward Execution Time (us) : 25.089
Run: 1, Forward Execution Time (us) : 25.206
Run: 2, Forward Execution Time (us) : 25.092

# Benchmarking PyTorch: embedding
# Mode: Eager
# Name: embedding_num_embeddings120_embedding_dim64_input_size16_cpu
# Input: num_embeddings: 120, embedding_dim: 64, input_size: 16, device: cpu
Run: 0, Forward Execution Time (us) : 25.161
Run: 1, Forward Execution Time (us) : 25.409
Run: 2, Forward Execution Time (us) : 25.348

# Benchmarking PyTorch: embedding
# Mode: Eager
# Name: embedding_num_embeddings120_embedding_dim64_input_size64_cpu
# Input: num_embeddings: 120, embedding_dim: 64, input_size: 64, device: cpu
Run: 0, Forward Execution Time (us) : 26.740
Run: 1, Forward Execution Time (us) : 26.888
Run: 2, Forward Execution Time (us) : 26.918

# Benchmarking PyTorch: embedding
# Mode: Eager
# Name: embedding_num_embeddings1000_embedding_dim64_input_size8_cpu
# Input: num_embeddings: 1000, embedding_dim: 64, input_size: 8, device: cpu
Run: 0, Forward Execution Time (us) : 24.955
Run: 1, Forward Execution Time (us) : 25.175
Run: 2, Forward Execution Time (us) : 25.156

# Benchmarking PyTorch: embedding
# Mode: Eager
# Name: embedding_num_embeddings1000_embedding_dim64_input_size16_cpu
# Input: num_embeddings: 1000, embedding_dim: 64, input_size: 16, device: cpu
Run: 0, Forward Execution Time (us) : 21.310
Run: 1, Forward Execution Time (us) : 25.353
Run: 2, Forward Execution Time (us) : 25.371

# Benchmarking PyTorch: embedding
# Mode: Eager
# Name: embedding_num_embeddings1000_embedding_dim64_input_size64_cpu
# Input: num_embeddings: 1000, embedding_dim: 64, input_size: 64, device: cpu
Run: 0, Forward Execution Time (us) : 22.654
Run: 1, Forward Execution Time (us) : 26.876
Run: 2, Forward Execution Time (us) : 26.873

# Benchmarking PyTorch: embedding
# Mode: Eager
# Name: embedding_num_embeddings2300_embedding_dim64_input_size8_cpu
# Input: num_embeddings: 2300, embedding_dim: 64, input_size: 8, device: cpu
Run: 0, Forward Execution Time (us) : 21.256
Run: 1, Forward Execution Time (us) : 25.285
Run: 2, Forward Execution Time (us) : 25.247

# Benchmarking PyTorch: embedding
# Mode: Eager
# Name: embedding_num_embeddings2300_embedding_dim64_input_size16_cpu
# Input: num_embeddings: 2300, embedding_dim: 64, input_size: 16, device: cpu
Run: 0, Forward Execution Time (us) : 25.329
Run: 1, Forward Execution Time (us) : 25.530
Run: 2, Forward Execution Time (us) : 25.497

# Benchmarking PyTorch: embedding
# Mode: Eager
# Name: embedding_num_embeddings2300_embedding_dim64_input_size64_cpu
# Input: num_embeddings: 2300, embedding_dim: 64, input_size: 64, device: cpu
Run: 0, Forward Execution Time (us) : 22.631
Run: 1, Forward Execution Time (us) : 26.879
Run: 2, Forward Execution Time (us) : 26.888

# Benchmarking PyTorch: embedding
# Mode: Eager
# Name: embedding_num_embeddings10_embedding_dim64_input_size8_cpu
# Input: num_embeddings: 10, embedding_dim: 64, input_size: 8, device: cpu
Run: 0, Backward Execution Time (us) : 90.870
Run: 1, Backward Execution Time (us) : 91.268
Run: 2, Backward Execution Time (us) : 91.390

# Benchmarking PyTorch: embedding
# Mode: Eager
# Name: embedding_num_embeddings10_embedding_dim64_input_size16_cpu
# Input: num_embeddings: 10, embedding_dim: 64, input_size: 16, device: cpu
Run: 0, Backward Execution Time (us) : 92.866
Run: 1, Backward Execution Time (us) : 93.243
Run: 2, Backward Execution Time (us) : 92.979

# Benchmarking PyTorch: embedding
# Mode: Eager
# Name: embedding_num_embeddings10_embedding_dim64_input_size64_cpu
# Input: num_embeddings: 10, embedding_dim: 64, input_size: 64, device: cpu
Run: 0, Backward Execution Time (us) : 104.930
Run: 1, Backward Execution Time (us) : 104.976
Run: 2, Backward Execution Time (us) : 105.560

# Benchmarking PyTorch: embedding
# Mode: Eager
# Name: embedding_num_embeddings120_embedding_dim64_input_size8_cpu
# Input: num_embeddings: 120, embedding_dim: 64, input_size: 8, device: cpu
Run: 0, Backward Execution Time (us) : 92.979
Run: 1, Backward Execution Time (us) : 92.994
Run: 2, Backward Execution Time (us) : 92.984

# Benchmarking PyTorch: embedding
# Mode: Eager
# Name: embedding_num_embeddings120_embedding_dim64_input_size16_cpu
# Input: num_embeddings: 120, embedding_dim: 64, input_size: 16, device: cpu
Run: 0, Backward Execution Time (us) : 94.831
Run: 1, Backward Execution Time (us) : 94.924
Run: 2, Backward Execution Time (us) : 95.104

# Benchmarking PyTorch: embedding
# Mode: Eager
# Name: embedding_num_embeddings120_embedding_dim64_input_size64_cpu
# Input: num_embeddings: 120, embedding_dim: 64, input_size: 64, device: cpu
Run: 0, Backward Execution Time (us) : 106.809
Run: 1, Backward Execution Time (us) : 107.198
Run: 2, Backward Execution Time (us) : 107.052

# Benchmarking PyTorch: embedding
# Mode: Eager
# Name: embedding_num_embeddings1000_embedding_dim64_input_size8_cpu
# Input: num_embeddings: 1000, embedding_dim: 64, input_size: 8, device: cpu
Run: 0, Backward Execution Time (us) : 116.928
Run: 1, Backward Execution Time (us) : 117.474
Run: 2, Backward Execution Time (us) : 116.912

# Benchmarking PyTorch: embedding
# Mode: Eager
# Name: embedding_num_embeddings1000_embedding_dim64_input_size16_cpu
# Input: num_embeddings: 1000, embedding_dim: 64, input_size: 16, device: cpu
Run: 0, Backward Execution Time (us) : 116.124
Run: 1, Backward Execution Time (us) : 116.304
Run: 2, Backward Execution Time (us) : 116.201

# Benchmarking PyTorch: embedding
# Mode: Eager
# Name: embedding_num_embeddings1000_embedding_dim64_input_size64_cpu
# Input: num_embeddings: 1000, embedding_dim: 64, input_size: 64, device: cpu
Run: 0, Backward Execution Time (us) : 131.676
Run: 1, Backward Execution Time (us) : 131.290
Run: 2, Backward Execution Time (us) : 131.776

# Benchmarking PyTorch: embedding
# Mode: Eager
# Name: embedding_num_embeddings2300_embedding_dim64_input_size8_cpu
# Input: num_embeddings: 2300, embedding_dim: 64, input_size: 8, device: cpu
Run: 0, Backward Execution Time (us) : 169.861
Run: 1, Backward Execution Time (us) : 171.672
Run: 2, Backward Execution Time (us) : 170.443

# Benchmarking PyTorch: embedding
# Mode: Eager
# Name: embedding_num_embeddings2300_embedding_dim64_input_size16_cpu
# Input: num_embeddings: 2300, embedding_dim: 64, input_size: 16, device: cpu
Run: 0, Backward Execution Time (us) : 174.162
Run: 1, Backward Execution Time (us) : 175.415
Run: 2, Backward Execution Time (us) : 174.926

# Benchmarking PyTorch: embedding
# Mode: Eager
# Name: embedding_num_embeddings2300_embedding_dim64_input_size64_cpu
# Input: num_embeddings: 2300, embedding_dim: 64, input_size: 64, device: cpu
Run: 0, Backward Execution Time (us) : 184.726
Run: 1, Backward Execution Time (us) : 188.362
Run: 2, Backward Execution Time (us) : 187.846

+ python -m pt.qcomparators_test --num-runs 3 --iterations 100 --warmup-iterations 10
not use dipu
# ----------------------------------------
# PyTorch/Caffe2 Operator Micro-benchmarks
# ----------------------------------------
# Tag : short

# Benchmarking PyTorch: eq
# Mode: Eager
# Name: eq_N8_dtypetorch.quint8_contigFalse_other_scalarFalse_out_variantFalse
# Input: N: 8, dtype: torch.quint8, contig: False, other_scalar: False, out_variant: False
Run: 0, Forward Execution Time (us) : 19.193
Run: 1, Forward Execution Time (us) : 22.848
Run: 2, Forward Execution Time (us) : 22.466

# Benchmarking PyTorch: eq
# Mode: Eager
# Name: eq_N8_dtypetorch.quint8_contigFalse_other_scalarFalse_out_variantTrue
# Input: N: 8, dtype: torch.quint8, contig: False, other_scalar: False, out_variant: True
Run: 0, Forward Execution Time (us) : 40.028
Run: 1, Forward Execution Time (us) : 40.377
Run: 2, Forward Execution Time (us) : 40.444

# Benchmarking PyTorch: eq
# Mode: Eager
# Name: eq_N8_dtypetorch.quint8_contigFalse_other_scalarTrue_out_variantFalse
# Input: N: 8, dtype: torch.quint8, contig: False, other_scalar: True, out_variant: False
Run: 0, Forward Execution Time (us) : 26.671
Run: 1, Forward Execution Time (us) : 26.724
Run: 2, Forward Execution Time (us) : 26.740

# Benchmarking PyTorch: eq
# Mode: Eager
# Name: eq_N8_dtypetorch.quint8_contigFalse_other_scalarTrue_out_variantTrue
# Input: N: 8, dtype: torch.quint8, contig: False, other_scalar: True, out_variant: True
Run: 0, Forward Execution Time (us) : 44.431
Run: 1, Forward Execution Time (us) : 44.869
Run: 2, Forward Execution Time (us) : 44.882

# Benchmarking PyTorch: eq
# Mode: Eager
# Name: eq_N8_dtypetorch.quint8_contigTrue_other_scalarFalse_out_variantFalse
# Input: N: 8, dtype: torch.quint8, contig: True, other_scalar: False, out_variant: False
Run: 0, Forward Execution Time (us) : 15.487
Run: 1, Forward Execution Time (us) : 14.192
Run: 2, Forward Execution Time (us) : 14.615

# Benchmarking PyTorch: eq
# Mode: Eager
# Name: eq_N8_dtypetorch.quint8_contigTrue_other_scalarFalse_out_variantTrue
# Input: N: 8, dtype: torch.quint8, contig: True, other_scalar: False, out_variant: True
Run: 0, Forward Execution Time (us) : 32.120
Run: 1, Forward Execution Time (us) : 32.356
Run: 2, Forward Execution Time (us) : 32.288

# Benchmarking PyTorch: eq
# Mode: Eager
# Name: eq_N8_dtypetorch.quint8_contigTrue_other_scalarTrue_out_variantFalse
# Input: N: 8, dtype: torch.quint8, contig: True, other_scalar: True, out_variant: False
Run: 0, Forward Execution Time (us) : 19.612
Run: 1, Forward Execution Time (us) : 19.711
Run: 2, Forward Execution Time (us) : 19.695

# Benchmarking PyTorch: eq
# Mode: Eager
# Name: eq_N8_dtypetorch.quint8_contigTrue_other_scalarTrue_out_variantTrue
# Input: N: 8, dtype: torch.quint8, contig: True, other_scalar: True, out_variant: True
Run: 0, Forward Execution Time (us) : 36.942
Run: 1, Forward Execution Time (us) : 37.389
Run: 2, Forward Execution Time (us) : 37.251

# Benchmarking PyTorch: eq
# Mode: Eager
# Name: eq_N8_dtypetorch.qint8_contigFalse_other_scalarFalse_out_variantFalse
# Input: N: 8, dtype: torch.qint8, contig: False, other_scalar: False, out_variant: False
Run: 0, Forward Execution Time (us) : 22.255
Run: 1, Forward Execution Time (us) : 22.446
Run: 2, Forward Execution Time (us) : 22.318

# Benchmarking PyTorch: eq
# Mode: Eager
# Name: eq_N8_dtypetorch.qint8_contigFalse_other_scalarFalse_out_variantTrue
# Input: N: 8, dtype: torch.qint8, contig: False, other_scalar: False, out_variant: True
Run: 0, Forward Execution Time (us) : 40.256
Run: 1, Forward Execution Time (us) : 40.563
Run: 2, Forward Execution Time (us) : 40.515

# Benchmarking PyTorch: eq
# Mode: Eager
# Name: eq_N8_dtypetorch.qint8_contigFalse_other_scalarTrue_out_variantFalse
# Input: N: 8, dtype: torch.qint8, contig: False, other_scalar: True, out_variant: False
Run: 0, Forward Execution Time (us) : 26.460
Run: 1, Forward Execution Time (us) : 26.621
Run: 2, Forward Execution Time (us) : 26.705

# Benchmarking PyTorch: eq
# Mode: Eager
# Name: eq_N8_dtypetorch.qint8_contigFalse_other_scalarTrue_out_variantTrue
# Input: N: 8, dtype: torch.qint8, contig: False, other_scalar: True, out_variant: True
Run: 0, Forward Execution Time (us) : 44.747
Run: 1, Forward Execution Time (us) : 44.923
Run: 2, Forward Execution Time (us) : 44.960

# Benchmarking PyTorch: eq
# Mode: Eager
# Name: eq_N8_dtypetorch.qint8_contigTrue_other_scalarFalse_out_variantFalse
# Input: N: 8, dtype: torch.qint8, contig: True, other_scalar: False, out_variant: False
Run: 0, Forward Execution Time (us) : 15.188
Run: 1, Forward Execution Time (us) : 14.478
Run: 2, Forward Execution Time (us) : 14.421

# Benchmarking PyTorch: eq
# Mode: Eager
# Name: eq_N8_dtypetorch.qint8_contigTrue_other_scalarFalse_out_variantTrue
# Input: N: 8, dtype: torch.qint8, contig: True, other_scalar: False, out_variant: True
Run: 0, Forward Execution Time (us) : 32.224
Run: 1, Forward Execution Time (us) : 32.437
Run: 2, Forward Execution Time (us) : 32.333

# Benchmarking PyTorch: eq
# Mode: Eager
# Name: eq_N8_dtypetorch.qint8_contigTrue_other_scalarTrue_out_variantFalse
# Input: N: 8, dtype: torch.qint8, contig: True, other_scalar: True, out_variant: False
Run: 0, Forward Execution Time (us) : 19.640
Run: 1, Forward Execution Time (us) : 19.737
Run: 2, Forward Execution Time (us) : 19.758

# Benchmarking PyTorch: eq
# Mode: Eager
# Name: eq_N8_dtypetorch.qint8_contigTrue_other_scalarTrue_out_variantTrue
# Input: N: 8, dtype: torch.qint8, contig: True, other_scalar: True, out_variant: True
Run: 0, Forward Execution Time (us) : 37.054
Run: 1, Forward Execution Time (us) : 37.321
Run: 2, Forward Execution Time (us) : 37.306

# Benchmarking PyTorch: eq
# Mode: Eager
# Name: eq_N8_dtypetorch.qint32_contigFalse_other_scalarFalse_out_variantFalse
# Input: N: 8, dtype: torch.qint32, contig: False, other_scalar: False, out_variant: False
Run: 0, Forward Execution Time (us) : 22.585
Run: 1, Forward Execution Time (us) : 22.652
Run: 2, Forward Execution Time (us) : 22.557

# Benchmarking PyTorch: eq
# Mode: Eager
# Name: eq_N8_dtypetorch.qint32_contigFalse_other_scalarFalse_out_variantTrue
# Input: N: 8, dtype: torch.qint32, contig: False, other_scalar: False, out_variant: True
Run: 0, Forward Execution Time (us) : 40.263
Run: 1, Forward Execution Time (us) : 40.593
Run: 2, Forward Execution Time (us) : 40.538

# Benchmarking PyTorch: eq
# Mode: Eager
# Name: eq_N8_dtypetorch.qint32_contigFalse_other_scalarTrue_out_variantFalse
# Input: N: 8, dtype: torch.qint32, contig: False, other_scalar: True, out_variant: False
Run: 0, Forward Execution Time (us) : 26.671
Run: 1, Forward Execution Time (us) : 26.867
Run: 2, Forward Execution Time (us) : 26.900

# Benchmarking PyTorch: eq
# Mode: Eager
# Name: eq_N8_dtypetorch.qint32_contigFalse_other_scalarTrue_out_variantTrue
# Input: N: 8, dtype: torch.qint32, contig: False, other_scalar: True, out_variant: True
Run: 0, Forward Execution Time (us) : 44.679
Run: 1, Forward Execution Time (us) : 44.859
Run: 2, Forward Execution Time (us) : 44.922

# Benchmarking PyTorch: eq
# Mode: Eager
# Name: eq_N8_dtypetorch.qint32_contigTrue_other_scalarFalse_out_variantFalse
# Input: N: 8, dtype: torch.qint32, contig: True, other_scalar: False, out_variant: False
Run: 0, Forward Execution Time (us) : 14.658
Run: 1, Forward Execution Time (us) : 15.011
Run: 2, Forward Execution Time (us) : 14.286

# Benchmarking PyTorch: eq
# Mode: Eager
# Name: eq_N8_dtypetorch.qint32_contigTrue_other_scalarFalse_out_variantTrue
# Input: N: 8, dtype: torch.qint32, contig: True, other_scalar: False, out_variant: True
Run: 0, Forward Execution Time (us) : 32.108
Run: 1, Forward Execution Time (us) : 32.403
Run: 2, Forward Execution Time (us) : 32.226

# Benchmarking PyTorch: eq
# Mode: Eager
# Name: eq_N8_dtypetorch.qint32_contigTrue_other_scalarTrue_out_variantFalse
# Input: N: 8, dtype: torch.qint32, contig: True, other_scalar: True, out_variant: False
Run: 0, Forward Execution Time (us) : 19.632
Run: 1, Forward Execution Time (us) : 19.849
Run: 2, Forward Execution Time (us) : 19.832

# Benchmarking PyTorch: eq
# Mode: Eager
# Name: eq_N8_dtypetorch.qint32_contigTrue_other_scalarTrue_out_variantTrue
# Input: N: 8, dtype: torch.qint32, contig: True, other_scalar: True, out_variant: True
Run: 0, Forward Execution Time (us) : 37.071
Run: 1, Forward Execution Time (us) : 37.276
Run: 2, Forward Execution Time (us) : 37.273

# Benchmarking PyTorch: eq
# Mode: Eager
# Name: eq_N64_dtypetorch.quint8_contigFalse_other_scalarFalse_out_variantFalse
# Input: N: 64, dtype: torch.quint8, contig: False, other_scalar: False, out_variant: False
Run: 0, Forward Execution Time (us) : 129.323
Run: 1, Forward Execution Time (us) : 129.557
Run: 2, Forward Execution Time (us) : 129.482

# Benchmarking PyTorch: eq
# Mode: Eager
# Name: eq_N64_dtypetorch.quint8_contigFalse_other_scalarFalse_out_variantTrue
# Input: N: 64, dtype: torch.quint8, contig: False, other_scalar: False, out_variant: True
Run: 0, Forward Execution Time (us) : 147.506
Run: 1, Forward Execution Time (us) : 147.731
Run: 2, Forward Execution Time (us) : 147.575

# Benchmarking PyTorch: eq
# Mode: Eager
# Name: eq_N64_dtypetorch.quint8_contigFalse_other_scalarTrue_out_variantFalse
# Input: N: 64, dtype: torch.quint8, contig: False, other_scalar: True, out_variant: False
Run: 0, Forward Execution Time (us) : 83.812
Run: 1, Forward Execution Time (us) : 83.943
Run: 2, Forward Execution Time (us) : 84.010

# Benchmarking PyTorch: eq
# Mode: Eager
# Name: eq_N64_dtypetorch.quint8_contigFalse_other_scalarTrue_out_variantTrue
# Input: N: 64, dtype: torch.quint8, contig: False, other_scalar: True, out_variant: True
Run: 0, Forward Execution Time (us) : 101.820
Run: 1, Forward Execution Time (us) : 102.147
Run: 2, Forward Execution Time (us) : 102.248

# Benchmarking PyTorch: eq
# Mode: Eager
# Name: eq_N64_dtypetorch.quint8_contigTrue_other_scalarFalse_out_variantFalse
# Input: N: 64, dtype: torch.quint8, contig: True, other_scalar: False, out_variant: False
Run: 0, Forward Execution Time (us) : 103.203
Run: 1, Forward Execution Time (us) : 102.697
Run: 2, Forward Execution Time (us) : 103.071

# Benchmarking PyTorch: eq
# Mode: Eager
# Name: eq_N64_dtypetorch.quint8_contigTrue_other_scalarFalse_out_variantTrue
# Input: N: 64, dtype: torch.quint8, contig: True, other_scalar: False, out_variant: True
Run: 0, Forward Execution Time (us) : 136.767
Run: 1, Forward Execution Time (us) : 137.052
Run: 2, Forward Execution Time (us) : 137.003

# Benchmarking PyTorch: eq
# Mode: Eager
# Name: eq_N64_dtypetorch.quint8_contigTrue_other_scalarTrue_out_variantFalse
# Input: N: 64, dtype: torch.quint8, contig: True, other_scalar: True, out_variant: False
Run: 0, Forward Execution Time (us) : 73.972
Run: 1, Forward Execution Time (us) : 74.185
Run: 2, Forward Execution Time (us) : 74.308

# Benchmarking PyTorch: eq
# Mode: Eager
# Name: eq_N64_dtypetorch.quint8_contigTrue_other_scalarTrue_out_variantTrue
# Input: N: 64, dtype: torch.quint8, contig: True, other_scalar: True, out_variant: True
Run: 0, Forward Execution Time (us) : 91.436
Run: 1, Forward Execution Time (us) : 91.814
Run: 2, Forward Execution Time (us) : 91.809

# Benchmarking PyTorch: eq
# Mode: Eager
# Name: eq_N64_dtypetorch.qint8_contigFalse_other_scalarFalse_out_variantFalse
# Input: N: 64, dtype: torch.qint8, contig: False, other_scalar: False, out_variant: False
Run: 0, Forward Execution Time (us) : 129.504
Run: 1, Forward Execution Time (us) : 129.755
Run: 2, Forward Execution Time (us) : 129.716

# Benchmarking PyTorch: eq
# Mode: Eager
# Name: eq_N64_dtypetorch.qint8_contigFalse_other_scalarFalse_out_variantTrue
# Input: N: 64, dtype: torch.qint8, contig: False, other_scalar: False, out_variant: True
Run: 0, Forward Execution Time (us) : 147.296
Run: 1, Forward Execution Time (us) : 147.776
Run: 2, Forward Execution Time (us) : 147.507

# Benchmarking PyTorch: eq
# Mode: Eager
# Name: eq_N64_dtypetorch.qint8_contigFalse_other_scalarTrue_out_variantFalse
# Input: N: 64, dtype: torch.qint8, contig: False, other_scalar: True, out_variant: False
Run: 0, Forward Execution Time (us) : 83.759
Run: 1, Forward Execution Time (us) : 83.796
Run: 2, Forward Execution Time (us) : 83.924

# Benchmarking PyTorch: eq
# Mode: Eager
# Name: eq_N64_dtypetorch.qint8_contigFalse_other_scalarTrue_out_variantTrue
# Input: N: 64, dtype: torch.qint8, contig: False, other_scalar: True, out_variant: True
Run: 0, Forward Execution Time (us) : 101.920
Run: 1, Forward Execution Time (us) : 102.207
Run: 2, Forward Execution Time (us) : 102.166

# Benchmarking PyTorch: eq
# Mode: Eager
# Name: eq_N64_dtypetorch.qint8_contigTrue_other_scalarFalse_out_variantFalse
# Input: N: 64, dtype: torch.qint8, contig: True, other_scalar: False, out_variant: False
Run: 0, Forward Execution Time (us) : 102.903
Run: 1, Forward Execution Time (us) : 102.485
Run: 2, Forward Execution Time (us) : 102.765

# Benchmarking PyTorch: eq
# Mode: Eager
# Name: eq_N64_dtypetorch.qint8_contigTrue_other_scalarFalse_out_variantTrue
# Input: N: 64, dtype: torch.qint8, contig: True, other_scalar: False, out_variant: True
Run: 0, Forward Execution Time (us) : 136.369
Run: 1, Forward Execution Time (us) : 136.571
Run: 2, Forward Execution Time (us) : 136.525

# Benchmarking PyTorch: eq
# Mode: Eager
# Name: eq_N64_dtypetorch.qint8_contigTrue_other_scalarTrue_out_variantFalse
# Input: N: 64, dtype: torch.qint8, contig: True, other_scalar: True, out_variant: False
Run: 0, Forward Execution Time (us) : 73.883
Run: 1, Forward Execution Time (us) : 74.094
Run: 2, Forward Execution Time (us) : 74.250

# Benchmarking PyTorch: eq
# Mode: Eager
# Name: eq_N64_dtypetorch.qint8_contigTrue_other_scalarTrue_out_variantTrue
# Input: N: 64, dtype: torch.qint8, contig: True, other_scalar: True, out_variant: True
Run: 0, Forward Execution Time (us) : 91.337
Run: 1, Forward Execution Time (us) : 91.709
Run: 2, Forward Execution Time (us) : 91.572

# Benchmarking PyTorch: eq
# Mode: Eager
# Name: eq_N64_dtypetorch.qint32_contigFalse_other_scalarFalse_out_variantFalse
# Input: N: 64, dtype: torch.qint32, contig: False, other_scalar: False, out_variant: False
Run: 0, Forward Execution Time (us) : 130.214
Run: 1, Forward Execution Time (us) : 130.347
Run: 2, Forward Execution Time (us) : 130.398

# Benchmarking PyTorch: eq
# Mode: Eager
# Name: eq_N64_dtypetorch.qint32_contigFalse_other_scalarFalse_out_variantTrue
# Input: N: 64, dtype: torch.qint32, contig: False, other_scalar: False, out_variant: True
Run: 0, Forward Execution Time (us) : 148.116
Run: 1, Forward Execution Time (us) : 148.373
Run: 2, Forward Execution Time (us) : 148.457

# Benchmarking PyTorch: eq
# Mode: Eager
# Name: eq_N64_dtypetorch.qint32_contigFalse_other_scalarTrue_out_variantFalse
# Input: N: 64, dtype: torch.qint32, contig: False, other_scalar: True, out_variant: False
Run: 0, Forward Execution Time (us) : 84.381
Run: 1, Forward Execution Time (us) : 84.474
Run: 2, Forward Execution Time (us) : 84.552

# Benchmarking PyTorch: eq
# Mode: Eager
# Name: eq_N64_dtypetorch.qint32_contigFalse_other_scalarTrue_out_variantTrue
# Input: N: 64, dtype: torch.qint32, contig: False, other_scalar: True, out_variant: True
Run: 0, Forward Execution Time (us) : 102.483
Run: 1, Forward Execution Time (us) : 102.740
Run: 2, Forward Execution Time (us) : 102.900

# Benchmarking PyTorch: eq
# Mode: Eager
# Name: eq_N64_dtypetorch.qint32_contigTrue_other_scalarFalse_out_variantFalse
# Input: N: 64, dtype: torch.qint32, contig: True, other_scalar: False, out_variant: False
Run: 0, Forward Execution Time (us) : 101.619
Run: 1, Forward Execution Time (us) : 103.407
Run: 2, Forward Execution Time (us) : 102.849

# Benchmarking PyTorch: eq
# Mode: Eager
# Name: eq_N64_dtypetorch.qint32_contigTrue_other_scalarFalse_out_variantTrue
# Input: N: 64, dtype: torch.qint32, contig: True, other_scalar: False, out_variant: True
Run: 0, Forward Execution Time (us) : 137.370
Run: 1, Forward Execution Time (us) : 137.208
Run: 2, Forward Execution Time (us) : 137.215

# Benchmarking PyTorch: eq
# Mode: Eager
# Name: eq_N64_dtypetorch.qint32_contigTrue_other_scalarTrue_out_variantFalse
# Input: N: 64, dtype: torch.qint32, contig: True, other_scalar: True, out_variant: False
Run: 0, Forward Execution Time (us) : 74.224
Run: 1, Forward Execution Time (us) : 74.395
Run: 2, Forward Execution Time (us) : 74.527

# Benchmarking PyTorch: eq
# Mode: Eager
# Name: eq_N64_dtypetorch.qint32_contigTrue_other_scalarTrue_out_variantTrue
# Input: N: 64, dtype: torch.qint32, contig: True, other_scalar: True, out_variant: True
Run: 0, Forward Execution Time (us) : 91.732
Run: 1, Forward Execution Time (us) : 91.902
Run: 2, Forward Execution Time (us) : 91.935

# Benchmarking PyTorch: ne
# Mode: Eager
# Name: ne_N8_dtypetorch.quint8_contigFalse_other_scalarFalse_out_variantFalse
# Input: N: 8, dtype: torch.quint8, contig: False, other_scalar: False, out_variant: False
Run: 0, Forward Execution Time (us) : 22.459
Run: 1, Forward Execution Time (us) : 22.546
Run: 2, Forward Execution Time (us) : 22.592

# Benchmarking PyTorch: ne
# Mode: Eager
# Name: ne_N8_dtypetorch.quint8_contigFalse_other_scalarFalse_out_variantTrue
# Input: N: 8, dtype: torch.quint8, contig: False, other_scalar: False, out_variant: True
Run: 0, Forward Execution Time (us) : 40.053
Run: 1, Forward Execution Time (us) : 40.266
Run: 2, Forward Execution Time (us) : 40.326

# Benchmarking PyTorch: ne
# Mode: Eager
# Name: ne_N8_dtypetorch.quint8_contigFalse_other_scalarTrue_out_variantFalse
# Input: N: 8, dtype: torch.quint8, contig: False, other_scalar: True, out_variant: False
Run: 0, Forward Execution Time (us) : 26.824
Run: 1, Forward Execution Time (us) : 26.853
Run: 2, Forward Execution Time (us) : 26.715

# Benchmarking PyTorch: ne
# Mode: Eager
# Name: ne_N8_dtypetorch.quint8_contigFalse_other_scalarTrue_out_variantTrue
# Input: N: 8, dtype: torch.quint8, contig: False, other_scalar: True, out_variant: True
Run: 0, Forward Execution Time (us) : 44.518
Run: 1, Forward Execution Time (us) : 44.767
Run: 2, Forward Execution Time (us) : 44.677

# Benchmarking PyTorch: ne
# Mode: Eager
# Name: ne_N8_dtypetorch.quint8_contigTrue_other_scalarFalse_out_variantFalse
# Input: N: 8, dtype: torch.quint8, contig: True, other_scalar: False, out_variant: False
Run: 0, Forward Execution Time (us) : 14.632
Run: 1, Forward Execution Time (us) : 14.982
Run: 2, Forward Execution Time (us) : 14.216

# Benchmarking PyTorch: ne
# Mode: Eager
# Name: ne_N8_dtypetorch.quint8_contigTrue_other_scalarFalse_out_variantTrue
# Input: N: 8, dtype: torch.quint8, contig: True, other_scalar: False, out_variant: True
Run: 0, Forward Execution Time (us) : 31.990
Run: 1, Forward Execution Time (us) : 32.278
Run: 2, Forward Execution Time (us) : 32.092

# Benchmarking PyTorch: ne
# Mode: Eager
# Name: ne_N8_dtypetorch.quint8_contigTrue_other_scalarTrue_out_variantFalse
# Input: N: 8, dtype: torch.quint8, contig: True, other_scalar: True, out_variant: False
Run: 0, Forward Execution Time (us) : 19.662
Run: 1, Forward Execution Time (us) : 19.859
Run: 2, Forward Execution Time (us) : 19.754

# Benchmarking PyTorch: ne
# Mode: Eager
# Name: ne_N8_dtypetorch.quint8_contigTrue_other_scalarTrue_out_variantTrue
# Input: N: 8, dtype: torch.quint8, contig: True, other_scalar: True, out_variant: True
Run: 0, Forward Execution Time (us) : 37.000
Run: 1, Forward Execution Time (us) : 37.141
Run: 2, Forward Execution Time (us) : 37.275

# Benchmarking PyTorch: ne
# Mode: Eager
# Name: ne_N8_dtypetorch.qint8_contigFalse_other_scalarFalse_out_variantFalse
# Input: N: 8, dtype: torch.qint8, contig: False, other_scalar: False, out_variant: False
Run: 0, Forward Execution Time (us) : 22.673
Run: 1, Forward Execution Time (us) : 22.678
Run: 2, Forward Execution Time (us) : 22.602

# Benchmarking PyTorch: ne
# Mode: Eager
# Name: ne_N8_dtypetorch.qint8_contigFalse_other_scalarFalse_out_variantTrue
# Input: N: 8, dtype: torch.qint8, contig: False, other_scalar: False, out_variant: True
Run: 0, Forward Execution Time (us) : 39.923
Run: 1, Forward Execution Time (us) : 40.361
Run: 2, Forward Execution Time (us) : 40.141

# Benchmarking PyTorch: ne
# Mode: Eager
# Name: ne_N8_dtypetorch.qint8_contigFalse_other_scalarTrue_out_variantFalse
# Input: N: 8, dtype: torch.qint8, contig: False, other_scalar: True, out_variant: False
Run: 0, Forward Execution Time (us) : 26.626
Run: 1, Forward Execution Time (us) : 26.761
Run: 2, Forward Execution Time (us) : 26.764

# Benchmarking PyTorch: ne
# Mode: Eager
# Name: ne_N8_dtypetorch.qint8_contigFalse_other_scalarTrue_out_variantTrue
# Input: N: 8, dtype: torch.qint8, contig: False, other_scalar: True, out_variant: True
Run: 0, Forward Execution Time (us) : 44.645
Run: 1, Forward Execution Time (us) : 44.755
Run: 2, Forward Execution Time (us) : 44.723

# Benchmarking PyTorch: ne
# Mode: Eager
# Name: ne_N8_dtypetorch.qint8_contigTrue_other_scalarFalse_out_variantFalse
# Input: N: 8, dtype: torch.qint8, contig: True, other_scalar: False, out_variant: False
Run: 0, Forward Execution Time (us) : 14.680
Run: 1, Forward Execution Time (us) : 15.041
Run: 2, Forward Execution Time (us) : 14.239

# Benchmarking PyTorch: ne
# Mode: Eager
# Name: ne_N8_dtypetorch.qint8_contigTrue_other_scalarFalse_out_variantTrue
# Input: N: 8, dtype: torch.qint8, contig: True, other_scalar: False, out_variant: True
Run: 0, Forward Execution Time (us) : 31.982
Run: 1, Forward Execution Time (us) : 32.231
Run: 2, Forward Execution Time (us) : 32.182

# Benchmarking PyTorch: ne
# Mode: Eager
# Name: ne_N8_dtypetorch.qint8_contigTrue_other_scalarTrue_out_variantFalse
# Input: N: 8, dtype: torch.qint8, contig: True, other_scalar: True, out_variant: False
Run: 0, Forward Execution Time (us) : 19.833
Run: 1, Forward Execution Time (us) : 19.877
Run: 2, Forward Execution Time (us) : 19.850

# Benchmarking PyTorch: ne
# Mode: Eager
# Name: ne_N8_dtypetorch.qint8_contigTrue_other_scalarTrue_out_variantTrue
# Input: N: 8, dtype: torch.qint8, contig: True, other_scalar: True, out_variant: True
Run: 0, Forward Execution Time (us) : 37.032
Run: 1, Forward Execution Time (us) : 37.077
Run: 2, Forward Execution Time (us) : 37.149

# Benchmarking PyTorch: ne
# Mode: Eager
# Name: ne_N8_dtypetorch.qint32_contigFalse_other_scalarFalse_out_variantFalse
# Input: N: 8, dtype: torch.qint32, contig: False, other_scalar: False, out_variant: False
Run: 0, Forward Execution Time (us) : 22.494
Run: 1, Forward Execution Time (us) : 22.564
Run: 2, Forward Execution Time (us) : 22.599

# Benchmarking PyTorch: ne
# Mode: Eager
# Name: ne_N8_dtypetorch.qint32_contigFalse_other_scalarFalse_out_variantTrue
# Input: N: 8, dtype: torch.qint32, contig: False, other_scalar: False, out_variant: True
Run: 0, Forward Execution Time (us) : 39.901
Run: 1, Forward Execution Time (us) : 40.298
Run: 2, Forward Execution Time (us) : 40.147

# Benchmarking PyTorch: ne
# Mode: Eager
# Name: ne_N8_dtypetorch.qint32_contigFalse_other_scalarTrue_out_variantFalse
# Input: N: 8, dtype: torch.qint32, contig: False, other_scalar: True, out_variant: False
Run: 0, Forward Execution Time (us) : 26.803
Run: 1, Forward Execution Time (us) : 26.896
Run: 2, Forward Execution Time (us) : 27.052

# Benchmarking PyTorch: ne
# Mode: Eager
# Name: ne_N8_dtypetorch.qint32_contigFalse_other_scalarTrue_out_variantTrue
# Input: N: 8, dtype: torch.qint32, contig: False, other_scalar: True, out_variant: True
Run: 0, Forward Execution Time (us) : 44.584
Run: 1, Forward Execution Time (us) : 44.910
Run: 2, Forward Execution Time (us) : 44.836

# Benchmarking PyTorch: ne
# Mode: Eager
# Name: ne_N8_dtypetorch.qint32_contigTrue_other_scalarFalse_out_variantFalse
# Input: N: 8, dtype: torch.qint32, contig: True, other_scalar: False, out_variant: False
Run: 0, Forward Execution Time (us) : 14.507
Run: 1, Forward Execution Time (us) : 15.002
Run: 2, Forward Execution Time (us) : 14.270

# Benchmarking PyTorch: ne
# Mode: Eager
# Name: ne_N8_dtypetorch.qint32_contigTrue_other_scalarFalse_out_variantTrue
# Input: N: 8, dtype: torch.qint32, contig: True, other_scalar: False, out_variant: True
Run: 0, Forward Execution Time (us) : 32.129
Run: 1, Forward Execution Time (us) : 32.085
Run: 2, Forward Execution Time (us) : 32.302

# Benchmarking PyTorch: ne
# Mode: Eager
# Name: ne_N8_dtypetorch.qint32_contigTrue_other_scalarTrue_out_variantFalse
# Input: N: 8, dtype: torch.qint32, contig: True, other_scalar: True, out_variant: False
Run: 0, Forward Execution Time (us) : 19.595
Run: 1, Forward Execution Time (us) : 19.720
Run: 2, Forward Execution Time (us) : 19.802

# Benchmarking PyTorch: ne
# Mode: Eager
# Name: ne_N8_dtypetorch.qint32_contigTrue_other_scalarTrue_out_variantTrue
# Input: N: 8, dtype: torch.qint32, contig: True, other_scalar: True, out_variant: True
Run: 0, Forward Execution Time (us) : 37.027
Run: 1, Forward Execution Time (us) : 37.320
Run: 2, Forward Execution Time (us) : 37.139

# Benchmarking PyTorch: ne
# Mode: Eager
# Name: ne_N64_dtypetorch.quint8_contigFalse_other_scalarFalse_out_variantFalse
# Input: N: 64, dtype: torch.quint8, contig: False, other_scalar: False, out_variant: False
Run: 0, Forward Execution Time (us) : 129.283
Run: 1, Forward Execution Time (us) : 129.510
Run: 2, Forward Execution Time (us) : 129.581

# Benchmarking PyTorch: ne
# Mode: Eager
# Name: ne_N64_dtypetorch.quint8_contigFalse_other_scalarFalse_out_variantTrue
# Input: N: 64, dtype: torch.quint8, contig: False, other_scalar: False, out_variant: True
Run: 0, Forward Execution Time (us) : 146.874
Run: 1, Forward Execution Time (us) : 147.180
Run: 2, Forward Execution Time (us) : 147.266

# Benchmarking PyTorch: ne
# Mode: Eager
# Name: ne_N64_dtypetorch.quint8_contigFalse_other_scalarTrue_out_variantFalse
# Input: N: 64, dtype: torch.quint8, contig: False, other_scalar: True, out_variant: False
Run: 0, Forward Execution Time (us) : 83.628
Run: 1, Forward Execution Time (us) : 83.831
Run: 2, Forward Execution Time (us) : 83.676

# Benchmarking PyTorch: ne
# Mode: Eager
# Name: ne_N64_dtypetorch.quint8_contigFalse_other_scalarTrue_out_variantTrue
# Input: N: 64, dtype: torch.quint8, contig: False, other_scalar: True, out_variant: True
Run: 0, Forward Execution Time (us) : 150.503
Run: 1, Forward Execution Time (us) : 102.342
Run: 2, Forward Execution Time (us) : 101.906

# Benchmarking PyTorch: ne
# Mode: Eager
# Name: ne_N64_dtypetorch.quint8_contigTrue_other_scalarFalse_out_variantFalse
# Input: N: 64, dtype: torch.quint8, contig: True, other_scalar: False, out_variant: False
Run: 0, Forward Execution Time (us) : 103.076
Run: 1, Forward Execution Time (us) : 102.600
Run: 2, Forward Execution Time (us) : 102.768

# Benchmarking PyTorch: ne
# Mode: Eager
# Name: ne_N64_dtypetorch.quint8_contigTrue_other_scalarFalse_out_variantTrue
# Input: N: 64, dtype: torch.quint8, contig: True, other_scalar: False, out_variant: True
Run: 0, Forward Execution Time (us) : 136.563
Run: 1, Forward Execution Time (us) : 136.531
Run: 2, Forward Execution Time (us) : 136.732

# Benchmarking PyTorch: ne
# Mode: Eager
# Name: ne_N64_dtypetorch.quint8_contigTrue_other_scalarTrue_out_variantFalse
# Input: N: 64, dtype: torch.quint8, contig: True, other_scalar: True, out_variant: False
Run: 0, Forward Execution Time (us) : 74.068
Run: 1, Forward Execution Time (us) : 74.206
Run: 2, Forward Execution Time (us) : 74.229

# Benchmarking PyTorch: ne
# Mode: Eager
# Name: ne_N64_dtypetorch.quint8_contigTrue_other_scalarTrue_out_variantTrue
# Input: N: 64, dtype: torch.quint8, contig: True, other_scalar: True, out_variant: True
Run: 0, Forward Execution Time (us) : 91.295
Run: 1, Forward Execution Time (us) : 91.451
Run: 2, Forward Execution Time (us) : 91.578

# Benchmarking PyTorch: ne
# Mode: Eager
# Name: ne_N64_dtypetorch.qint8_contigFalse_other_scalarFalse_out_variantFalse
# Input: N: 64, dtype: torch.qint8, contig: False, other_scalar: False, out_variant: False
Run: 0, Forward Execution Time (us) : 129.509
Run: 1, Forward Execution Time (us) : 129.641
Run: 2, Forward Execution Time (us) : 129.725

# Benchmarking PyTorch: ne
# Mode: Eager
# Name: ne_N64_dtypetorch.qint8_contigFalse_other_scalarFalse_out_variantTrue
# Input: N: 64, dtype: torch.qint8, contig: False, other_scalar: False, out_variant: True
Run: 0, Forward Execution Time (us) : 147.161
Run: 1, Forward Execution Time (us) : 147.378
Run: 2, Forward Execution Time (us) : 147.380

# Benchmarking PyTorch: ne
# Mode: Eager
# Name: ne_N64_dtypetorch.qint8_contigFalse_other_scalarTrue_out_variantFalse
# Input: N: 64, dtype: torch.qint8, contig: False, other_scalar: True, out_variant: False
Run: 0, Forward Execution Time (us) : 83.919
Run: 1, Forward Execution Time (us) : 83.969
Run: 2, Forward Execution Time (us) : 83.879

# Benchmarking PyTorch: ne
# Mode: Eager
# Name: ne_N64_dtypetorch.qint8_contigFalse_other_scalarTrue_out_variantTrue
# Input: N: 64, dtype: torch.qint8, contig: False, other_scalar: True, out_variant: True
Run: 0, Forward Execution Time (us) : 101.839
Run: 1, Forward Execution Time (us) : 102.217
Run: 2, Forward Execution Time (us) : 102.167

# Benchmarking PyTorch: ne
# Mode: Eager
# Name: ne_N64_dtypetorch.qint8_contigTrue_other_scalarFalse_out_variantFalse
# Input: N: 64, dtype: torch.qint8, contig: True, other_scalar: False, out_variant: False
Run: 0, Forward Execution Time (us) : 101.311
Run: 1, Forward Execution Time (us) : 103.146
Run: 2, Forward Execution Time (us) : 102.429

# Benchmarking PyTorch: ne
# Mode: Eager
# Name: ne_N64_dtypetorch.qint8_contigTrue_other_scalarFalse_out_variantTrue
# Input: N: 64, dtype: torch.qint8, contig: True, other_scalar: False, out_variant: True
Run: 0, Forward Execution Time (us) : 136.637
Run: 1, Forward Execution Time (us) : 136.574
Run: 2, Forward Execution Time (us) : 136.526

# Benchmarking PyTorch: ne
# Mode: Eager
# Name: ne_N64_dtypetorch.qint8_contigTrue_other_scalarTrue_out_variantFalse
# Input: N: 64, dtype: torch.qint8, contig: True, other_scalar: True, out_variant: False
Run: 0, Forward Execution Time (us) : 74.062
Run: 1, Forward Execution Time (us) : 74.086
Run: 2, Forward Execution Time (us) : 74.110

# Benchmarking PyTorch: ne
# Mode: Eager
# Name: ne_N64_dtypetorch.qint8_contigTrue_other_scalarTrue_out_variantTrue
# Input: N: 64, dtype: torch.qint8, contig: True, other_scalar: True, out_variant: True
Run: 0, Forward Execution Time (us) : 91.210
Run: 1, Forward Execution Time (us) : 91.339
Run: 2, Forward Execution Time (us) : 91.284

# Benchmarking PyTorch: ne
# Mode: Eager
# Name: ne_N64_dtypetorch.qint32_contigFalse_other_scalarFalse_out_variantFalse
# Input: N: 64, dtype: torch.qint32, contig: False, other_scalar: False, out_variant: False
Run: 0, Forward Execution Time (us) : 130.445
Run: 1, Forward Execution Time (us) : 130.495
Run: 2, Forward Execution Time (us) : 130.583

# Benchmarking PyTorch: ne
# Mode: Eager
# Name: ne_N64_dtypetorch.qint32_contigFalse_other_scalarFalse_out_variantTrue
# Input: N: 64, dtype: torch.qint32, contig: False, other_scalar: False, out_variant: True
Run: 0, Forward Execution Time (us) : 147.876
Run: 1, Forward Execution Time (us) : 148.116
Run: 2, Forward Execution Time (us) : 147.998

# Benchmarking PyTorch: ne
# Mode: Eager
# Name: ne_N64_dtypetorch.qint32_contigFalse_other_scalarTrue_out_variantFalse
# Input: N: 64, dtype: torch.qint32, contig: False, other_scalar: True, out_variant: False
Run: 0, Forward Execution Time (us) : 84.438
Run: 1, Forward Execution Time (us) : 84.324
Run: 2, Forward Execution Time (us) : 84.417

# Benchmarking PyTorch: ne
# Mode: Eager
# Name: ne_N64_dtypetorch.qint32_contigFalse_other_scalarTrue_out_variantTrue
# Input: N: 64, dtype: torch.qint32, contig: False, other_scalar: True, out_variant: True
Run: 0, Forward Execution Time (us) : 102.301
Run: 1, Forward Execution Time (us) : 102.478
Run: 2, Forward Execution Time (us) : 102.586

# Benchmarking PyTorch: ne
# Mode: Eager
# Name: ne_N64_dtypetorch.qint32_contigTrue_other_scalarFalse_out_variantFalse
# Input: N: 64, dtype: torch.qint32, contig: True, other_scalar: False, out_variant: False
Run: 0, Forward Execution Time (us) : 101.652
Run: 1, Forward Execution Time (us) : 103.374
Run: 2, Forward Execution Time (us) : 102.948

# Benchmarking PyTorch: ne
# Mode: Eager
# Name: ne_N64_dtypetorch.qint32_contigTrue_other_scalarFalse_out_variantTrue
# Input: N: 64, dtype: torch.qint32, contig: True, other_scalar: False, out_variant: True
Run: 0, Forward Execution Time (us) : 137.124
Run: 1, Forward Execution Time (us) : 137.179
Run: 2, Forward Execution Time (us) : 137.314

# Benchmarking PyTorch: ne
# Mode: Eager
# Name: ne_N64_dtypetorch.qint32_contigTrue_other_scalarTrue_out_variantFalse
# Input: N: 64, dtype: torch.qint32, contig: True, other_scalar: True, out_variant: False
Run: 0, Forward Execution Time (us) : 74.429
Run: 1, Forward Execution Time (us) : 74.488
Run: 2, Forward Execution Time (us) : 74.606

# Benchmarking PyTorch: ne
# Mode: Eager
# Name: ne_N64_dtypetorch.qint32_contigTrue_other_scalarTrue_out_variantTrue
# Input: N: 64, dtype: torch.qint32, contig: True, other_scalar: True, out_variant: True
Run: 0, Forward Execution Time (us) : 91.549
Run: 1, Forward Execution Time (us) : 91.807
Run: 2, Forward Execution Time (us) : 91.786

# Benchmarking PyTorch: lt
# Mode: Eager
# Name: lt_N8_dtypetorch.quint8_contigFalse_other_scalarFalse_out_variantFalse
# Input: N: 8, dtype: torch.quint8, contig: False, other_scalar: False, out_variant: False
Run: 0, Forward Execution Time (us) : 22.770
Run: 1, Forward Execution Time (us) : 22.422
Run: 2, Forward Execution Time (us) : 22.519

# Benchmarking PyTorch: lt
# Mode: Eager
# Name: lt_N8_dtypetorch.quint8_contigFalse_other_scalarFalse_out_variantTrue
# Input: N: 8, dtype: torch.quint8, contig: False, other_scalar: False, out_variant: True
Run: 0, Forward Execution Time (us) : 40.182
Run: 1, Forward Execution Time (us) : 40.821
Run: 2, Forward Execution Time (us) : 40.450

# Benchmarking PyTorch: lt
# Mode: Eager
# Name: lt_N8_dtypetorch.quint8_contigFalse_other_scalarTrue_out_variantFalse
# Input: N: 8, dtype: torch.quint8, contig: False, other_scalar: True, out_variant: False
Run: 0, Forward Execution Time (us) : 27.052
Run: 1, Forward Execution Time (us) : 26.750
Run: 2, Forward Execution Time (us) : 26.546

# Benchmarking PyTorch: lt
# Mode: Eager
# Name: lt_N8_dtypetorch.quint8_contigFalse_other_scalarTrue_out_variantTrue
# Input: N: 8, dtype: torch.quint8, contig: False, other_scalar: True, out_variant: True
Run: 0, Forward Execution Time (us) : 44.642
Run: 1, Forward Execution Time (us) : 44.936
Run: 2, Forward Execution Time (us) : 44.954

# Benchmarking PyTorch: lt
# Mode: Eager
# Name: lt_N8_dtypetorch.quint8_contigTrue_other_scalarFalse_out_variantFalse
# Input: N: 8, dtype: torch.quint8, contig: True, other_scalar: False, out_variant: False
Run: 0, Forward Execution Time (us) : 15.137
Run: 1, Forward Execution Time (us) : 14.189
Run: 2, Forward Execution Time (us) : 14.353

# Benchmarking PyTorch: lt
# Mode: Eager
# Name: lt_N8_dtypetorch.quint8_contigTrue_other_scalarFalse_out_variantTrue
# Input: N: 8, dtype: torch.quint8, contig: True, other_scalar: False, out_variant: True
Run: 0, Forward Execution Time (us) : 32.172
Run: 1, Forward Execution Time (us) : 32.486
Run: 2, Forward Execution Time (us) : 32.445

# Benchmarking PyTorch: lt
# Mode: Eager
# Name: lt_N8_dtypetorch.quint8_contigTrue_other_scalarTrue_out_variantFalse
# Input: N: 8, dtype: torch.quint8, contig: True, other_scalar: True, out_variant: False
Run: 0, Forward Execution Time (us) : 19.650
Run: 1, Forward Execution Time (us) : 19.784
Run: 2, Forward Execution Time (us) : 19.839

# Benchmarking PyTorch: lt
# Mode: Eager
# Name: lt_N8_dtypetorch.quint8_contigTrue_other_scalarTrue_out_variantTrue
# Input: N: 8, dtype: torch.quint8, contig: True, other_scalar: True, out_variant: True
Run: 0, Forward Execution Time (us) : 36.990
Run: 1, Forward Execution Time (us) : 37.270
Run: 2, Forward Execution Time (us) : 37.151

# Benchmarking PyTorch: lt
# Mode: Eager
# Name: lt_N8_dtypetorch.qint8_contigFalse_other_scalarFalse_out_variantFalse
# Input: N: 8, dtype: torch.qint8, contig: False, other_scalar: False, out_variant: False
Run: 0, Forward Execution Time (us) : 22.440
Run: 1, Forward Execution Time (us) : 22.651
Run: 2, Forward Execution Time (us) : 22.442

# Benchmarking PyTorch: lt
# Mode: Eager
# Name: lt_N8_dtypetorch.qint8_contigFalse_other_scalarFalse_out_variantTrue
# Input: N: 8, dtype: torch.qint8, contig: False, other_scalar: False, out_variant: True
Run: 0, Forward Execution Time (us) : 40.270
Run: 1, Forward Execution Time (us) : 40.407
Run: 2, Forward Execution Time (us) : 40.474

# Benchmarking PyTorch: lt
# Mode: Eager
# Name: lt_N8_dtypetorch.qint8_contigFalse_other_scalarTrue_out_variantFalse
# Input: N: 8, dtype: torch.qint8, contig: False, other_scalar: True, out_variant: False
Run: 0, Forward Execution Time (us) : 26.532
Run: 1, Forward Execution Time (us) : 26.724
Run: 2, Forward Execution Time (us) : 26.764

# Benchmarking PyTorch: lt
# Mode: Eager
# Name: lt_N8_dtypetorch.qint8_contigFalse_other_scalarTrue_out_variantTrue
# Input: N: 8, dtype: torch.qint8, contig: False, other_scalar: True, out_variant: True
Run: 0, Forward Execution Time (us) : 44.705
Run: 1, Forward Execution Time (us) : 44.975
Run: 2, Forward Execution Time (us) : 44.802

# Benchmarking PyTorch: lt
# Mode: Eager
# Name: lt_N8_dtypetorch.qint8_contigTrue_other_scalarFalse_out_variantFalse
# Input: N: 8, dtype: torch.qint8, contig: True, other_scalar: False, out_variant: False
Run: 0, Forward Execution Time (us) : 14.822
Run: 1, Forward Execution Time (us) : 14.198
Run: 2, Forward Execution Time (us) : 14.164

# Benchmarking PyTorch: lt
# Mode: Eager
# Name: lt_N8_dtypetorch.qint8_contigTrue_other_scalarFalse_out_variantTrue
# Input: N: 8, dtype: torch.qint8, contig: True, other_scalar: False, out_variant: True
Run: 0, Forward Execution Time (us) : 32.203
Run: 1, Forward Execution Time (us) : 32.482
Run: 2, Forward Execution Time (us) : 32.451

# Benchmarking PyTorch: lt
# Mode: Eager
# Name: lt_N8_dtypetorch.qint8_contigTrue_other_scalarTrue_out_variantFalse
# Input: N: 8, dtype: torch.qint8, contig: True, other_scalar: True, out_variant: False
Run: 0, Forward Execution Time (us) : 19.671
Run: 1, Forward Execution Time (us) : 19.654
Run: 2, Forward Execution Time (us) : 19.778

# Benchmarking PyTorch: lt
# Mode: Eager
# Name: lt_N8_dtypetorch.qint8_contigTrue_other_scalarTrue_out_variantTrue
# Input: N: 8, dtype: torch.qint8, contig: True, other_scalar: True, out_variant: True
Run: 0, Forward Execution Time (us) : 36.996
Run: 1, Forward Execution Time (us) : 37.124
Run: 2, Forward Execution Time (us) : 37.047

# Benchmarking PyTorch: lt
# Mode: Eager
# Name: lt_N8_dtypetorch.qint32_contigFalse_other_scalarFalse_out_variantFalse
# Input: N: 8, dtype: torch.qint32, contig: False, other_scalar: False, out_variant: False
Run: 0, Forward Execution Time (us) : 22.456
Run: 1, Forward Execution Time (us) : 22.632
Run: 2, Forward Execution Time (us) : 22.531

# Benchmarking PyTorch: lt
# Mode: Eager
# Name: lt_N8_dtypetorch.qint32_contigFalse_other_scalarFalse_out_variantTrue
# Input: N: 8, dtype: torch.qint32, contig: False, other_scalar: False, out_variant: True
Run: 0, Forward Execution Time (us) : 40.243
Run: 1, Forward Execution Time (us) : 40.496
Run: 2, Forward Execution Time (us) : 40.439

# Benchmarking PyTorch: lt
# Mode: Eager
# Name: lt_N8_dtypetorch.qint32_contigFalse_other_scalarTrue_out_variantFalse
# Input: N: 8, dtype: torch.qint32, contig: False, other_scalar: True, out_variant: False
Run: 0, Forward Execution Time (us) : 26.733
Run: 1, Forward Execution Time (us) : 26.857
Run: 2, Forward Execution Time (us) : 26.919

# Benchmarking PyTorch: lt
# Mode: Eager
# Name: lt_N8_dtypetorch.qint32_contigFalse_other_scalarTrue_out_variantTrue
# Input: N: 8, dtype: torch.qint32, contig: False, other_scalar: True, out_variant: True
Run: 0, Forward Execution Time (us) : 44.510
Run: 1, Forward Execution Time (us) : 44.820
Run: 2, Forward Execution Time (us) : 44.688

# Benchmarking PyTorch: lt
# Mode: Eager
# Name: lt_N8_dtypetorch.qint32_contigTrue_other_scalarFalse_out_variantFalse
# Input: N: 8, dtype: torch.qint32, contig: True, other_scalar: False, out_variant: False
Run: 0, Forward Execution Time (us) : 14.862
Run: 1, Forward Execution Time (us) : 15.274
Run: 2, Forward Execution Time (us) : 14.229

# Benchmarking PyTorch: lt
# Mode: Eager
# Name: lt_N8_dtypetorch.qint32_contigTrue_other_scalarFalse_out_variantTrue
# Input: N: 8, dtype: torch.qint32, contig: True, other_scalar: False, out_variant: True
Run: 0, Forward Execution Time (us) : 32.262
Run: 1, Forward Execution Time (us) : 32.485
Run: 2, Forward Execution Time (us) : 32.412

# Benchmarking PyTorch: lt
# Mode: Eager
# Name: lt_N8_dtypetorch.qint32_contigTrue_other_scalarTrue_out_variantFalse
# Input: N: 8, dtype: torch.qint32, contig: True, other_scalar: True, out_variant: False
Run: 0, Forward Execution Time (us) : 19.663
Run: 1, Forward Execution Time (us) : 19.870
Run: 2, Forward Execution Time (us) : 19.823

# Benchmarking PyTorch: lt
# Mode: Eager
# Name: lt_N8_dtypetorch.qint32_contigTrue_other_scalarTrue_out_variantTrue
# Input: N: 8, dtype: torch.qint32, contig: True, other_scalar: True, out_variant: True
Run: 0, Forward Execution Time (us) : 36.965
Run: 1, Forward Execution Time (us) : 37.269
Run: 2, Forward Execution Time (us) : 37.179

# Benchmarking PyTorch: lt
# Mode: Eager
# Name: lt_N64_dtypetorch.quint8_contigFalse_other_scalarFalse_out_variantFalse
# Input: N: 64, dtype: torch.quint8, contig: False, other_scalar: False, out_variant: False
Run: 0, Forward Execution Time (us) : 129.340
Run: 1, Forward Execution Time (us) : 129.471
Run: 2, Forward Execution Time (us) : 129.547

# Benchmarking PyTorch: lt
# Mode: Eager
# Name: lt_N64_dtypetorch.quint8_contigFalse_other_scalarFalse_out_variantTrue
# Input: N: 64, dtype: torch.quint8, contig: False, other_scalar: False, out_variant: True
Run: 0, Forward Execution Time (us) : 147.135
Run: 1, Forward Execution Time (us) : 147.365
Run: 2, Forward Execution Time (us) : 147.415

# Benchmarking PyTorch: lt
# Mode: Eager
# Name: lt_N64_dtypetorch.quint8_contigFalse_other_scalarTrue_out_variantFalse
# Input: N: 64, dtype: torch.quint8, contig: False, other_scalar: True, out_variant: False
Run: 0, Forward Execution Time (us) : 83.498
Run: 1, Forward Execution Time (us) : 83.620
Run: 2, Forward Execution Time (us) : 83.632

# Benchmarking PyTorch: lt
# Mode: Eager
# Name: lt_N64_dtypetorch.quint8_contigFalse_other_scalarTrue_out_variantTrue
# Input: N: 64, dtype: torch.quint8, contig: False, other_scalar: True, out_variant: True
Run: 0, Forward Execution Time (us) : 101.534
Run: 1, Forward Execution Time (us) : 101.965
Run: 2, Forward Execution Time (us) : 101.812

# Benchmarking PyTorch: lt
# Mode: Eager
# Name: lt_N64_dtypetorch.quint8_contigTrue_other_scalarFalse_out_variantFalse
# Input: N: 64, dtype: torch.quint8, contig: True, other_scalar: False, out_variant: False
Run: 0, Forward Execution Time (us) : 101.500
Run: 1, Forward Execution Time (us) : 103.510
Run: 2, Forward Execution Time (us) : 102.717

# Benchmarking PyTorch: lt
# Mode: Eager
# Name: lt_N64_dtypetorch.quint8_contigTrue_other_scalarFalse_out_variantTrue
# Input: N: 64, dtype: torch.quint8, contig: True, other_scalar: False, out_variant: True
Run: 0, Forward Execution Time (us) : 136.497
Run: 1, Forward Execution Time (us) : 136.868
Run: 2, Forward Execution Time (us) : 136.736

# Benchmarking PyTorch: lt
# Mode: Eager
# Name: lt_N64_dtypetorch.quint8_contigTrue_other_scalarTrue_out_variantFalse
# Input: N: 64, dtype: torch.quint8, contig: True, other_scalar: True, out_variant: False
Run: 0, Forward Execution Time (us) : 74.047
Run: 1, Forward Execution Time (us) : 74.211
Run: 2, Forward Execution Time (us) : 74.039

# Benchmarking PyTorch: lt
# Mode: Eager
# Name: lt_N64_dtypetorch.quint8_contigTrue_other_scalarTrue_out_variantTrue
# Input: N: 64, dtype: torch.quint8, contig: True, other_scalar: True, out_variant: True
Run: 0, Forward Execution Time (us) : 91.255
Run: 1, Forward Execution Time (us) : 91.468
Run: 2, Forward Execution Time (us) : 91.413

# Benchmarking PyTorch: lt
# Mode: Eager
# Name: lt_N64_dtypetorch.qint8_contigFalse_other_scalarFalse_out_variantFalse
# Input: N: 64, dtype: torch.qint8, contig: False, other_scalar: False, out_variant: False
Run: 0, Forward Execution Time (us) : 129.597
Run: 1, Forward Execution Time (us) : 129.597
Run: 2, Forward Execution Time (us) : 129.674

# Benchmarking PyTorch: lt
# Mode: Eager
# Name: lt_N64_dtypetorch.qint8_contigFalse_other_scalarFalse_out_variantTrue
# Input: N: 64, dtype: torch.qint8, contig: False, other_scalar: False, out_variant: True
Run: 0, Forward Execution Time (us) : 147.590
Run: 1, Forward Execution Time (us) : 147.672
Run: 2, Forward Execution Time (us) : 147.768

# Benchmarking PyTorch: lt
# Mode: Eager
# Name: lt_N64_dtypetorch.qint8_contigFalse_other_scalarTrue_out_variantFalse
# Input: N: 64, dtype: torch.qint8, contig: False, other_scalar: True, out_variant: False
Run: 0, Forward Execution Time (us) : 83.608
Run: 1, Forward Execution Time (us) : 83.839
Run: 2, Forward Execution Time (us) : 83.882

# Benchmarking PyTorch: lt
# Mode: Eager
# Name: lt_N64_dtypetorch.qint8_contigFalse_other_scalarTrue_out_variantTrue
# Input: N: 64, dtype: torch.qint8, contig: False, other_scalar: True, out_variant: True
Run: 0, Forward Execution Time (us) : 101.553
Run: 1, Forward Execution Time (us) : 102.002
Run: 2, Forward Execution Time (us) : 102.077

# Benchmarking PyTorch: lt
# Mode: Eager
# Name: lt_N64_dtypetorch.qint8_contigTrue_other_scalarFalse_out_variantFalse
# Input: N: 64, dtype: torch.qint8, contig: True, other_scalar: False, out_variant: False
Run: 0, Forward Execution Time (us) : 101.542
Run: 1, Forward Execution Time (us) : 102.722
Run: 2, Forward Execution Time (us) : 103.325

# Benchmarking PyTorch: lt
# Mode: Eager
# Name: lt_N64_dtypetorch.qint8_contigTrue_other_scalarFalse_out_variantTrue
# Input: N: 64, dtype: torch.qint8, contig: True, other_scalar: False, out_variant: True
Run: 0, Forward Execution Time (us) : 136.526
Run: 1, Forward Execution Time (us) : 136.746
Run: 2, Forward Execution Time (us) : 136.676

# Benchmarking PyTorch: lt
# Mode: Eager
# Name: lt_N64_dtypetorch.qint8_contigTrue_other_scalarTrue_out_variantFalse
# Input: N: 64, dtype: torch.qint8, contig: True, other_scalar: True, out_variant: False
Run: 0, Forward Execution Time (us) : 73.867
Run: 1, Forward Execution Time (us) : 74.105
Run: 2, Forward Execution Time (us) : 74.191

# Benchmarking PyTorch: lt
# Mode: Eager
# Name: lt_N64_dtypetorch.qint8_contigTrue_other_scalarTrue_out_variantTrue
# Input: N: 64, dtype: torch.qint8, contig: True, other_scalar: True, out_variant: True
Run: 0, Forward Execution Time (us) : 91.231
Run: 1, Forward Execution Time (us) : 91.432
Run: 2, Forward Execution Time (us) : 91.599

# Benchmarking PyTorch: lt
# Mode: Eager
# Name: lt_N64_dtypetorch.qint32_contigFalse_other_scalarFalse_out_variantFalse
# Input: N: 64, dtype: torch.qint32, contig: False, other_scalar: False, out_variant: False
Run: 0, Forward Execution Time (us) : 130.589
Run: 1, Forward Execution Time (us) : 130.520
Run: 2, Forward Execution Time (us) : 130.668

# Benchmarking PyTorch: lt
# Mode: Eager
# Name: lt_N64_dtypetorch.qint32_contigFalse_other_scalarFalse_out_variantTrue
# Input: N: 64, dtype: torch.qint32, contig: False, other_scalar: False, out_variant: True
Run: 0, Forward Execution Time (us) : 148.045
Run: 1, Forward Execution Time (us) : 148.361
Run: 2, Forward Execution Time (us) : 148.277

# Benchmarking PyTorch: lt
# Mode: Eager
# Name: lt_N64_dtypetorch.qint32_contigFalse_other_scalarTrue_out_variantFalse
# Input: N: 64, dtype: torch.qint32, contig: False, other_scalar: True, out_variant: False
Run: 0, Forward Execution Time (us) : 84.344
Run: 1, Forward Execution Time (us) : 84.583
Run: 2, Forward Execution Time (us) : 84.622

# Benchmarking PyTorch: lt
# Mode: Eager
# Name: lt_N64_dtypetorch.qint32_contigFalse_other_scalarTrue_out_variantTrue
# Input: N: 64, dtype: torch.qint32, contig: False, other_scalar: True, out_variant: True
Run: 0, Forward Execution Time (us) : 102.511
Run: 1, Forward Execution Time (us) : 102.714
Run: 2, Forward Execution Time (us) : 102.567

# Benchmarking PyTorch: lt
# Mode: Eager
# Name: lt_N64_dtypetorch.qint32_contigTrue_other_scalarFalse_out_variantFalse
# Input: N: 64, dtype: torch.qint32, contig: True, other_scalar: False, out_variant: False
Run: 0, Forward Execution Time (us) : 101.837
Run: 1, Forward Execution Time (us) : 103.183
Run: 2, Forward Execution Time (us) : 103.693

# Benchmarking PyTorch: lt
# Mode: Eager
# Name: lt_N64_dtypetorch.qint32_contigTrue_other_scalarFalse_out_variantTrue
# Input: N: 64, dtype: torch.qint32, contig: True, other_scalar: False, out_variant: True
Run: 0, Forward Execution Time (us) : 137.395
Run: 1, Forward Execution Time (us) : 137.545
Run: 2, Forward Execution Time (us) : 137.351

# Benchmarking PyTorch: lt
# Mode: Eager
# Name: lt_N64_dtypetorch.qint32_contigTrue_other_scalarTrue_out_variantFalse
# Input: N: 64, dtype: torch.qint32, contig: True, other_scalar: True, out_variant: False
Run: 0, Forward Execution Time (us) : 74.291
Run: 1, Forward Execution Time (us) : 74.394
Run: 2, Forward Execution Time (us) : 74.395

# Benchmarking PyTorch: lt
# Mode: Eager
# Name: lt_N64_dtypetorch.qint32_contigTrue_other_scalarTrue_out_variantTrue
# Input: N: 64, dtype: torch.qint32, contig: True, other_scalar: True, out_variant: True
Run: 0, Forward Execution Time (us) : 91.517
Run: 1, Forward Execution Time (us) : 91.822
Run: 2, Forward Execution Time (us) : 91.686

# Benchmarking PyTorch: gt
# Mode: Eager
# Name: gt_N8_dtypetorch.quint8_contigFalse_other_scalarFalse_out_variantFalse
# Input: N: 8, dtype: torch.quint8, contig: False, other_scalar: False, out_variant: False
Run: 0, Forward Execution Time (us) : 22.489
Run: 1, Forward Execution Time (us) : 22.421
Run: 2, Forward Execution Time (us) : 22.459

# Benchmarking PyTorch: gt
# Mode: Eager
# Name: gt_N8_dtypetorch.quint8_contigFalse_other_scalarFalse_out_variantTrue
# Input: N: 8, dtype: torch.quint8, contig: False, other_scalar: False, out_variant: True
Run: 0, Forward Execution Time (us) : 40.156
Run: 1, Forward Execution Time (us) : 40.435
Run: 2, Forward Execution Time (us) : 40.456

# Benchmarking PyTorch: gt
# Mode: Eager
# Name: gt_N8_dtypetorch.quint8_contigFalse_other_scalarTrue_out_variantFalse
# Input: N: 8, dtype: torch.quint8, contig: False, other_scalar: True, out_variant: False
Run: 0, Forward Execution Time (us) : 26.449
Run: 1, Forward Execution Time (us) : 26.671
Run: 2, Forward Execution Time (us) : 26.570

# Benchmarking PyTorch: gt
# Mode: Eager
# Name: gt_N8_dtypetorch.quint8_contigFalse_other_scalarTrue_out_variantTrue
# Input: N: 8, dtype: torch.quint8, contig: False, other_scalar: True, out_variant: True
Run: 0, Forward Execution Time (us) : 44.508
Run: 1, Forward Execution Time (us) : 44.948
Run: 2, Forward Execution Time (us) : 44.890

# Benchmarking PyTorch: gt
# Mode: Eager
# Name: gt_N8_dtypetorch.quint8_contigTrue_other_scalarFalse_out_variantFalse
# Input: N: 8, dtype: torch.quint8, contig: True, other_scalar: False, out_variant: False
Run: 0, Forward Execution Time (us) : 14.069
Run: 1, Forward Execution Time (us) : 14.270
Run: 2, Forward Execution Time (us) : 14.687

# Benchmarking PyTorch: gt
# Mode: Eager
# Name: gt_N8_dtypetorch.quint8_contigTrue_other_scalarFalse_out_variantTrue
# Input: N: 8, dtype: torch.quint8, contig: True, other_scalar: False, out_variant: True
Run: 0, Forward Execution Time (us) : 32.072
Run: 1, Forward Execution Time (us) : 32.227
Run: 2, Forward Execution Time (us) : 32.260

# Benchmarking PyTorch: gt
# Mode: Eager
# Name: gt_N8_dtypetorch.quint8_contigTrue_other_scalarTrue_out_variantFalse
# Input: N: 8, dtype: torch.quint8, contig: True, other_scalar: True, out_variant: False
Run: 0, Forward Execution Time (us) : 19.644
Run: 1, Forward Execution Time (us) : 19.783
Run: 2, Forward Execution Time (us) : 19.788

# Benchmarking PyTorch: gt
# Mode: Eager
# Name: gt_N8_dtypetorch.quint8_contigTrue_other_scalarTrue_out_variantTrue
# Input: N: 8, dtype: torch.quint8, contig: True, other_scalar: True, out_variant: True
Run: 0, Forward Execution Time (us) : 36.908
Run: 1, Forward Execution Time (us) : 37.150
Run: 2, Forward Execution Time (us) : 37.123

# Benchmarking PyTorch: gt
# Mode: Eager
# Name: gt_N8_dtypetorch.qint8_contigFalse_other_scalarFalse_out_variantFalse
# Input: N: 8, dtype: torch.qint8, contig: False, other_scalar: False, out_variant: False
Run: 0, Forward Execution Time (us) : 22.466
Run: 1, Forward Execution Time (us) : 22.513
Run: 2, Forward Execution Time (us) : 22.483

# Benchmarking PyTorch: gt
# Mode: Eager
# Name: gt_N8_dtypetorch.qint8_contigFalse_other_scalarFalse_out_variantTrue
# Input: N: 8, dtype: torch.qint8, contig: False, other_scalar: False, out_variant: True
Run: 0, Forward Execution Time (us) : 40.068
Run: 1, Forward Execution Time (us) : 40.353
Run: 2, Forward Execution Time (us) : 40.426

# Benchmarking PyTorch: gt
# Mode: Eager
# Name: gt_N8_dtypetorch.qint8_contigFalse_other_scalarTrue_out_variantFalse
# Input: N: 8, dtype: torch.qint8, contig: False, other_scalar: True, out_variant: False
Run: 0, Forward Execution Time (us) : 26.524
Run: 1, Forward Execution Time (us) : 26.702
Run: 2, Forward Execution Time (us) : 26.651

# Benchmarking PyTorch: gt
# Mode: Eager
# Name: gt_N8_dtypetorch.qint8_contigFalse_other_scalarTrue_out_variantTrue
# Input: N: 8, dtype: torch.qint8, contig: False, other_scalar: True, out_variant: True
Run: 0, Forward Execution Time (us) : 44.435
Run: 1, Forward Execution Time (us) : 45.023
Run: 2, Forward Execution Time (us) : 44.973

# Benchmarking PyTorch: gt
# Mode: Eager
# Name: gt_N8_dtypetorch.qint8_contigTrue_other_scalarFalse_out_variantFalse
# Input: N: 8, dtype: torch.qint8, contig: True, other_scalar: False, out_variant: False
Run: 0, Forward Execution Time (us) : 14.153
Run: 1, Forward Execution Time (us) : 14.194
Run: 2, Forward Execution Time (us) : 14.631

# Benchmarking PyTorch: gt
# Mode: Eager
# Name: gt_N8_dtypetorch.qint8_contigTrue_other_scalarFalse_out_variantTrue
# Input: N: 8, dtype: torch.qint8, contig: True, other_scalar: False, out_variant: True
Run: 0, Forward Execution Time (us) : 32.100
Run: 1, Forward Execution Time (us) : 32.417
Run: 2, Forward Execution Time (us) : 32.352

# Benchmarking PyTorch: gt
# Mode: Eager
# Name: gt_N8_dtypetorch.qint8_contigTrue_other_scalarTrue_out_variantFalse
# Input: N: 8, dtype: torch.qint8, contig: True, other_scalar: True, out_variant: False
Run: 0, Forward Execution Time (us) : 19.640
Run: 1, Forward Execution Time (us) : 19.806
Run: 2, Forward Execution Time (us) : 19.806

# Benchmarking PyTorch: gt
# Mode: Eager
# Name: gt_N8_dtypetorch.qint8_contigTrue_other_scalarTrue_out_variantTrue
# Input: N: 8, dtype: torch.qint8, contig: True, other_scalar: True, out_variant: True
Run: 0, Forward Execution Time (us) : 37.114
Run: 1, Forward Execution Time (us) : 37.366
Run: 2, Forward Execution Time (us) : 37.205

# Benchmarking PyTorch: gt
# Mode: Eager
# Name: gt_N8_dtypetorch.qint32_contigFalse_other_scalarFalse_out_variantFalse
# Input: N: 8, dtype: torch.qint32, contig: False, other_scalar: False, out_variant: False
Run: 0, Forward Execution Time (us) : 22.596
Run: 1, Forward Execution Time (us) : 22.665
Run: 2, Forward Execution Time (us) : 22.567

# Benchmarking PyTorch: gt
# Mode: Eager
# Name: gt_N8_dtypetorch.qint32_contigFalse_other_scalarFalse_out_variantTrue
# Input: N: 8, dtype: torch.qint32, contig: False, other_scalar: False, out_variant: True
Run: 0, Forward Execution Time (us) : 40.268
Run: 1, Forward Execution Time (us) : 40.643
Run: 2, Forward Execution Time (us) : 40.451

# Benchmarking PyTorch: gt
# Mode: Eager
# Name: gt_N8_dtypetorch.qint32_contigFalse_other_scalarTrue_out_variantFalse
# Input: N: 8, dtype: torch.qint32, contig: False, other_scalar: True, out_variant: False
Run: 0, Forward Execution Time (us) : 26.672
Run: 1, Forward Execution Time (us) : 26.886
Run: 2, Forward Execution Time (us) : 26.834

# Benchmarking PyTorch: gt
# Mode: Eager
# Name: gt_N8_dtypetorch.qint32_contigFalse_other_scalarTrue_out_variantTrue
# Input: N: 8, dtype: torch.qint32, contig: False, other_scalar: True, out_variant: True
Run: 0, Forward Execution Time (us) : 44.543
Run: 1, Forward Execution Time (us) : 44.925
Run: 2, Forward Execution Time (us) : 44.946

# Benchmarking PyTorch: gt
# Mode: Eager
# Name: gt_N8_dtypetorch.qint32_contigTrue_other_scalarFalse_out_variantFalse
# Input: N: 8, dtype: torch.qint32, contig: True, other_scalar: False, out_variant: False
Run: 0, Forward Execution Time (us) : 14.192
Run: 1, Forward Execution Time (us) : 14.273
Run: 2, Forward Execution Time (us) : 14.648

# Benchmarking PyTorch: gt
# Mode: Eager
# Name: gt_N8_dtypetorch.qint32_contigTrue_other_scalarFalse_out_variantTrue
# Input: N: 8, dtype: torch.qint32, contig: True, other_scalar: False, out_variant: True
Run: 0, Forward Execution Time (us) : 32.071
Run: 1, Forward Execution Time (us) : 32.506
Run: 2, Forward Execution Time (us) : 32.301

# Benchmarking PyTorch: gt
# Mode: Eager
# Name: gt_N8_dtypetorch.qint32_contigTrue_other_scalarTrue_out_variantFalse
# Input: N: 8, dtype: torch.qint32, contig: True, other_scalar: True, out_variant: False
Run: 0, Forward Execution Time (us) : 19.629
Run: 1, Forward Execution Time (us) : 19.782
Run: 2, Forward Execution Time (us) : 19.810

# Benchmarking PyTorch: gt
# Mode: Eager
# Name: gt_N8_dtypetorch.qint32_contigTrue_other_scalarTrue_out_variantTrue
# Input: N: 8, dtype: torch.qint32, contig: True, other_scalar: True, out_variant: True
Run: 0, Forward Execution Time (us) : 36.993
Run: 1, Forward Execution Time (us) : 37.303
Run: 2, Forward Execution Time (us) : 37.257

# Benchmarking PyTorch: gt
# Mode: Eager
# Name: gt_N64_dtypetorch.quint8_contigFalse_other_scalarFalse_out_variantFalse
# Input: N: 64, dtype: torch.quint8, contig: False, other_scalar: False, out_variant: False
Run: 0, Forward Execution Time (us) : 162.172
Run: 1, Forward Execution Time (us) : 129.648
Run: 2, Forward Execution Time (us) : 129.583

# Benchmarking PyTorch: gt
# Mode: Eager
# Name: gt_N64_dtypetorch.quint8_contigFalse_other_scalarFalse_out_variantTrue
# Input: N: 64, dtype: torch.quint8, contig: False, other_scalar: False, out_variant: True
Run: 0, Forward Execution Time (us) : 147.285
Run: 1, Forward Execution Time (us) : 147.472
Run: 2, Forward Execution Time (us) : 147.546

# Benchmarking PyTorch: gt
# Mode: Eager
# Name: gt_N64_dtypetorch.quint8_contigFalse_other_scalarTrue_out_variantFalse
# Input: N: 64, dtype: torch.quint8, contig: False, other_scalar: True, out_variant: False
Run: 0, Forward Execution Time (us) : 83.725
Run: 1, Forward Execution Time (us) : 83.862
Run: 2, Forward Execution Time (us) : 83.844

# Benchmarking PyTorch: gt
# Mode: Eager
# Name: gt_N64_dtypetorch.quint8_contigFalse_other_scalarTrue_out_variantTrue
# Input: N: 64, dtype: torch.quint8, contig: False, other_scalar: True, out_variant: True
Run: 0, Forward Execution Time (us) : 101.486
Run: 1, Forward Execution Time (us) : 101.878
Run: 2, Forward Execution Time (us) : 101.815

# Benchmarking PyTorch: gt
# Mode: Eager
# Name: gt_N64_dtypetorch.quint8_contigTrue_other_scalarFalse_out_variantFalse
# Input: N: 64, dtype: torch.quint8, contig: True, other_scalar: False, out_variant: False
Run: 0, Forward Execution Time (us) : 101.302
Run: 1, Forward Execution Time (us) : 102.831
Run: 2, Forward Execution Time (us) : 102.533

# Benchmarking PyTorch: gt
# Mode: Eager
# Name: gt_N64_dtypetorch.quint8_contigTrue_other_scalarFalse_out_variantTrue
# Input: N: 64, dtype: torch.quint8, contig: True, other_scalar: False, out_variant: True
Run: 0, Forward Execution Time (us) : 136.497
Run: 1, Forward Execution Time (us) : 136.741
Run: 2, Forward Execution Time (us) : 136.652

# Benchmarking PyTorch: gt
# Mode: Eager
# Name: gt_N64_dtypetorch.quint8_contigTrue_other_scalarTrue_out_variantFalse
# Input: N: 64, dtype: torch.quint8, contig: True, other_scalar: True, out_variant: False
Run: 0, Forward Execution Time (us) : 73.946
Run: 1, Forward Execution Time (us) : 74.161
Run: 2, Forward Execution Time (us) : 74.100

# Benchmarking PyTorch: gt
# Mode: Eager
# Name: gt_N64_dtypetorch.quint8_contigTrue_other_scalarTrue_out_variantTrue
# Input: N: 64, dtype: torch.quint8, contig: True, other_scalar: True, out_variant: True
Run: 0, Forward Execution Time (us) : 91.166
Run: 1, Forward Execution Time (us) : 91.403
Run: 2, Forward Execution Time (us) : 91.608

# Benchmarking PyTorch: gt
# Mode: Eager
# Name: gt_N64_dtypetorch.qint8_contigFalse_other_scalarFalse_out_variantFalse
# Input: N: 64, dtype: torch.qint8, contig: False, other_scalar: False, out_variant: False
Run: 0, Forward Execution Time (us) : 129.536
Run: 1, Forward Execution Time (us) : 129.781
Run: 2, Forward Execution Time (us) : 129.819

# Benchmarking PyTorch: gt
# Mode: Eager
# Name: gt_N64_dtypetorch.qint8_contigFalse_other_scalarFalse_out_variantTrue
# Input: N: 64, dtype: torch.qint8, contig: False, other_scalar: False, out_variant: True
Run: 0, Forward Execution Time (us) : 147.486
Run: 1, Forward Execution Time (us) : 147.701
Run: 2, Forward Execution Time (us) : 147.755

# Benchmarking PyTorch: gt
# Mode: Eager
# Name: gt_N64_dtypetorch.qint8_contigFalse_other_scalarTrue_out_variantFalse
# Input: N: 64, dtype: torch.qint8, contig: False, other_scalar: True, out_variant: False
Run: 0, Forward Execution Time (us) : 83.925
Run: 1, Forward Execution Time (us) : 84.039
Run: 2, Forward Execution Time (us) : 84.044

# Benchmarking PyTorch: gt
# Mode: Eager
# Name: gt_N64_dtypetorch.qint8_contigFalse_other_scalarTrue_out_variantTrue
# Input: N: 64, dtype: torch.qint8, contig: False, other_scalar: True, out_variant: True
Run: 0, Forward Execution Time (us) : 101.837
Run: 1, Forward Execution Time (us) : 102.244
Run: 2, Forward Execution Time (us) : 102.282

# Benchmarking PyTorch: gt
# Mode: Eager
# Name: gt_N64_dtypetorch.qint8_contigTrue_other_scalarFalse_out_variantFalse
# Input: N: 64, dtype: torch.qint8, contig: True, other_scalar: False, out_variant: False
Run: 0, Forward Execution Time (us) : 101.198
Run: 1, Forward Execution Time (us) : 103.413
Run: 2, Forward Execution Time (us) : 102.445

# Benchmarking PyTorch: gt
# Mode: Eager
# Name: gt_N64_dtypetorch.qint8_contigTrue_other_scalarFalse_out_variantTrue
# Input: N: 64, dtype: torch.qint8, contig: True, other_scalar: False, out_variant: True
Run: 0, Forward Execution Time (us) : 136.373
Run: 1, Forward Execution Time (us) : 136.574
Run: 2, Forward Execution Time (us) : 136.514

# Benchmarking PyTorch: gt
# Mode: Eager
# Name: gt_N64_dtypetorch.qint8_contigTrue_other_scalarTrue_out_variantFalse
# Input: N: 64, dtype: torch.qint8, contig: True, other_scalar: True, out_variant: False
Run: 0, Forward Execution Time (us) : 73.919
Run: 1, Forward Execution Time (us) : 74.076
Run: 2, Forward Execution Time (us) : 73.984

# Benchmarking PyTorch: gt
# Mode: Eager
# Name: gt_N64_dtypetorch.qint8_contigTrue_other_scalarTrue_out_variantTrue
# Input: N: 64, dtype: torch.qint8, contig: True, other_scalar: True, out_variant: True
Run: 0, Forward Execution Time (us) : 91.232
Run: 1, Forward Execution Time (us) : 91.429
Run: 2, Forward Execution Time (us) : 91.658

# Benchmarking PyTorch: gt
# Mode: Eager
# Name: gt_N64_dtypetorch.qint32_contigFalse_other_scalarFalse_out_variantFalse
# Input: N: 64, dtype: torch.qint32, contig: False, other_scalar: False, out_variant: False
Run: 0, Forward Execution Time (us) : 130.253
Run: 1, Forward Execution Time (us) : 130.431
Run: 2, Forward Execution Time (us) : 130.491

# Benchmarking PyTorch: gt
# Mode: Eager
# Name: gt_N64_dtypetorch.qint32_contigFalse_other_scalarFalse_out_variantTrue
# Input: N: 64, dtype: torch.qint32, contig: False, other_scalar: False, out_variant: True
Run: 0, Forward Execution Time (us) : 147.919
Run: 1, Forward Execution Time (us) : 148.284
Run: 2, Forward Execution Time (us) : 148.125

# Benchmarking PyTorch: gt
# Mode: Eager
# Name: gt_N64_dtypetorch.qint32_contigFalse_other_scalarTrue_out_variantFalse
# Input: N: 64, dtype: torch.qint32, contig: False, other_scalar: True, out_variant: False
Run: 0, Forward Execution Time (us) : 84.315
Run: 1, Forward Execution Time (us) : 84.537
Run: 2, Forward Execution Time (us) : 84.502

# Benchmarking PyTorch: gt
# Mode: Eager
# Name: gt_N64_dtypetorch.qint32_contigFalse_other_scalarTrue_out_variantTrue
# Input: N: 64, dtype: torch.qint32, contig: False, other_scalar: True, out_variant: True
Run: 0, Forward Execution Time (us) : 102.238
Run: 1, Forward Execution Time (us) : 102.554
Run: 2, Forward Execution Time (us) : 102.563

# Benchmarking PyTorch: gt
# Mode: Eager
# Name: gt_N64_dtypetorch.qint32_contigTrue_other_scalarFalse_out_variantFalse
# Input: N: 64, dtype: torch.qint32, contig: True, other_scalar: False, out_variant: False
Run: 0, Forward Execution Time (us) : 101.764
Run: 1, Forward Execution Time (us) : 103.491
Run: 2, Forward Execution Time (us) : 103.032

# Benchmarking PyTorch: gt
# Mode: Eager
# Name: gt_N64_dtypetorch.qint32_contigTrue_other_scalarFalse_out_variantTrue
# Input: N: 64, dtype: torch.qint32, contig: True, other_scalar: False, out_variant: True
Run: 0, Forward Execution Time (us) : 137.173
Run: 1, Forward Execution Time (us) : 137.322
Run: 2, Forward Execution Time (us) : 137.497

# Benchmarking PyTorch: gt
# Mode: Eager
# Name: gt_N64_dtypetorch.qint32_contigTrue_other_scalarTrue_out_variantFalse
# Input: N: 64, dtype: torch.qint32, contig: True, other_scalar: True, out_variant: False
Run: 0, Forward Execution Time (us) : 74.237
Run: 1, Forward Execution Time (us) : 74.344
Run: 2, Forward Execution Time (us) : 74.436

# Benchmarking PyTorch: gt
# Mode: Eager
# Name: gt_N64_dtypetorch.qint32_contigTrue_other_scalarTrue_out_variantTrue
# Input: N: 64, dtype: torch.qint32, contig: True, other_scalar: True, out_variant: True
Run: 0, Forward Execution Time (us) : 91.774
Run: 1, Forward Execution Time (us) : 91.994
Run: 2, Forward Execution Time (us) : 91.870

# Benchmarking PyTorch: le
# Mode: Eager
# Name: le_N8_dtypetorch.quint8_contigFalse_other_scalarFalse_out_variantFalse
# Input: N: 8, dtype: torch.quint8, contig: False, other_scalar: False, out_variant: False
Run: 0, Forward Execution Time (us) : 22.412
Run: 1, Forward Execution Time (us) : 22.454
Run: 2, Forward Execution Time (us) : 22.507

# Benchmarking PyTorch: le
# Mode: Eager
# Name: le_N8_dtypetorch.quint8_contigFalse_other_scalarFalse_out_variantTrue
# Input: N: 8, dtype: torch.quint8, contig: False, other_scalar: False, out_variant: True
Run: 0, Forward Execution Time (us) : 40.019
Run: 1, Forward Execution Time (us) : 40.369
Run: 2, Forward Execution Time (us) : 40.475

# Benchmarking PyTorch: le
# Mode: Eager
# Name: le_N8_dtypetorch.quint8_contigFalse_other_scalarTrue_out_variantFalse
# Input: N: 8, dtype: torch.quint8, contig: False, other_scalar: True, out_variant: False
Run: 0, Forward Execution Time (us) : 26.649
Run: 1, Forward Execution Time (us) : 26.607
Run: 2, Forward Execution Time (us) : 26.707

# Benchmarking PyTorch: le
# Mode: Eager
# Name: le_N8_dtypetorch.quint8_contigFalse_other_scalarTrue_out_variantTrue
# Input: N: 8, dtype: torch.quint8, contig: False, other_scalar: True, out_variant: True
Run: 0, Forward Execution Time (us) : 44.549
Run: 1, Forward Execution Time (us) : 44.762
Run: 2, Forward Execution Time (us) : 44.715

# Benchmarking PyTorch: le
# Mode: Eager
# Name: le_N8_dtypetorch.quint8_contigTrue_other_scalarFalse_out_variantFalse
# Input: N: 8, dtype: torch.quint8, contig: True, other_scalar: False, out_variant: False
Run: 0, Forward Execution Time (us) : 14.645
Run: 1, Forward Execution Time (us) : 15.127
Run: 2, Forward Execution Time (us) : 14.302

# Benchmarking PyTorch: le
# Mode: Eager
# Name: le_N8_dtypetorch.quint8_contigTrue_other_scalarFalse_out_variantTrue
# Input: N: 8, dtype: torch.quint8, contig: True, other_scalar: False, out_variant: True
Run: 0, Forward Execution Time (us) : 32.105
Run: 1, Forward Execution Time (us) : 32.248
Run: 2, Forward Execution Time (us) : 32.276

# Benchmarking PyTorch: le
# Mode: Eager
# Name: le_N8_dtypetorch.quint8_contigTrue_other_scalarTrue_out_variantFalse
# Input: N: 8, dtype: torch.quint8, contig: True, other_scalar: True, out_variant: False
Run: 0, Forward Execution Time (us) : 19.650
Run: 1, Forward Execution Time (us) : 19.788
Run: 2, Forward Execution Time (us) : 19.780

# Benchmarking PyTorch: le
# Mode: Eager
# Name: le_N8_dtypetorch.quint8_contigTrue_other_scalarTrue_out_variantTrue
# Input: N: 8, dtype: torch.quint8, contig: True, other_scalar: True, out_variant: True
Run: 0, Forward Execution Time (us) : 36.912
Run: 1, Forward Execution Time (us) : 37.222
Run: 2, Forward Execution Time (us) : 37.233

# Benchmarking PyTorch: le
# Mode: Eager
# Name: le_N8_dtypetorch.qint8_contigFalse_other_scalarFalse_out_variantFalse
# Input: N: 8, dtype: torch.qint8, contig: False, other_scalar: False, out_variant: False
Run: 0, Forward Execution Time (us) : 22.572
Run: 1, Forward Execution Time (us) : 22.612
Run: 2, Forward Execution Time (us) : 22.636

# Benchmarking PyTorch: le
# Mode: Eager
# Name: le_N8_dtypetorch.qint8_contigFalse_other_scalarFalse_out_variantTrue
# Input: N: 8, dtype: torch.qint8, contig: False, other_scalar: False, out_variant: True
Run: 0, Forward Execution Time (us) : 40.030
Run: 1, Forward Execution Time (us) : 40.226
Run: 2, Forward Execution Time (us) : 40.345

# Benchmarking PyTorch: le
# Mode: Eager
# Name: le_N8_dtypetorch.qint8_contigFalse_other_scalarTrue_out_variantFalse
# Input: N: 8, dtype: torch.qint8, contig: False, other_scalar: True, out_variant: False
Run: 0, Forward Execution Time (us) : 26.451
Run: 1, Forward Execution Time (us) : 26.543
Run: 2, Forward Execution Time (us) : 26.722

# Benchmarking PyTorch: le
# Mode: Eager
# Name: le_N8_dtypetorch.qint8_contigFalse_other_scalarTrue_out_variantTrue
# Input: N: 8, dtype: torch.qint8, contig: False, other_scalar: True, out_variant: True
Run: 0, Forward Execution Time (us) : 44.565
Run: 1, Forward Execution Time (us) : 44.782
Run: 2, Forward Execution Time (us) : 44.735

# Benchmarking PyTorch: le
# Mode: Eager
# Name: le_N8_dtypetorch.qint8_contigTrue_other_scalarFalse_out_variantFalse
# Input: N: 8, dtype: torch.qint8, contig: True, other_scalar: False, out_variant: False
Run: 0, Forward Execution Time (us) : 14.742
Run: 1, Forward Execution Time (us) : 15.198
Run: 2, Forward Execution Time (us) : 14.346

# Benchmarking PyTorch: le
# Mode: Eager
# Name: le_N8_dtypetorch.qint8_contigTrue_other_scalarFalse_out_variantTrue
# Input: N: 8, dtype: torch.qint8, contig: True, other_scalar: False, out_variant: True
Run: 0, Forward Execution Time (us) : 32.027
Run: 1, Forward Execution Time (us) : 32.250
Run: 2, Forward Execution Time (us) : 32.247

# Benchmarking PyTorch: le
# Mode: Eager
# Name: le_N8_dtypetorch.qint8_contigTrue_other_scalarTrue_out_variantFalse
# Input: N: 8, dtype: torch.qint8, contig: True, other_scalar: True, out_variant: False
Run: 0, Forward Execution Time (us) : 19.628
Run: 1, Forward Execution Time (us) : 19.762
Run: 2, Forward Execution Time (us) : 19.863

# Benchmarking PyTorch: le
# Mode: Eager
# Name: le_N8_dtypetorch.qint8_contigTrue_other_scalarTrue_out_variantTrue
# Input: N: 8, dtype: torch.qint8, contig: True, other_scalar: True, out_variant: True
Run: 0, Forward Execution Time (us) : 36.868
Run: 1, Forward Execution Time (us) : 37.369
Run: 2, Forward Execution Time (us) : 37.209

# Benchmarking PyTorch: le
# Mode: Eager
# Name: le_N8_dtypetorch.qint32_contigFalse_other_scalarFalse_out_variantFalse
# Input: N: 8, dtype: torch.qint32, contig: False, other_scalar: False, out_variant: False
Run: 0, Forward Execution Time (us) : 22.386
Run: 1, Forward Execution Time (us) : 22.525
Run: 2, Forward Execution Time (us) : 22.664

# Benchmarking PyTorch: le
# Mode: Eager
# Name: le_N8_dtypetorch.qint32_contigFalse_other_scalarFalse_out_variantTrue
# Input: N: 8, dtype: torch.qint32, contig: False, other_scalar: False, out_variant: True
Run: 0, Forward Execution Time (us) : 40.109
Run: 1, Forward Execution Time (us) : 40.347
Run: 2, Forward Execution Time (us) : 40.488

# Benchmarking PyTorch: le
# Mode: Eager
# Name: le_N8_dtypetorch.qint32_contigFalse_other_scalarTrue_out_variantFalse
# Input: N: 8, dtype: torch.qint32, contig: False, other_scalar: True, out_variant: False
Run: 0, Forward Execution Time (us) : 26.657
Run: 1, Forward Execution Time (us) : 26.862
Run: 2, Forward Execution Time (us) : 26.828

# Benchmarking PyTorch: le
# Mode: Eager
# Name: le_N8_dtypetorch.qint32_contigFalse_other_scalarTrue_out_variantTrue
# Input: N: 8, dtype: torch.qint32, contig: False, other_scalar: True, out_variant: True
Run: 0, Forward Execution Time (us) : 44.669
Run: 1, Forward Execution Time (us) : 44.916
Run: 2, Forward Execution Time (us) : 44.968

# Benchmarking PyTorch: le
# Mode: Eager
# Name: le_N8_dtypetorch.qint32_contigTrue_other_scalarFalse_out_variantFalse
# Input: N: 8, dtype: torch.qint32, contig: True, other_scalar: False, out_variant: False
Run: 0, Forward Execution Time (us) : 14.743
Run: 1, Forward Execution Time (us) : 15.124
Run: 2, Forward Execution Time (us) : 14.279

# Benchmarking PyTorch: le
# Mode: Eager
# Name: le_N8_dtypetorch.qint32_contigTrue_other_scalarFalse_out_variantTrue
# Input: N: 8, dtype: torch.qint32, contig: True, other_scalar: False, out_variant: True
Run: 0, Forward Execution Time (us) : 32.131
Run: 1, Forward Execution Time (us) : 32.273
Run: 2, Forward Execution Time (us) : 32.291

# Benchmarking PyTorch: le
# Mode: Eager
# Name: le_N8_dtypetorch.qint32_contigTrue_other_scalarTrue_out_variantFalse
# Input: N: 8, dtype: torch.qint32, contig: True, other_scalar: True, out_variant: False
Run: 0, Forward Execution Time (us) : 19.771
Run: 1, Forward Execution Time (us) : 19.758
Run: 2, Forward Execution Time (us) : 19.798

# Benchmarking PyTorch: le
# Mode: Eager
# Name: le_N8_dtypetorch.qint32_contigTrue_other_scalarTrue_out_variantTrue
# Input: N: 8, dtype: torch.qint32, contig: True, other_scalar: True, out_variant: True
Run: 0, Forward Execution Time (us) : 37.022
Run: 1, Forward Execution Time (us) : 37.181
Run: 2, Forward Execution Time (us) : 37.217

# Benchmarking PyTorch: le
# Mode: Eager
# Name: le_N64_dtypetorch.quint8_contigFalse_other_scalarFalse_out_variantFalse
# Input: N: 64, dtype: torch.quint8, contig: False, other_scalar: False, out_variant: False
Run: 0, Forward Execution Time (us) : 129.144
Run: 1, Forward Execution Time (us) : 129.362
Run: 2, Forward Execution Time (us) : 129.436

# Benchmarking PyTorch: le
# Mode: Eager
# Name: le_N64_dtypetorch.quint8_contigFalse_other_scalarFalse_out_variantTrue
# Input: N: 64, dtype: torch.quint8, contig: False, other_scalar: False, out_variant: True
Run: 0, Forward Execution Time (us) : 146.776
Run: 1, Forward Execution Time (us) : 147.264
Run: 2, Forward Execution Time (us) : 147.131

# Benchmarking PyTorch: le
# Mode: Eager
# Name: le_N64_dtypetorch.quint8_contigFalse_other_scalarTrue_out_variantFalse
# Input: N: 64, dtype: torch.quint8, contig: False, other_scalar: True, out_variant: False
Run: 0, Forward Execution Time (us) : 83.461
Run: 1, Forward Execution Time (us) : 83.591
Run: 2, Forward Execution Time (us) : 83.589

# Benchmarking PyTorch: le
# Mode: Eager
# Name: le_N64_dtypetorch.quint8_contigFalse_other_scalarTrue_out_variantTrue
# Input: N: 64, dtype: torch.quint8, contig: False, other_scalar: True, out_variant: True
Run: 0, Forward Execution Time (us) : 101.230
Run: 1, Forward Execution Time (us) : 101.582
Run: 2, Forward Execution Time (us) : 101.567

# Benchmarking PyTorch: le
# Mode: Eager
# Name: le_N64_dtypetorch.quint8_contigTrue_other_scalarFalse_out_variantFalse
# Input: N: 64, dtype: torch.quint8, contig: True, other_scalar: False, out_variant: False
Run: 0, Forward Execution Time (us) : 101.176
Run: 1, Forward Execution Time (us) : 102.251
Run: 2, Forward Execution Time (us) : 102.684

# Benchmarking PyTorch: le
# Mode: Eager
# Name: le_N64_dtypetorch.quint8_contigTrue_other_scalarFalse_out_variantTrue
# Input: N: 64, dtype: torch.quint8, contig: True, other_scalar: False, out_variant: True
Run: 0, Forward Execution Time (us) : 136.004
Run: 1, Forward Execution Time (us) : 136.211
Run: 2, Forward Execution Time (us) : 136.498

# Benchmarking PyTorch: le
# Mode: Eager
# Name: le_N64_dtypetorch.quint8_contigTrue_other_scalarTrue_out_variantFalse
# Input: N: 64, dtype: torch.quint8, contig: True, other_scalar: True, out_variant: False
Run: 0, Forward Execution Time (us) : 74.031
Run: 1, Forward Execution Time (us) : 74.048
Run: 2, Forward Execution Time (us) : 74.057

# Benchmarking PyTorch: le
# Mode: Eager
# Name: le_N64_dtypetorch.quint8_contigTrue_other_scalarTrue_out_variantTrue
# Input: N: 64, dtype: torch.quint8, contig: True, other_scalar: True, out_variant: True
Run: 0, Forward Execution Time (us) : 91.114
Run: 1, Forward Execution Time (us) : 91.462
Run: 2, Forward Execution Time (us) : 91.310

# Benchmarking PyTorch: le
# Mode: Eager
# Name: le_N64_dtypetorch.qint8_contigFalse_other_scalarFalse_out_variantFalse
# Input: N: 64, dtype: torch.qint8, contig: False, other_scalar: False, out_variant: False
Run: 0, Forward Execution Time (us) : 129.336
Run: 1, Forward Execution Time (us) : 129.647
Run: 2, Forward Execution Time (us) : 129.492

# Benchmarking PyTorch: le
# Mode: Eager
# Name: le_N64_dtypetorch.qint8_contigFalse_other_scalarFalse_out_variantTrue
# Input: N: 64, dtype: torch.qint8, contig: False, other_scalar: False, out_variant: True
Run: 0, Forward Execution Time (us) : 147.147
Run: 1, Forward Execution Time (us) : 147.310
Run: 2, Forward Execution Time (us) : 147.337

# Benchmarking PyTorch: le
# Mode: Eager
# Name: le_N64_dtypetorch.qint8_contigFalse_other_scalarTrue_out_variantFalse
# Input: N: 64, dtype: torch.qint8, contig: False, other_scalar: True, out_variant: False
Run: 0, Forward Execution Time (us) : 83.632
Run: 1, Forward Execution Time (us) : 83.733
Run: 2, Forward Execution Time (us) : 83.804

# Benchmarking PyTorch: le
# Mode: Eager
# Name: le_N64_dtypetorch.qint8_contigFalse_other_scalarTrue_out_variantTrue
# Input: N: 64, dtype: torch.qint8, contig: False, other_scalar: True, out_variant: True
Run: 0, Forward Execution Time (us) : 101.653
Run: 1, Forward Execution Time (us) : 101.885
Run: 2, Forward Execution Time (us) : 101.986

# Benchmarking PyTorch: le
# Mode: Eager
# Name: le_N64_dtypetorch.qint8_contigTrue_other_scalarFalse_out_variantFalse
# Input: N: 64, dtype: torch.qint8, contig: True, other_scalar: False, out_variant: False
Run: 0, Forward Execution Time (us) : 101.076
Run: 1, Forward Execution Time (us) : 102.336
Run: 2, Forward Execution Time (us) : 102.673

# Benchmarking PyTorch: le
# Mode: Eager
# Name: le_N64_dtypetorch.qint8_contigTrue_other_scalarFalse_out_variantTrue
# Input: N: 64, dtype: torch.qint8, contig: True, other_scalar: False, out_variant: True
Run: 0, Forward Execution Time (us) : 135.968
Run: 1, Forward Execution Time (us) : 136.243
Run: 2, Forward Execution Time (us) : 136.202

# Benchmarking PyTorch: le
# Mode: Eager
# Name: le_N64_dtypetorch.qint8_contigTrue_other_scalarTrue_out_variantFalse
# Input: N: 64, dtype: torch.qint8, contig: True, other_scalar: True, out_variant: False
Run: 0, Forward Execution Time (us) : 73.772
Run: 1, Forward Execution Time (us) : 73.855
Run: 2, Forward Execution Time (us) : 73.926

# Benchmarking PyTorch: le
# Mode: Eager
# Name: le_N64_dtypetorch.qint8_contigTrue_other_scalarTrue_out_variantTrue
# Input: N: 64, dtype: torch.qint8, contig: True, other_scalar: True, out_variant: True
Run: 0, Forward Execution Time (us) : 90.810
Run: 1, Forward Execution Time (us) : 91.159
Run: 2, Forward Execution Time (us) : 91.275

# Benchmarking PyTorch: le
# Mode: Eager
# Name: le_N64_dtypetorch.qint32_contigFalse_other_scalarFalse_out_variantFalse
# Input: N: 64, dtype: torch.qint32, contig: False, other_scalar: False, out_variant: False
Run: 0, Forward Execution Time (us) : 130.097
Run: 1, Forward Execution Time (us) : 130.204
Run: 2, Forward Execution Time (us) : 130.219

# Benchmarking PyTorch: le
# Mode: Eager
# Name: le_N64_dtypetorch.qint32_contigFalse_other_scalarFalse_out_variantTrue
# Input: N: 64, dtype: torch.qint32, contig: False, other_scalar: False, out_variant: True
Run: 0, Forward Execution Time (us) : 147.481
Run: 1, Forward Execution Time (us) : 147.949
Run: 2, Forward Execution Time (us) : 148.006

# Benchmarking PyTorch: le
# Mode: Eager
# Name: le_N64_dtypetorch.qint32_contigFalse_other_scalarTrue_out_variantFalse
# Input: N: 64, dtype: torch.qint32, contig: False, other_scalar: True, out_variant: False
Run: 0, Forward Execution Time (us) : 84.119
Run: 1, Forward Execution Time (us) : 84.251
Run: 2, Forward Execution Time (us) : 84.213

# Benchmarking PyTorch: le
# Mode: Eager
# Name: le_N64_dtypetorch.qint32_contigFalse_other_scalarTrue_out_variantTrue
# Input: N: 64, dtype: torch.qint32, contig: False, other_scalar: True, out_variant: True
Run: 0, Forward Execution Time (us) : 101.864
Run: 1, Forward Execution Time (us) : 102.112
Run: 2, Forward Execution Time (us) : 102.252

# Benchmarking PyTorch: le
# Mode: Eager
# Name: le_N64_dtypetorch.qint32_contigTrue_other_scalarFalse_out_variantFalse
# Input: N: 64, dtype: torch.qint32, contig: True, other_scalar: False, out_variant: False
Run: 0, Forward Execution Time (us) : 101.752
Run: 1, Forward Execution Time (us) : 103.290
Run: 2, Forward Execution Time (us) : 103.703

# Benchmarking PyTorch: le
# Mode: Eager
# Name: le_N64_dtypetorch.qint32_contigTrue_other_scalarFalse_out_variantTrue
# Input: N: 64, dtype: torch.qint32, contig: True, other_scalar: False, out_variant: True
Run: 0, Forward Execution Time (us) : 136.628
Run: 1, Forward Execution Time (us) : 137.068
Run: 2, Forward Execution Time (us) : 137.065

# Benchmarking PyTorch: le
# Mode: Eager
# Name: le_N64_dtypetorch.qint32_contigTrue_other_scalarTrue_out_variantFalse
# Input: N: 64, dtype: torch.qint32, contig: True, other_scalar: True, out_variant: False
Run: 0, Forward Execution Time (us) : 74.269
Run: 1, Forward Execution Time (us) : 74.199
Run: 2, Forward Execution Time (us) : 74.217

# Benchmarking PyTorch: le
# Mode: Eager
# Name: le_N64_dtypetorch.qint32_contigTrue_other_scalarTrue_out_variantTrue
# Input: N: 64, dtype: torch.qint32, contig: True, other_scalar: True, out_variant: True
Run: 0, Forward Execution Time (us) : 91.296
Run: 1, Forward Execution Time (us) : 91.429
Run: 2, Forward Execution Time (us) : 91.339

# Benchmarking PyTorch: ge
# Mode: Eager
# Name: ge_N8_dtypetorch.quint8_contigFalse_other_scalarFalse_out_variantFalse
# Input: N: 8, dtype: torch.quint8, contig: False, other_scalar: False, out_variant: False
Run: 0, Forward Execution Time (us) : 22.475
Run: 1, Forward Execution Time (us) : 22.500
Run: 2, Forward Execution Time (us) : 22.559

# Benchmarking PyTorch: ge
# Mode: Eager
# Name: ge_N8_dtypetorch.quint8_contigFalse_other_scalarFalse_out_variantTrue
# Input: N: 8, dtype: torch.quint8, contig: False, other_scalar: False, out_variant: True
Run: 0, Forward Execution Time (us) : 40.101
Run: 1, Forward Execution Time (us) : 40.390
Run: 2, Forward Execution Time (us) : 40.340

# Benchmarking PyTorch: ge
# Mode: Eager
# Name: ge_N8_dtypetorch.quint8_contigFalse_other_scalarTrue_out_variantFalse
# Input: N: 8, dtype: torch.quint8, contig: False, other_scalar: True, out_variant: False
Run: 0, Forward Execution Time (us) : 26.711
Run: 1, Forward Execution Time (us) : 26.875
Run: 2, Forward Execution Time (us) : 26.892

# Benchmarking PyTorch: ge
# Mode: Eager
# Name: ge_N8_dtypetorch.quint8_contigFalse_other_scalarTrue_out_variantTrue
# Input: N: 8, dtype: torch.quint8, contig: False, other_scalar: True, out_variant: True
Run: 0, Forward Execution Time (us) : 44.712
Run: 1, Forward Execution Time (us) : 45.066
Run: 2, Forward Execution Time (us) : 44.977

# Benchmarking PyTorch: ge
# Mode: Eager
# Name: ge_N8_dtypetorch.quint8_contigTrue_other_scalarFalse_out_variantFalse
# Input: N: 8, dtype: torch.quint8, contig: True, other_scalar: False, out_variant: False
Run: 0, Forward Execution Time (us) : 14.585
Run: 1, Forward Execution Time (us) : 14.923
Run: 2, Forward Execution Time (us) : 15.249

# Benchmarking PyTorch: ge
# Mode: Eager
# Name: ge_N8_dtypetorch.quint8_contigTrue_other_scalarFalse_out_variantTrue
# Input: N: 8, dtype: torch.quint8, contig: True, other_scalar: False, out_variant: True
Run: 0, Forward Execution Time (us) : 32.063
Run: 1, Forward Execution Time (us) : 32.246
Run: 2, Forward Execution Time (us) : 32.295

# Benchmarking PyTorch: ge
# Mode: Eager
# Name: ge_N8_dtypetorch.quint8_contigTrue_other_scalarTrue_out_variantFalse
# Input: N: 8, dtype: torch.quint8, contig: True, other_scalar: True, out_variant: False
Run: 0, Forward Execution Time (us) : 19.767
Run: 1, Forward Execution Time (us) : 19.839
Run: 2, Forward Execution Time (us) : 19.907

# Benchmarking PyTorch: ge
# Mode: Eager
# Name: ge_N8_dtypetorch.quint8_contigTrue_other_scalarTrue_out_variantTrue
# Input: N: 8, dtype: torch.quint8, contig: True, other_scalar: True, out_variant: True
Run: 0, Forward Execution Time (us) : 37.084
Run: 1, Forward Execution Time (us) : 37.218
Run: 2, Forward Execution Time (us) : 37.264

# Benchmarking PyTorch: ge
# Mode: Eager
# Name: ge_N8_dtypetorch.qint8_contigFalse_other_scalarFalse_out_variantFalse
# Input: N: 8, dtype: torch.qint8, contig: False, other_scalar: False, out_variant: False
Run: 0, Forward Execution Time (us) : 22.477
Run: 1, Forward Execution Time (us) : 22.623
Run: 2, Forward Execution Time (us) : 22.633

# Benchmarking PyTorch: ge
# Mode: Eager
# Name: ge_N8_dtypetorch.qint8_contigFalse_other_scalarFalse_out_variantTrue
# Input: N: 8, dtype: torch.qint8, contig: False, other_scalar: False, out_variant: True
Run: 0, Forward Execution Time (us) : 40.067
Run: 1, Forward Execution Time (us) : 40.395
Run: 2, Forward Execution Time (us) : 40.360

# Benchmarking PyTorch: ge
# Mode: Eager
# Name: ge_N8_dtypetorch.qint8_contigFalse_other_scalarTrue_out_variantFalse
# Input: N: 8, dtype: torch.qint8, contig: False, other_scalar: True, out_variant: False
Run: 0, Forward Execution Time (us) : 26.694
Run: 1, Forward Execution Time (us) : 26.862
Run: 2, Forward Execution Time (us) : 26.788

# Benchmarking PyTorch: ge
# Mode: Eager
# Name: ge_N8_dtypetorch.qint8_contigFalse_other_scalarTrue_out_variantTrue
# Input: N: 8, dtype: torch.qint8, contig: False, other_scalar: True, out_variant: True
Run: 0, Forward Execution Time (us) : 44.641
Run: 1, Forward Execution Time (us) : 44.864
Run: 2, Forward Execution Time (us) : 44.931

# Benchmarking PyTorch: ge
# Mode: Eager
# Name: ge_N8_dtypetorch.qint8_contigTrue_other_scalarFalse_out_variantFalse
# Input: N: 8, dtype: torch.qint8, contig: True, other_scalar: False, out_variant: False
Run: 0, Forward Execution Time (us) : 14.173
Run: 1, Forward Execution Time (us) : 14.712
Run: 2, Forward Execution Time (us) : 15.006

# Benchmarking PyTorch: ge
# Mode: Eager
# Name: ge_N8_dtypetorch.qint8_contigTrue_other_scalarFalse_out_variantTrue
# Input: N: 8, dtype: torch.qint8, contig: True, other_scalar: False, out_variant: True
Run: 0, Forward Execution Time (us) : 32.108
Run: 1, Forward Execution Time (us) : 32.280
Run: 2, Forward Execution Time (us) : 32.232

# Benchmarking PyTorch: ge
# Mode: Eager
# Name: ge_N8_dtypetorch.qint8_contigTrue_other_scalarTrue_out_variantFalse
# Input: N: 8, dtype: torch.qint8, contig: True, other_scalar: True, out_variant: False
Run: 0, Forward Execution Time (us) : 19.745
Run: 1, Forward Execution Time (us) : 19.889
Run: 2, Forward Execution Time (us) : 19.868

# Benchmarking PyTorch: ge
# Mode: Eager
# Name: ge_N8_dtypetorch.qint8_contigTrue_other_scalarTrue_out_variantTrue
# Input: N: 8, dtype: torch.qint8, contig: True, other_scalar: True, out_variant: True
Run: 0, Forward Execution Time (us) : 37.011
Run: 1, Forward Execution Time (us) : 37.336
Run: 2, Forward Execution Time (us) : 37.235

# Benchmarking PyTorch: ge
# Mode: Eager
# Name: ge_N8_dtypetorch.qint32_contigFalse_other_scalarFalse_out_variantFalse
# Input: N: 8, dtype: torch.qint32, contig: False, other_scalar: False, out_variant: False
Run: 0, Forward Execution Time (us) : 22.405
Run: 1, Forward Execution Time (us) : 22.601
Run: 2, Forward Execution Time (us) : 22.557

# Benchmarking PyTorch: ge
# Mode: Eager
# Name: ge_N8_dtypetorch.qint32_contigFalse_other_scalarFalse_out_variantTrue
# Input: N: 8, dtype: torch.qint32, contig: False, other_scalar: False, out_variant: True
Run: 0, Forward Execution Time (us) : 40.286
Run: 1, Forward Execution Time (us) : 40.507
Run: 2, Forward Execution Time (us) : 40.403

# Benchmarking PyTorch: ge
# Mode: Eager
# Name: ge_N8_dtypetorch.qint32_contigFalse_other_scalarTrue_out_variantFalse
# Input: N: 8, dtype: torch.qint32, contig: False, other_scalar: True, out_variant: False
Run: 0, Forward Execution Time (us) : 26.883
Run: 1, Forward Execution Time (us) : 26.906
Run: 2, Forward Execution Time (us) : 26.894

# Benchmarking PyTorch: ge
# Mode: Eager
# Name: ge_N8_dtypetorch.qint32_contigFalse_other_scalarTrue_out_variantTrue
# Input: N: 8, dtype: torch.qint32, contig: False, other_scalar: True, out_variant: True
Run: 0, Forward Execution Time (us) : 44.766
Run: 1, Forward Execution Time (us) : 44.948
Run: 2, Forward Execution Time (us) : 44.979

# Benchmarking PyTorch: ge
# Mode: Eager
# Name: ge_N8_dtypetorch.qint32_contigTrue_other_scalarFalse_out_variantFalse
# Input: N: 8, dtype: torch.qint32, contig: True, other_scalar: False, out_variant: False
Run: 0, Forward Execution Time (us) : 14.144
Run: 1, Forward Execution Time (us) : 14.423
Run: 2, Forward Execution Time (us) : 14.858

# Benchmarking PyTorch: ge
# Mode: Eager
# Name: ge_N8_dtypetorch.qint32_contigTrue_other_scalarFalse_out_variantTrue
# Input: N: 8, dtype: torch.qint32, contig: True, other_scalar: False, out_variant: True
Run: 0, Forward Execution Time (us) : 32.036
Run: 1, Forward Execution Time (us) : 32.304
Run: 2, Forward Execution Time (us) : 32.343

# Benchmarking PyTorch: ge
# Mode: Eager
# Name: ge_N8_dtypetorch.qint32_contigTrue_other_scalarTrue_out_variantFalse
# Input: N: 8, dtype: torch.qint32, contig: True, other_scalar: True, out_variant: False
Run: 0, Forward Execution Time (us) : 19.712
Run: 1, Forward Execution Time (us) : 19.859
Run: 2, Forward Execution Time (us) : 19.856

# Benchmarking PyTorch: ge
# Mode: Eager
# Name: ge_N8_dtypetorch.qint32_contigTrue_other_scalarTrue_out_variantTrue
# Input: N: 8, dtype: torch.qint32, contig: True, other_scalar: True, out_variant: True
Run: 0, Forward Execution Time (us) : 37.008
Run: 1, Forward Execution Time (us) : 37.247
Run: 2, Forward Execution Time (us) : 37.141

# Benchmarking PyTorch: ge
# Mode: Eager
# Name: ge_N64_dtypetorch.quint8_contigFalse_other_scalarFalse_out_variantFalse
# Input: N: 64, dtype: torch.quint8, contig: False, other_scalar: False, out_variant: False
Run: 0, Forward Execution Time (us) : 129.047
Run: 1, Forward Execution Time (us) : 129.227
Run: 2, Forward Execution Time (us) : 129.160

# Benchmarking PyTorch: ge
# Mode: Eager
# Name: ge_N64_dtypetorch.quint8_contigFalse_other_scalarFalse_out_variantTrue
# Input: N: 64, dtype: torch.quint8, contig: False, other_scalar: False, out_variant: True
Run: 0, Forward Execution Time (us) : 146.807
Run: 1, Forward Execution Time (us) : 147.133
Run: 2, Forward Execution Time (us) : 147.037

# Benchmarking PyTorch: ge
# Mode: Eager
# Name: ge_N64_dtypetorch.quint8_contigFalse_other_scalarTrue_out_variantFalse
# Input: N: 64, dtype: torch.quint8, contig: False, other_scalar: True, out_variant: False
Run: 0, Forward Execution Time (us) : 83.546
Run: 1, Forward Execution Time (us) : 83.794
Run: 2, Forward Execution Time (us) : 83.739

# Benchmarking PyTorch: ge
# Mode: Eager
# Name: ge_N64_dtypetorch.quint8_contigFalse_other_scalarTrue_out_variantTrue
# Input: N: 64, dtype: torch.quint8, contig: False, other_scalar: True, out_variant: True
Run: 0, Forward Execution Time (us) : 101.692
Run: 1, Forward Execution Time (us) : 101.901
Run: 2, Forward Execution Time (us) : 101.841

# Benchmarking PyTorch: ge
# Mode: Eager
# Name: ge_N64_dtypetorch.quint8_contigTrue_other_scalarFalse_out_variantFalse
# Input: N: 64, dtype: torch.quint8, contig: True, other_scalar: False, out_variant: False
Run: 0, Forward Execution Time (us) : 101.257
Run: 1, Forward Execution Time (us) : 102.844
Run: 2, Forward Execution Time (us) : 102.376

# Benchmarking PyTorch: ge
# Mode: Eager
# Name: ge_N64_dtypetorch.quint8_contigTrue_other_scalarFalse_out_variantTrue
# Input: N: 64, dtype: torch.quint8, contig: True, other_scalar: False, out_variant: True
Run: 0, Forward Execution Time (us) : 136.439
Run: 1, Forward Execution Time (us) : 136.619
Run: 2, Forward Execution Time (us) : 136.477

# Benchmarking PyTorch: ge
# Mode: Eager
# Name: ge_N64_dtypetorch.quint8_contigTrue_other_scalarTrue_out_variantFalse
# Input: N: 64, dtype: torch.quint8, contig: True, other_scalar: True, out_variant: False
Run: 0, Forward Execution Time (us) : 73.928
Run: 1, Forward Execution Time (us) : 74.099
Run: 2, Forward Execution Time (us) : 74.053

# Benchmarking PyTorch: ge
# Mode: Eager
# Name: ge_N64_dtypetorch.quint8_contigTrue_other_scalarTrue_out_variantTrue
# Input: N: 64, dtype: torch.quint8, contig: True, other_scalar: True, out_variant: True
Run: 0, Forward Execution Time (us) : 91.214
Run: 1, Forward Execution Time (us) : 91.526
Run: 2, Forward Execution Time (us) : 91.416

# Benchmarking PyTorch: ge
# Mode: Eager
# Name: ge_N64_dtypetorch.qint8_contigFalse_other_scalarFalse_out_variantFalse
# Input: N: 64, dtype: torch.qint8, contig: False, other_scalar: False, out_variant: False
Run: 0, Forward Execution Time (us) : 129.197
Run: 1, Forward Execution Time (us) : 129.252
Run: 2, Forward Execution Time (us) : 129.278

# Benchmarking PyTorch: ge
# Mode: Eager
# Name: ge_N64_dtypetorch.qint8_contigFalse_other_scalarFalse_out_variantTrue
# Input: N: 64, dtype: torch.qint8, contig: False, other_scalar: False, out_variant: True
Run: 0, Forward Execution Time (us) : 147.072
Run: 1, Forward Execution Time (us) : 147.301
Run: 2, Forward Execution Time (us) : 147.413

# Benchmarking PyTorch: ge
# Mode: Eager
# Name: ge_N64_dtypetorch.qint8_contigFalse_other_scalarTrue_out_variantFalse
# Input: N: 64, dtype: torch.qint8, contig: False, other_scalar: True, out_variant: False
Run: 0, Forward Execution Time (us) : 83.781
Run: 1, Forward Execution Time (us) : 83.772
Run: 2, Forward Execution Time (us) : 83.901

# Benchmarking PyTorch: ge
# Mode: Eager
# Name: ge_N64_dtypetorch.qint8_contigFalse_other_scalarTrue_out_variantTrue
# Input: N: 64, dtype: torch.qint8, contig: False, other_scalar: True, out_variant: True
Run: 0, Forward Execution Time (us) : 101.992
Run: 1, Forward Execution Time (us) : 102.085
Run: 2, Forward Execution Time (us) : 102.077

# Benchmarking PyTorch: ge
# Mode: Eager
# Name: ge_N64_dtypetorch.qint8_contigTrue_other_scalarFalse_out_variantFalse
# Input: N: 64, dtype: torch.qint8, contig: True, other_scalar: False, out_variant: False
Run: 0, Forward Execution Time (us) : 101.071
Run: 1, Forward Execution Time (us) : 102.507
Run: 2, Forward Execution Time (us) : 102.337

# Benchmarking PyTorch: ge
# Mode: Eager
# Name: ge_N64_dtypetorch.qint8_contigTrue_other_scalarFalse_out_variantTrue
# Input: N: 64, dtype: torch.qint8, contig: True, other_scalar: False, out_variant: True
Run: 0, Forward Execution Time (us) : 136.175
Run: 1, Forward Execution Time (us) : 136.620
Run: 2, Forward Execution Time (us) : 136.512

# Benchmarking PyTorch: ge
# Mode: Eager
# Name: ge_N64_dtypetorch.qint8_contigTrue_other_scalarTrue_out_variantFalse
# Input: N: 64, dtype: torch.qint8, contig: True, other_scalar: True, out_variant: False
Run: 0, Forward Execution Time (us) : 73.790
Run: 1, Forward Execution Time (us) : 73.954
Run: 2, Forward Execution Time (us) : 74.048

# Benchmarking PyTorch: ge
# Mode: Eager
# Name: ge_N64_dtypetorch.qint8_contigTrue_other_scalarTrue_out_variantTrue
# Input: N: 64, dtype: torch.qint8, contig: True, other_scalar: True, out_variant: True
Run: 0, Forward Execution Time (us) : 91.361
Run: 1, Forward Execution Time (us) : 91.694
Run: 2, Forward Execution Time (us) : 91.545

# Benchmarking PyTorch: ge
# Mode: Eager
# Name: ge_N64_dtypetorch.qint32_contigFalse_other_scalarFalse_out_variantFalse
# Input: N: 64, dtype: torch.qint32, contig: False, other_scalar: False, out_variant: False
Run: 0, Forward Execution Time (us) : 130.091
Run: 1, Forward Execution Time (us) : 130.262
Run: 2, Forward Execution Time (us) : 130.206

# Benchmarking PyTorch: ge
# Mode: Eager
# Name: ge_N64_dtypetorch.qint32_contigFalse_other_scalarFalse_out_variantTrue
# Input: N: 64, dtype: torch.qint32, contig: False, other_scalar: False, out_variant: True
Run: 0, Forward Execution Time (us) : 147.886
/nvme/share/share/platform/env/miniconda3.8/envs/pt2.0_diopi/pytorch2.0/benchmarks/operator_benchmark/pt/qcomparators_test.py:56: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [8, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /nvme/share/share/platform/dep/DIOPI_pytorch/pytorch2.0_c263bd/aten/src/ATen/native/Resize.cpp:33.)
  return self.qop(q_input_a, q_input_b, out=torch.tensor(True, dtype=torch.bool))
/nvme/share/share/platform/env/miniconda3.8/envs/pt2.0_diopi/pytorch2.0/benchmarks/operator_benchmark/pt/qcomparators_test.py:54: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [8, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /nvme/share/share/platform/dep/DIOPI_pytorch/pytorch2.0_c263bd/aten/src/ATen/native/Resize.cpp:33.)
  return self.qop(q_input_a, 42, out=torch.tensor(True, dtype=torch.bool))
/nvme/share/share/platform/env/miniconda3.8/envs/pt2.0_diopi/pytorch2.0/benchmarks/operator_benchmark/pt/qcomparators_test.py:56: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [64, 64]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /nvme/share/share/platform/dep/DIOPI_pytorch/pytorch2.0_c263bd/aten/src/ATen/native/Resize.cpp:33.)
  return self.qop(q_input_a, q_input_b, out=torch.tensor(True, dtype=torch.bool))
/nvme/share/share/platform/env/miniconda3.8/envs/pt2.0_diopi/pytorch2.0/benchmarks/operator_benchmark/pt/qcomparators_test.py:54: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [64, 64]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /nvme/share/share/platform/dep/DIOPI_pytorch/pytorch2.0_c263bd/aten/src/ATen/native/Resize.cpp:33.)
  return self.qop(q_input_a, 42, out=torch.tensor(True, dtype=torch.bool))
Run: 1, Forward Execution Time (us) : 148.330
Run: 2, Forward Execution Time (us) : 148.237

# Benchmarking PyTorch: ge
# Mode: Eager
# Name: ge_N64_dtypetorch.qint32_contigFalse_other_scalarTrue_out_variantFalse
# Input: N: 64, dtype: torch.qint32, contig: False, other_scalar: True, out_variant: False
Run: 0, Forward Execution Time (us) : 84.319
Run: 1, Forward Execution Time (us) : 84.586
Run: 2, Forward Execution Time (us) : 84.559

# Benchmarking PyTorch: ge
# Mode: Eager
# Name: ge_N64_dtypetorch.qint32_contigFalse_other_scalarTrue_out_variantTrue
# Input: N: 64, dtype: torch.qint32, contig: False, other_scalar: True, out_variant: True
Run: 0, Forward Execution Time (us) : 102.380
Run: 1, Forward Execution Time (us) : 102.623
Run: 2, Forward Execution Time (us) : 102.553

# Benchmarking PyTorch: ge
# Mode: Eager
# Name: ge_N64_dtypetorch.qint32_contigTrue_other_scalarFalse_out_variantFalse
# Input: N: 64, dtype: torch.qint32, contig: True, other_scalar: False, out_variant: False
Run: 0, Forward Execution Time (us) : 101.669
Run: 1, Forward Execution Time (us) : 103.089
Run: 2, Forward Execution Time (us) : 103.637

# Benchmarking PyTorch: ge
# Mode: Eager
# Name: ge_N64_dtypetorch.qint32_contigTrue_other_scalarFalse_out_variantTrue
# Input: N: 64, dtype: torch.qint32, contig: True, other_scalar: False, out_variant: True
Run: 0, Forward Execution Time (us) : 136.795
Run: 1, Forward Execution Time (us) : 137.040
Run: 2, Forward Execution Time (us) : 136.996

# Benchmarking PyTorch: ge
# Mode: Eager
# Name: ge_N64_dtypetorch.qint32_contigTrue_other_scalarTrue_out_variantFalse
# Input: N: 64, dtype: torch.qint32, contig: True, other_scalar: True, out_variant: False
Run: 0, Forward Execution Time (us) : 74.153
Run: 1, Forward Execution Time (us) : 74.292
Run: 2, Forward Execution Time (us) : 74.537

# Benchmarking PyTorch: ge
# Mode: Eager
# Name: ge_N64_dtypetorch.qint32_contigTrue_other_scalarTrue_out_variantTrue
# Input: N: 64, dtype: torch.qint32, contig: True, other_scalar: True, out_variant: True
Run: 0, Forward Execution Time (us) : 91.592
Run: 1, Forward Execution Time (us) : 91.643
Run: 2, Forward Execution Time (us) : 91.584

+ python -m pt.as_strided_test --num-runs 3 --iterations 100 --warmup-iterations 10
not use dipu
# ----------------------------------------
# PyTorch/Caffe2 Operator Micro-benchmarks
# ----------------------------------------
# Tag : short

# Benchmarking PyTorch: as_strided
# Mode: Eager
# Name: as_strided_M8_N8_size(2,2)_stride(1,1)_storage_offset0_cpu
# Input: M: 8, N: 8, size: (2, 2), stride: (1, 1), storage_offset: 0, device: cpu
Run: 0, Forward Execution Time (us) : 7.518
Run: 1, Forward Execution Time (us) : 8.698
Run: 2, Forward Execution Time (us) : 8.729

# Benchmarking PyTorch: as_strided
# Mode: Eager
# Name: as_strided_M8_N8_size(2,2)_stride(1,1)_storage_offset0_cuda
# Input: M: 8, N: 8, size: (2, 2), stride: (1, 1), storage_offset: 0, device: cuda
Run: 0, Forward Execution Time (us) : 7.684
Run: 1, Forward Execution Time (us) : 9.233
Run: 2, Forward Execution Time (us) : 8.864

# Benchmarking PyTorch: as_strided
# Mode: Eager
# Name: as_strided_M256_N256_size(32,32)_stride(1,1)_storage_offset0_cpu
# Input: M: 256, N: 256, size: (32, 32), stride: (1, 1), storage_offset: 0, device: cpu
Run: 0, Forward Execution Time (us) : 7.548
Run: 1, Forward Execution Time (us) : 8.993
Run: 2, Forward Execution Time (us) : 8.701

# Benchmarking PyTorch: as_strided
# Mode: Eager
# Name: as_strided_M256_N256_size(32,32)_stride(1,1)_storage_offset0_cuda
# Input: M: 256, N: 256, size: (32, 32), stride: (1, 1), storage_offset: 0, device: cuda
Run: 0, Forward Execution Time (us) : 9.091
Run: 1, Forward Execution Time (us) : 8.902
Run: 2, Forward Execution Time (us) : 8.938

# Benchmarking PyTorch: as_strided
# Mode: Eager
# Name: as_strided_M512_N512_size(64,64)_stride(2,2)_storage_offset1_cpu
# Input: M: 512, N: 512, size: (64, 64), stride: (2, 2), storage_offset: 1, device: cpu
Run: 0, Forward Execution Time (us) : 7.602
Run: 1, Forward Execution Time (us) : 8.916
Run: 2, Forward Execution Time (us) : 8.668

# Benchmarking PyTorch: as_strided
# Mode: Eager
# Name: as_strided_M512_N512_size(64,64)_stride(2,2)_storage_offset1_cuda
# Input: M: 512, N: 512, size: (64, 64), stride: (2, 2), storage_offset: 1, device: cuda
Run: 0, Forward Execution Time (us) : 9.127
Run: 1, Forward Execution Time (us) : 8.898
Run: 2, Forward Execution Time (us) : 8.853

+ python -m pt.fill_test --num-runs 3 --iterations 100 --warmup-iterations 10
not use dipu
# ----------------------------------------
# PyTorch/Caffe2 Operator Micro-benchmarks
# ----------------------------------------
# Tag : short

# Benchmarking PyTorch: fill_
# Mode: Eager
# Name: fill__N1_cpu_dtypetorch.int32
# Input: N: 1, device: cpu, dtype: torch.int32
Run: 0, Forward Execution Time (us) : 3.225
Run: 1, Forward Execution Time (us) : 4.002
Run: 2, Forward Execution Time (us) : 3.855

# Benchmarking PyTorch: fill_
# Mode: Eager
# Name: fill__N1_cuda_dtypetorch.int32
# Input: N: 1, device: cuda, dtype: torch.int32
Run: 0, Forward Execution Time (us) : 10.979
Run: 1, Forward Execution Time (us) : 11.008
Run: 2, Forward Execution Time (us) : 11.036

# Benchmarking PyTorch: fill_
# Mode: Eager
# Name: fill__N1024_cpu_dtypetorch.int32
# Input: N: 1024, device: cpu, dtype: torch.int32
Run: 0, Forward Execution Time (us) : 5.046
Run: 1, Forward Execution Time (us) : 5.084
Run: 2, Forward Execution Time (us) : 5.044

# Benchmarking PyTorch: fill_
# Mode: Eager
# Name: fill__N1024_cuda_dtypetorch.int32
# Input: N: 1024, device: cuda, dtype: torch.int32
Run: 0, Forward Execution Time (us) : 10.891
Run: 1, Forward Execution Time (us) : 11.092
Run: 2, Forward Execution Time (us) : 10.913

# Benchmarking PyTorch: fill_
# Mode: Eager
# Name: fill__N2048_cpu_dtypetorch.int32
# Input: N: 2048, device: cpu, dtype: torch.int32
Run: 0, Forward Execution Time (us) : 5.190
Run: 1, Forward Execution Time (us) : 5.276
Run: 2, Forward Execution Time (us) : 5.267

# Benchmarking PyTorch: fill_
# Mode: Eager
# Name: fill__N2048_cuda_dtypetorch.int32
# Input: N: 2048, device: cuda, dtype: torch.int32
Run: 0, Forward Execution Time (us) : 10.802
Run: 1, Forward Execution Time (us) : 10.920
Run: 2, Forward Execution Time (us) : 10.853

+ python -m pt.linear_test --num-runs 3 --iterations 100 --warmup-iterations 10
not use dipu
# ----------------------------------------
# PyTorch/Caffe2 Operator Micro-benchmarks
# ----------------------------------------
# Tag : short

# Benchmarking PyTorch: linear
# Mode: Eager
# Name: linear_N1_IN1_OUT1_cpu
# Input: N: 1, IN: 1, OUT: 1, device: cpu
Run: 0, Forward Execution Time (us) : 25.407
Run: 1, Forward Execution Time (us) : 30.268
Run: 2, Forward Execution Time (us) : 30.010

# Benchmarking PyTorch: linear
# Mode: Eager
# Name: linear_N1_IN1_OUT1_cuda
# Input: N: 1, IN: 1, OUT: 1, device: cuda
Run: 0, Forward Execution Time (us) : 42.391
Run: 1, Forward Execution Time (us) : 50.145
Run: 2, Forward Execution Time (us) : 49.681

# Benchmarking PyTorch: linear
# Mode: Eager
# Name: linear_N4_IN256_OUT128_cpu
# Input: N: 4, IN: 256, OUT: 128, device: cpu
Run: 0, Forward Execution Time (us) : 55.105
Run: 1, Forward Execution Time (us) : 55.486
Run: 2, Forward Execution Time (us) : 55.419

# Benchmarking PyTorch: linear
# Mode: Eager
# Name: linear_N4_IN256_OUT128_cuda
# Input: N: 4, IN: 256, OUT: 128, device: cuda
Run: 0, Forward Execution Time (us) : 49.843
Run: 1, Forward Execution Time (us) : 50.165
Run: 2, Forward Execution Time (us) : 50.168

# Benchmarking PyTorch: linear
# Mode: Eager
# Name: linear_N16_IN512_OUT256_cpu
# Input: N: 16, IN: 512, OUT: 256, device: cpu
Run: 0, Forward Execution Time (us) : 187.624
Run: 1, Forward Execution Time (us) : 187.462
Run: 2, Forward Execution Time (us) : 187.715

# Benchmarking PyTorch: linear
# Mode: Eager
# Name: linear_N16_IN512_OUT256_cuda
# Input: N: 16, IN: 512, OUT: 256, device: cuda
Run: 0, Forward Execution Time (us) : 62.547
Run: 1, Forward Execution Time (us) : 62.911
Run: 2, Forward Execution Time (us) : 62.788

+ python -m pt.qtensor_method_test --num-runs 3 --iterations 100 --warmup-iterations 10
not use dipu
# ----------------------------------------
# PyTorch/Caffe2 Operator Micro-benchmarks
# ----------------------------------------
# Tag : short

# Benchmarking PyTorch: QMethodTensorInputCopyBenchmark
# Mode: Eager
# Name: QMethodTensorInputCopyBenchmark_M32_N32_dtypetorch.quint8_contigFalse
# Input: M: 32, N: 32, dtype: torch.quint8, contig: False
Run: 0, Forward Execution Time (us) : 2.741
Run: 1, Forward Execution Time (us) : 3.287
Run: 2, Forward Execution Time (us) : 3.145

# Benchmarking PyTorch: QMethodTensorInputCopyBenchmark
# Mode: Eager
# Name: QMethodTensorInputCopyBenchmark_M32_N32_dtypetorch.quint8_contigTrue
# Input: M: 32, N: 32, dtype: torch.quint8, contig: True
Run: 0, Forward Execution Time (us) : 3.081
Run: 1, Forward Execution Time (us) : 3.121
Run: 2, Forward Execution Time (us) : 3.221

+ python -m pt.batchnorm_test --num-runs 3 --iterations 100 --warmup-iterations 10
not use dipu
# ----------------------------------------
# PyTorch/Caffe2 Operator Micro-benchmarks
# ----------------------------------------
# Tag : short

# Benchmarking PyTorch: batchnorm
# Mode: Eager
# Name: batchnorm_M1_N256_K3136_cpu_trainingTrue_cudnnFalse
# Input: M: 1, N: 256, K: 3136, device: cpu, training: True, cudnn: False
Run: 0, Forward Execution Time (us) : 2111.917
Run: 1, Forward Execution Time (us) : 2915.554
Run: 2, Forward Execution Time (us) : 2896.921

# Benchmarking PyTorch: batchnorm
# Mode: Eager
# Name: batchnorm_M1_N256_K3136_cpu_trainingFalse_cudnnFalse
# Input: M: 1, N: 256, K: 3136, device: cpu, training: False, cudnn: False
Run: 0, Forward Execution Time (us) : 1259.540
Run: 1, Forward Execution Time (us) : 1258.550
Run: 2, Forward Execution Time (us) : 1250.487

# Benchmarking PyTorch: batchnorm
# Mode: Eager
# Name: batchnorm_M1_N256_K3136_cuda_trainingTrue_cudnnTrue
# Input: M: 1, N: 256, K: 3136, device: cuda, training: True, cudnn: True
Run: 0, Forward Execution Time (us) : 55.863
Run: 1, Forward Execution Time (us) : 66.055
Run: 2, Forward Execution Time (us) : 65.657

# Benchmarking PyTorch: batchnorm
# Mode: Eager
# Name: batchnorm_M1_N256_K3136_cuda_trainingTrue_cudnnFalse
# Input: M: 1, N: 256, K: 3136, device: cuda, training: True, cudnn: False
Run: 0, Forward Execution Time (us) : 79.532
Run: 1, Forward Execution Time (us) : 79.615
Run: 2, Forward Execution Time (us) : 79.615

# Benchmarking PyTorch: batchnorm
# Mode: Eager
# Name: batchnorm_M1_N256_K3136_cuda_trainingFalse_cudnnTrue
# Input: M: 1, N: 256, K: 3136, device: cuda, training: False, cudnn: True
Run: 0, Forward Execution Time (us) : 55.177
Run: 1, Forward Execution Time (us) : 55.555
Run: 2, Forward Execution Time (us) : 55.510

# Benchmarking PyTorch: batchnorm
# Mode: Eager
# Name: batchnorm_M1_N256_K3136_cuda_trainingFalse_cudnnFalse
# Input: M: 1, N: 256, K: 3136, device: cuda, training: False, cudnn: False
Run: 0, Forward Execution Time (us) : 74.605
Run: 1, Forward Execution Time (us) : 74.813
Run: 2, Forward Execution Time (us) : 75.027

# Benchmarking PyTorch: batchnorm
# Mode: Eager
# Name: batchnorm_M1_N256_K3136_cpu_trainingTrue_cudnnFalse_bwdall
# Input: M: 1, N: 256, K: 3136, device: cpu, training: True, cudnn: False
Run: 0, Backward Execution Time (us) : 1632.997
Run: 1, Backward Execution Time (us) : 1549.428
Run: 2, Backward Execution Time (us) : 1554.772

# Benchmarking PyTorch: batchnorm
# Mode: Eager
# Name: batchnorm_M1_N256_K3136_cpu_trainingTrue_cudnnFalse_bwd1
# Input: M: 1, N: 256, K: 3136, device: cpu, training: True, cudnn: False
Run: 0, Backward Execution Time (us) : 1579.037
Run: 1, Backward Execution Time (us) : 1542.193
Run: 2, Backward Execution Time (us) : 1549.458

# Benchmarking PyTorch: batchnorm
# Mode: Eager
# Name: batchnorm_M1_N256_K3136_cpu_trainingFalse_cudnnFalse_bwdall
# Input: M: 1, N: 256, K: 3136, device: cpu, training: False, cudnn: False
Run: 0, Backward Execution Time (us) : 1506.377
Run: 1, Backward Execution Time (us) : 1502.779
Run: 2, Backward Execution Time (us) : 1500.399

# Benchmarking PyTorch: batchnorm
# Mode: Eager
# Name: batchnorm_M1_N256_K3136_cpu_trainingFalse_cudnnFalse_bwd1
# Input: M: 1, N: 256, K: 3136, device: cpu, training: False, cudnn: False
Run: 0, Backward Execution Time (us) : 1503.569
Run: 1, Backward Execution Time (us) : 1538.845
Run: 2, Backward Execution Time (us) : 1534.882

# Benchmarking PyTorch: batchnorm
# Mode: Eager
# Name: batchnorm_M1_N256_K3136_cuda_trainingTrue_cudnnTrue_bwdall
# Input: M: 1, N: 256, K: 3136, device: cuda, training: True, cudnn: True
Run: 0, Backward Execution Time (us) : 135.117
Run: 1, Backward Execution Time (us) : 134.518
Run: 2, Backward Execution Time (us) : 133.245

# Benchmarking PyTorch: batchnorm
# Mode: Eager
# Name: batchnorm_M1_N256_K3136_cuda_trainingTrue_cudnnTrue_bwd1
# Input: M: 1, N: 256, K: 3136, device: cuda, training: True, cudnn: True
Run: 0, Backward Execution Time (us) : 134.247
Run: 1, Backward Execution Time (us) : 134.016
Run: 2, Backward Execution Time (us) : 134.394

# Benchmarking PyTorch: batchnorm
# Mode: Eager
# Name: batchnorm_M1_N256_K3136_cuda_trainingTrue_cudnnFalse_bwdall
# Input: M: 1, N: 256, K: 3136, device: cuda, training: True, cudnn: False
Run: 0, Backward Execution Time (us) : 120.810
Run: 1, Backward Execution Time (us) : 119.862
Run: 2, Backward Execution Time (us) : 121.038

# Benchmarking PyTorch: batchnorm
# Mode: Eager
# Name: batchnorm_M1_N256_K3136_cuda_trainingTrue_cudnnFalse_bwd1
# Input: M: 1, N: 256, K: 3136, device: cuda, training: True, cudnn: False
Run: 0, Backward Execution Time (us) : 120.801
Run: 1, Backward Execution Time (us) : 120.788
Run: 2, Backward Execution Time (us) : 120.678

# Benchmarking PyTorch: batchnorm
# Mode: Eager
# Name: batchnorm_M1_N256_K3136_cuda_trainingFalse_cudnnTrue_bwdall
# Input: M: 1, N: 256, K: 3136, device: cuda, training: False, cudnn: True
Run: 0, Backward Execution Time (us) : 125.410
Run: 1, Backward Execution Time (us) : 126.472
Run: 2, Backward Execution Time (us) : 125.515

# Benchmarking PyTorch: batchnorm
# Mode: Eager
# Name: batchnorm_M1_N256_K3136_cuda_trainingFalse_cudnnTrue_bwd1
# Input: M: 1, N: 256, K: 3136, device: cuda, training: False, cudnn: True
Run: 0, Backward Execution Time (us) : 126.156
Run: 1, Backward Execution Time (us) : 125.808
Run: 2, Backward Execution Time (us) : 125.792

# Benchmarking PyTorch: batchnorm
# Mode: Eager
# Name: batchnorm_M1_N256_K3136_cuda_trainingFalse_cudnnFalse_bwdall
# Input: M: 1, N: 256, K: 3136, device: cuda, training: False, cudnn: False
Run: 0, Backward Execution Time (us) : 117.671
Run: 1, Backward Execution Time (us) : 115.655
Run: 2, Backward Execution Time (us) : 116.598

# Benchmarking PyTorch: batchnorm
# Mode: Eager
# Name: batchnorm_M1_N256_K3136_cuda_trainingFalse_cudnnFalse_bwd1
# Input: M: 1, N: 256, K: 3136, device: cuda, training: False, cudnn: False
Run: 0, Backward Execution Time (us) : 116.231
Run: 1, Backward Execution Time (us) : 116.444
Run: 2, Backward Execution Time (us) : 116.404

# Benchmarking PyTorch: batchnorm
# Mode: Eager
# Name: batchnorm_N3136_C256_cpu_trainingTrue_cudnnFalse
# Input: N: 3136, C: 256, device: cpu, training: True, cudnn: False
Run: 0, Forward Execution Time (us) : 665.277
Run: 1, Forward Execution Time (us) : 668.806
Run: 2, Forward Execution Time (us) : 670.594

# Benchmarking PyTorch: batchnorm
# Mode: Eager
# Name: batchnorm_N3136_C256_cpu_trainingFalse_cudnnFalse
# Input: N: 3136, C: 256, device: cpu, training: False, cudnn: False
Run: 0, Forward Execution Time (us) : 357.272
Run: 1, Forward Execution Time (us) : 408.775
Run: 2, Forward Execution Time (us) : 409.729

# Benchmarking PyTorch: batchnorm
# Mode: Eager
# Name: batchnorm_N3136_C256_cuda_trainingTrue_cudnnTrue
# Input: N: 3136, C: 256, device: cuda, training: True, cudnn: True
Run: 0, Forward Execution Time (us) : 88.503
Run: 1, Forward Execution Time (us) : 89.609
Run: 2, Forward Execution Time (us) : 88.971

# Benchmarking PyTorch: batchnorm
# Mode: Eager
# Name: batchnorm_N3136_C256_cuda_trainingTrue_cudnnFalse
# Input: N: 3136, C: 256, device: cuda, training: True, cudnn: False
Run: 0, Forward Execution Time (us) : 88.803
Run: 1, Forward Execution Time (us) : 88.838
Run: 2, Forward Execution Time (us) : 88.894

# Benchmarking PyTorch: batchnorm
# Mode: Eager
# Name: batchnorm_N3136_C256_cuda_trainingFalse_cudnnTrue
# Input: N: 3136, C: 256, device: cuda, training: False, cudnn: True
Run: 0, Forward Execution Time (us) : 70.815
Run: 1, Forward Execution Time (us) : 71.078
Run: 2, Forward Execution Time (us) : 71.011

# Benchmarking PyTorch: batchnorm
# Mode: Eager
# Name: batchnorm_N3136_C256_cuda_trainingFalse_cudnnFalse
# Input: N: 3136, C: 256, device: cuda, training: False, cudnn: False
Run: 0, Forward Execution Time (us) : 70.964
Run: 1, Forward Execution Time (us) : 71.318
Run: 2, Forward Execution Time (us) : 71.246

# Benchmarking PyTorch: batchnorm
# Mode: Eager
# Name: batchnorm_N3136_C256_cpu_trainingTrue_cudnnFalse_bwdall
# Input: N: 3136, C: 256, device: cpu, training: True, cudnn: False
Run: 0, Backward Execution Time (us) : 2293.045
Run: 1, Backward Execution Time (us) : 2263.920
Run: 2, Backward Execution Time (us) : 2344.615

# Benchmarking PyTorch: batchnorm
# Mode: Eager
# Name: batchnorm_N3136_C256_cpu_trainingTrue_cudnnFalse_bwd1
# Input: N: 3136, C: 256, device: cpu, training: True, cudnn: False
Run: 0, Backward Execution Time (us) : 2259.433
Run: 1, Backward Execution Time (us) : 2271.577
Run: 2, Backward Execution Time (us) : 2283.019

# Benchmarking PyTorch: batchnorm
# Mode: Eager
# Name: batchnorm_N3136_C256_cpu_trainingFalse_cudnnFalse_bwdall
# Input: N: 3136, C: 256, device: cpu, training: False, cudnn: False
Run: 0, Backward Execution Time (us) : 1683.132
Run: 1, Backward Execution Time (us) : 1700.734
Run: 2, Backward Execution Time (us) : 1690.816

# Benchmarking PyTorch: batchnorm
# Mode: Eager
# Name: batchnorm_N3136_C256_cpu_trainingFalse_cudnnFalse_bwd1
# Input: N: 3136, C: 256, device: cpu, training: False, cudnn: False
Run: 0, Backward Execution Time (us) : 1704.495
Run: 1, Backward Execution Time (us) : 1703.352
Run: 2, Backward Execution Time (us) : 1823.852

# Benchmarking PyTorch: batchnorm
# Mode: Eager
# Name: batchnorm_N3136_C256_cuda_trainingTrue_cudnnTrue_bwdall
# Input: N: 3136, C: 256, device: cuda, training: True, cudnn: True
Run: 0, Backward Execution Time (us) : 147.616
Run: 1, Backward Execution Time (us) : 148.391
Run: 2, Backward Execution Time (us) : 147.775

# Benchmarking PyTorch: batchnorm
# Mode: Eager
# Name: batchnorm_N3136_C256_cuda_trainingTrue_cudnnTrue_bwd1
# Input: N: 3136, C: 256, device: cuda, training: True, cudnn: True
Run: 0, Backward Execution Time (us) : 147.928
Run: 1, Backward Execution Time (us) : 146.915
Run: 2, Backward Execution Time (us) : 148.033

# Benchmarking PyTorch: batchnorm
# Mode: Eager
# Name: batchnorm_N3136_C256_cuda_trainingTrue_cudnnFalse_bwdall
# Input: N: 3136, C: 256, device: cuda, training: True, cudnn: False
Run: 0, Backward Execution Time (us) : 147.222
Run: 1, Backward Execution Time (us) : 146.202
Run: 2, Backward Execution Time (us) : 147.298

# Benchmarking PyTorch: batchnorm
# Mode: Eager
# Name: batchnorm_N3136_C256_cuda_trainingTrue_cudnnFalse_bwd1
# Input: N: 3136, C: 256, device: cuda, training: True, cudnn: False
Run: 0, Backward Execution Time (us) : 147.285
Run: 1, Backward Execution Time (us) : 147.515
Run: 2, Backward Execution Time (us) : 147.709

# Benchmarking PyTorch: batchnorm
# Mode: Eager
# Name: batchnorm_N3136_C256_cuda_trainingFalse_cudnnTrue_bwdall
# Input: N: 3136, C: 256, device: cuda, training: False, cudnn: True
Run: 0, Backward Execution Time (us) : 115.647
Run: 1, Backward Execution Time (us) : 116.156
Run: 2, Backward Execution Time (us) : 115.593

# Benchmarking PyTorch: batchnorm
# Mode: Eager
# Name: batchnorm_N3136_C256_cuda_trainingFalse_cudnnTrue_bwd1
# Input: N: 3136, C: 256, device: cuda, training: False, cudnn: True
Run: 0, Backward Execution Time (us) : 117.454
Run: 1, Backward Execution Time (us) : 117.779
Run: 2, Backward Execution Time (us) : 116.159

# Benchmarking PyTorch: batchnorm
# Mode: Eager
# Name: batchnorm_N3136_C256_cuda_trainingFalse_cudnnFalse_bwdall
# Input: N: 3136, C: 256, device: cuda, training: False, cudnn: False
Run: 0, Backward Execution Time (us) : 116.193
Run: 1, Backward Execution Time (us) : 116.276
Run: 2, Backward Execution Time (us) : 116.235

# Benchmarking PyTorch: batchnorm
# Mode: Eager
# Name: batchnorm_N3136_C256_cuda_trainingFalse_cudnnFalse_bwd1
# Input: N: 3136, C: 256, device: cuda, training: False, cudnn: False
Run: 0, Backward Execution Time (us) : 116.638
Run: 1, Backward Execution Time (us) : 116.767
Run: 2, Backward Execution Time (us) : 116.075

+ python -m pt.gather_test --num-runs 3 --iterations 100 --warmup-iterations 10
not use dipu
# ----------------------------------------
# PyTorch/Caffe2 Operator Micro-benchmarks
# ----------------------------------------
# Tag : short

# Benchmarking PyTorch: gather
# Mode: Eager
# Name: gather_M256_N512_dim0_cpu
# Input: M: 256, N: 512, dim: 0, device: cpu
Run: 0, Forward Execution Time (us) : 269.718
Run: 1, Forward Execution Time (us) : 309.027
Run: 2, Forward Execution Time (us) : 308.488

# Benchmarking PyTorch: gather
# Mode: Eager
# Name: gather_M256_N512_dim0_cuda
# Input: M: 256, N: 512, dim: 0, device: cuda
Run: 0, Forward Execution Time (us) : 16.836
Run: 1, Forward Execution Time (us) : 20.051
Run: 2, Forward Execution Time (us) : 19.907

# Benchmarking PyTorch: gather
# Mode: Eager
# Name: gather_M512_N512_dim1_cpu
# Input: M: 512, N: 512, dim: 1, device: cpu
Run: 0, Forward Execution Time (us) : 361.107
Run: 1, Forward Execution Time (us) : 419.265
Run: 2, Forward Execution Time (us) : 418.873

# Benchmarking PyTorch: gather
# Mode: Eager
# Name: gather_M512_N512_dim1_cuda
# Input: M: 512, N: 512, dim: 1, device: cuda
Run: 0, Forward Execution Time (us) : 16.732
Run: 1, Forward Execution Time (us) : 19.825
Run: 2, Forward Execution Time (us) : 19.932

+ python -m pt.quantization_test --num-runs 3 --iterations 100 --warmup-iterations 10
not use dipu
# ----------------------------------------
# PyTorch/Caffe2 Operator Micro-benchmarks
# ----------------------------------------
# Tag : short

# Benchmarking PyTorch: QuantizePerTensor
# Mode: Eager
# Name: QuantizePerTensor_C3_M512_N512_dtypetorch.quint8_modeQ
# Input: C: 3, M: 512, N: 512, dtype: torch.quint8, mode: Q
Run: 0, Forward Execution Time (us) : 16385.060
Run: 1, Forward Execution Time (us) : 16328.297
Run: 2, Forward Execution Time (us) : 16359.732

# Benchmarking PyTorch: DequantizePerTensor
# Mode: Eager
# Name: DequantizePerTensor_C3_M512_N512_dtypetorch.quint8_modeD
# Input: C: 3, M: 512, N: 512, dtype: torch.quint8, mode: D
Run: 0, Forward Execution Time (us) : 9052.264
Run: 1, Forward Execution Time (us) : 9063.544
Run: 2, Forward Execution Time (us) : 8998.964

# Benchmarking PyTorch: QuantizePerChannel
# Mode: Eager
# Name: QuantizePerChannel_C3_M512_N512_dtypetorch.quint8_modeQ_axis0
# Input: C: 3, M: 512, N: 512, dtype: torch.quint8, mode: Q, axis: 0
Run: 0, Forward Execution Time (us) : 16371.732
Run: 1, Forward Execution Time (us) : 16372.845
Run: 2, Forward Execution Time (us) : 16370.577

# Benchmarking PyTorch: DequantizePerChannel
# Mode: Eager
# Name: DequantizePerChannel_C3_M512_N512_dtypetorch.quint8_modeD_axis0
# Input: C: 3, M: 512, N: 512, dtype: torch.quint8, mode: D, axis: 0
Run: 0, Forward Execution Time (us) : 1226.448
Run: 1, Forward Execution Time (us) : 1227.310
Run: 2, Forward Execution Time (us) : 1227.358

# Benchmarking PyTorch: FakeQuantize
# Mode: Eager
# Name: FakeQuantize_N1_C3_H512_W512_zero_point_dtypetorch.int32_cpu
# Input: N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, device: cpu
Run: 0, Forward Execution Time (us) : 26533.377
Run: 1, Forward Execution Time (us) : 26577.752
Run: 2, Forward Execution Time (us) : 26529.981

# Benchmarking PyTorch: FakeQuantize
# Mode: Eager
# Name: FakeQuantize_N1_C3_H512_W512_zero_point_dtypetorch.int32_cuda
# Input: N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, device: cuda
Run: 0, Forward Execution Time (us) : 656.926
Run: 1, Forward Execution Time (us) : 757.237
Run: 2, Forward Execution Time (us) : 756.207

# Benchmarking PyTorch: learnable_kernel
# Mode: Eager
# Name: learnable_kernel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits4_cpu
# Input: N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 4, device: cpu
Run: 0, Forward Execution Time (us) : 25857.868
Run: 1, Forward Execution Time (us) : 25846.247
Run: 2, Forward Execution Time (us) : 25850.199

# Benchmarking PyTorch: learnable_kernel
# Mode: Eager
# Name: learnable_kernel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits4_cuda
# Input: N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 4, device: cuda
Run: 0, Forward Execution Time (us) : 59.140
Run: 1, Forward Execution Time (us) : 59.699
Run: 2, Forward Execution Time (us) : 59.457

# Benchmarking PyTorch: learnable_kernel
# Mode: Eager
# Name: learnable_kernel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits8_cpu
# Input: N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 8, device: cpu
Run: 0, Forward Execution Time (us) : 25846.602
Run: 1, Forward Execution Time (us) : 25907.225
Run: 2, Forward Execution Time (us) : 25854.504

# Benchmarking PyTorch: learnable_kernel
# Mode: Eager
# Name: learnable_kernel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits8_cuda
# Input: N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 8, device: cuda
Run: 0, Forward Execution Time (us) : 59.386
Run: 1, Forward Execution Time (us) : 59.698
Run: 2, Forward Execution Time (us) : 59.503

# Benchmarking PyTorch: original_kernel
# Mode: Eager
# Name: original_kernel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits4_cpu
# Input: N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 4, device: cpu
Run: 0, Forward Execution Time (us) : 25848.708
Run: 1, Forward Execution Time (us) : 25839.011
Run: 2, Forward Execution Time (us) : 25848.906

# Benchmarking PyTorch: original_kernel
# Mode: Eager
# Name: original_kernel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits4_cuda
# Input: N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 4, device: cuda
Run: 0, Forward Execution Time (us) : 24.093
Run: 1, Forward Execution Time (us) : 24.245
Run: 2, Forward Execution Time (us) : 24.222

# Benchmarking PyTorch: original_kernel
# Mode: Eager
# Name: original_kernel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits8_cpu
# Input: N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 8, device: cpu
Run: 0, Forward Execution Time (us) : 25850.783
Run: 1, Forward Execution Time (us) : 25845.216
Run: 2, Forward Execution Time (us) : 25886.681

# Benchmarking PyTorch: original_kernel
# Mode: Eager
# Name: original_kernel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits8_cuda
# Input: N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 8, device: cuda
Run: 0, Forward Execution Time (us) : 24.045
Run: 1, Forward Execution Time (us) : 24.193
Run: 2, Forward Execution Time (us) : 24.193

# Benchmarking PyTorch: learnable_kernel
# Mode: Eager
# Name: learnable_kernel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits4_cpu_bwdall
# Input: N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 4, device: cpu
Run: 0, Backward Execution Time (us) : 13103.326
Run: 1, Backward Execution Time (us) : 13103.024
Run: 2, Backward Execution Time (us) : 13150.378

# Benchmarking PyTorch: learnable_kernel
# Mode: Eager
# Name: learnable_kernel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits4_cpu_bwd1
# Input: N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 4, device: cpu
Run: 0, Backward Execution Time (us) : 13108.309
Run: 1, Backward Execution Time (us) : 13129.363
Run: 2, Backward Execution Time (us) : 13102.681

# Benchmarking PyTorch: learnable_kernel
# Mode: Eager
# Name: learnable_kernel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits4_cpu_bwd2
# Input: N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 4, device: cpu
Run: 0, Backward Execution Time (us) : 13043.174
Run: 1, Backward Execution Time (us) : 13142.812
Run: 2, Backward Execution Time (us) : 13083.727

# Benchmarking PyTorch: learnable_kernel
# Mode: Eager
# Name: learnable_kernel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits4_cpu_bwd3
# Input: N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 4, device: cpu
Run: 0, Backward Execution Time (us) : 13112.950
Run: 1, Backward Execution Time (us) : 13132.723
Run: 2, Backward Execution Time (us) : 13143.148

# Benchmarking PyTorch: learnable_kernel
# Mode: Eager
# Name: learnable_kernel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits4_cuda_bwdall
# Input: N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 4, device: cuda
Run: 0, Backward Execution Time (us) : 249.481
Run: 1, Backward Execution Time (us) : 248.460
Run: 2, Backward Execution Time (us) : 248.167

# Benchmarking PyTorch: learnable_kernel
# Mode: Eager
# Name: learnable_kernel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits4_cuda_bwd1
# Input: N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 4, device: cuda
Run: 0, Backward Execution Time (us) : 248.489
Run: 1, Backward Execution Time (us) : 250.886
Run: 2, Backward Execution Time (us) : 247.641

# Benchmarking PyTorch: learnable_kernel
# Mode: Eager
# Name: learnable_kernel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits4_cuda_bwd2
# Input: N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 4, device: cuda
Run: 0, Backward Execution Time (us) : 246.370
Run: 1, Backward Execution Time (us) : 246.212
Run: 2, Backward Execution Time (us) : 246.385

# Benchmarking PyTorch: learnable_kernel
# Mode: Eager
# Name: learnable_kernel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits4_cuda_bwd3
# Input: N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 4, device: cuda
Run: 0, Backward Execution Time (us) : 246.944
Run: 1, Backward Execution Time (us) : 246.868
Run: 2, Backward Execution Time (us) : 246.473

# Benchmarking PyTorch: learnable_kernel
# Mode: Eager
# Name: learnable_kernel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits8_cpu_bwdall
# Input: N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 8, device: cpu
Run: 0, Backward Execution Time (us) : 13105.614
Run: 1, Backward Execution Time (us) : 13063.787
Run: 2, Backward Execution Time (us) : 13064.459

# Benchmarking PyTorch: learnable_kernel
# Mode: Eager
# Name: learnable_kernel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits8_cpu_bwd1
# Input: N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 8, device: cpu
Run: 0, Backward Execution Time (us) : 13048.248
Run: 1, Backward Execution Time (us) : 13143.445
Run: 2, Backward Execution Time (us) : 13104.356

# Benchmarking PyTorch: learnable_kernel
# Mode: Eager
# Name: learnable_kernel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits8_cpu_bwd2
# Input: N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 8, device: cpu
Run: 0, Backward Execution Time (us) : 13084.686
Run: 1, Backward Execution Time (us) : 13139.060
Run: 2, Backward Execution Time (us) : 13087.855

# Benchmarking PyTorch: learnable_kernel
# Mode: Eager
# Name: learnable_kernel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits8_cpu_bwd3
# Input: N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 8, device: cpu
Run: 0, Backward Execution Time (us) : 13131.481
Run: 1, Backward Execution Time (us) : 13090.657
Run: 2, Backward Execution Time (us) : 13068.654

# Benchmarking PyTorch: learnable_kernel
# Mode: Eager
# Name: learnable_kernel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits8_cuda_bwdall
# Input: N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 8, device: cuda
Run: 0, Backward Execution Time (us) : 247.867
Run: 1, Backward Execution Time (us) : 247.175
Run: 2, Backward Execution Time (us) : 247.265

# Benchmarking PyTorch: learnable_kernel
# Mode: Eager
# Name: learnable_kernel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits8_cuda_bwd1
# Input: N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 8, device: cuda
Run: 0, Backward Execution Time (us) : 246.838
Run: 1, Backward Execution Time (us) : 248.550
Run: 2, Backward Execution Time (us) : 246.938

# Benchmarking PyTorch: learnable_kernel
# Mode: Eager
# Name: learnable_kernel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits8_cuda_bwd2
# Input: N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 8, device: cuda
Run: 0, Backward Execution Time (us) : 247.088
Run: 1, Backward Execution Time (us) : 247.696
Run: 2, Backward Execution Time (us) : 247.176

# Benchmarking PyTorch: learnable_kernel
# Mode: Eager
# Name: learnable_kernel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits8_cuda_bwd3
# Input: N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 8, device: cuda
Run: 0, Backward Execution Time (us) : 247.288
Run: 1, Backward Execution Time (us) : 246.898
Run: 2, Backward Execution Time (us) : 246.767

# Benchmarking PyTorch: original_kernel
# Mode: Eager
# Name: original_kernel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits4_cpu_bwdall
# Input: N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 4, device: cpu
Run: 0, Backward Execution Time (us) : 2239.075
Run: 1, Backward Execution Time (us) : 2242.357
Run: 2, Backward Execution Time (us) : 2232.642

# Benchmarking PyTorch: original_kernel
# Mode: Eager
# Name: original_kernel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits4_cpu_bwd1
# Input: N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 4, device: cpu
Run: 0, Backward Execution Time (us) : 2228.170
Run: 1, Backward Execution Time (us) : 2241.462
Run: 2, Backward Execution Time (us) : 2228.501

# Benchmarking PyTorch: original_kernel
# Mode: Eager
# Name: original_kernel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits4_cpu_bwd2
# Input: N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 4, device: cpu
Run: 0, Backward Execution Time (us) : 2226.867
Run: 1, Backward Execution Time (us) : 2218.830
Run: 2, Backward Execution Time (us) : 2227.570

# Benchmarking PyTorch: original_kernel
# Mode: Eager
# Name: original_kernel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits4_cpu_bwd3
# Input: N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 4, device: cpu
Run: 0, Backward Execution Time (us) : 2237.652
Run: 1, Backward Execution Time (us) : 2229.165
Run: 2, Backward Execution Time (us) : 2235.087

# Benchmarking PyTorch: original_kernel
# Mode: Eager
# Name: original_kernel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits4_cuda_bwdall
# Input: N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 4, device: cuda
Run: 0, Backward Execution Time (us) : 127.617
Run: 1, Backward Execution Time (us) : 127.514
Run: 2, Backward Execution Time (us) : 128.322

# Benchmarking PyTorch: original_kernel
# Mode: Eager
# Name: original_kernel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits4_cuda_bwd1
# Input: N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 4, device: cuda
Run: 0, Backward Execution Time (us) : 125.574
Run: 1, Backward Execution Time (us) : 127.088
Run: 2, Backward Execution Time (us) : 124.922

# Benchmarking PyTorch: original_kernel
# Mode: Eager
# Name: original_kernel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits4_cuda_bwd2
# Input: N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 4, device: cuda
Run: 0, Backward Execution Time (us) : 127.021
Run: 1, Backward Execution Time (us) : 126.628
Run: 2, Backward Execution Time (us) : 126.231

# Benchmarking PyTorch: original_kernel
# Mode: Eager
# Name: original_kernel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits4_cuda_bwd3
# Input: N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 4, device: cuda
Run: 0, Backward Execution Time (us) : 125.569
Run: 1, Backward Execution Time (us) : 126.745
Run: 2, Backward Execution Time (us) : 126.098

# Benchmarking PyTorch: original_kernel
# Mode: Eager
# Name: original_kernel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits8_cpu_bwdall
# Input: N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 8, device: cpu
Run: 0, Backward Execution Time (us) : 2234.511
Run: 1, Backward Execution Time (us) : 2239.504
Run: 2, Backward Execution Time (us) : 2228.132

# Benchmarking PyTorch: original_kernel
# Mode: Eager
# Name: original_kernel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits8_cpu_bwd1
# Input: N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 8, device: cpu
Run: 0, Backward Execution Time (us) : 2228.128
Run: 1, Backward Execution Time (us) : 2232.444
Run: 2, Backward Execution Time (us) : 2231.165

# Benchmarking PyTorch: original_kernel
# Mode: Eager
# Name: original_kernel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits8_cpu_bwd2
# Input: N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 8, device: cpu
Run: 0, Backward Execution Time (us) : 2246.903
Run: 1, Backward Execution Time (us) : 2218.996
Run: 2, Backward Execution Time (us) : 2222.770

# Benchmarking PyTorch: original_kernel
# Mode: Eager
# Name: original_kernel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits8_cpu_bwd3
# Input: N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 8, device: cpu
Run: 0, Backward Execution Time (us) : 2238.895
Run: 1, Backward Execution Time (us) : 2228.609
Run: 2, Backward Execution Time (us) : 2231.604

# Benchmarking PyTorch: original_kernel
# Mode: Eager
# Name: original_kernel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits8_cuda_bwdall
# Input: N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 8, device: cuda
Run: 0, Backward Execution Time (us) : 127.912
Run: 1, Backward Execution Time (us) : 126.612
Run: 2, Backward Execution Time (us) : 128.073

# Benchmarking PyTorch: original_kernel
# Mode: Eager
# Name: original_kernel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits8_cuda_bwd1
# Input: N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 8, device: cuda
Run: 0, Backward Execution Time (us) : 126.086
Run: 1, Backward Execution Time (us) : 127.572
Run: 2, Backward Execution Time (us) : 125.681

# Benchmarking PyTorch: original_kernel
# Mode: Eager
# Name: original_kernel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits8_cuda_bwd2
# Input: N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 8, device: cuda
Run: 0, Backward Execution Time (us) : 128.671
Run: 1, Backward Execution Time (us) : 127.964
Run: 2, Backward Execution Time (us) : 127.316

# Benchmarking PyTorch: original_kernel
# Mode: Eager
# Name: original_kernel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits8_cuda_bwd3
# Input: N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 8, device: cuda
Run: 0, Backward Execution Time (us) : 126.823
Run: 1, Backward Execution Time (us) : 127.536
Run: 2, Backward Execution Time (us) : 127.855

# Benchmarking PyTorch: learnable_kernel
# Mode: Eager
# Name: learnable_kernel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits4_cpu
# Input: N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 4, device: cpu
Run: 0, Forward Execution Time (us) : 34929.233
Run: 1, Forward Execution Time (us) : 34922.033
Run: 2, Forward Execution Time (us) : 34931.990

# Benchmarking PyTorch: learnable_kernel
# Mode: Eager
# Name: learnable_kernel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits4_cuda
# Input: N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 4, device: cuda
Run: 0, Forward Execution Time (us) : 141.172
Run: 1, Forward Execution Time (us) : 141.874
Run: 2, Forward Execution Time (us) : 141.967

# Benchmarking PyTorch: learnable_kernel
# Mode: Eager
# Name: learnable_kernel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits8_cpu
# Input: N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 8, device: cpu
Run: 0, Forward Execution Time (us) : 34924.428
Run: 1, Forward Execution Time (us) : 34916.641
Run: 2, Forward Execution Time (us) : 34956.476

# Benchmarking PyTorch: learnable_kernel
# Mode: Eager
# Name: learnable_kernel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits8_cuda
# Input: N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 8, device: cuda
Run: 0, Forward Execution Time (us) : 141.751
Run: 1, Forward Execution Time (us) : 141.897
Run: 2, Forward Execution Time (us) : 142.104

# Benchmarking PyTorch: original_kernel
# Mode: Eager
# Name: original_kernel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits4_cpu
# Input: N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 4, device: cpu
Run: 0, Forward Execution Time (us) : 34915.079
Run: 1, Forward Execution Time (us) : 34907.466
Run: 2, Forward Execution Time (us) : 34916.372

# Benchmarking PyTorch: original_kernel
# Mode: Eager
# Name: original_kernel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits4_cuda
# Input: N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 4, device: cuda
Run: 0, Forward Execution Time (us) : 112.889
Run: 1, Forward Execution Time (us) : 113.437
Run: 2, Forward Execution Time (us) : 113.662

# Benchmarking PyTorch: original_kernel
# Mode: Eager
# Name: original_kernel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits8_cpu
# Input: N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 8, device: cpu
Run: 0, Forward Execution Time (us) : 34911.058
Run: 1, Forward Execution Time (us) : 34936.483
Run: 2, Forward Execution Time (us) : 34961.284

# Benchmarking PyTorch: original_kernel
# Mode: Eager
# Name: original_kernel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits8_cuda
# Input: N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 8, device: cuda
Run: 0, Forward Execution Time (us) : 114.164
Run: 1, Forward Execution Time (us) : 114.380
Run: 2, Forward Execution Time (us) : 114.351

# Benchmarking PyTorch: learnable_kernel
# Mode: Eager
# Name: learnable_kernel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits4_cpu_bwdall
# Input: N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 4, device: cpu
Run: 0, Backward Execution Time (us) : 13776.649
Run: 1, Backward Execution Time (us) : 13806.289
Run: 2, Backward Execution Time (us) : 13837.074

# Benchmarking PyTorch: learnable_kernel
# Mode: Eager
# Name: learnable_kernel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits4_cpu_bwd1
# Input: N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 4, device: cpu
Run: 0, Backward Execution Time (us) : 13672.834
Run: 1, Backward Execution Time (us) : 13868.283
Run: 2, Backward Execution Time (us) : 13821.419

# Benchmarking PyTorch: learnable_kernel
# Mode: Eager
# Name: learnable_kernel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits4_cpu_bwd2
# Input: N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 4, device: cpu
Run: 0, Backward Execution Time (us) : 13592.549
Run: 1, Backward Execution Time (us) : 13641.298
Run: 2, Backward Execution Time (us) : 13755.513

# Benchmarking PyTorch: learnable_kernel
# Mode: Eager
# Name: learnable_kernel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits4_cpu_bwd3
# Input: N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 4, device: cpu
Run: 0, Backward Execution Time (us) : 13877.138
Run: 1, Backward Execution Time (us) : 13902.881
Run: 2, Backward Execution Time (us) : 13755.323

# Benchmarking PyTorch: learnable_kernel
# Mode: Eager
# Name: learnable_kernel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits4_cuda_bwdall
# Input: N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 4, device: cuda
Run: 0, Backward Execution Time (us) : 292.325
Run: 1, Backward Execution Time (us) : 298.454
Run: 2, Backward Execution Time (us) : 296.045

# Benchmarking PyTorch: learnable_kernel
# Mode: Eager
# Name: learnable_kernel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits4_cuda_bwd1
# Input: N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 4, device: cuda
Run: 0, Backward Execution Time (us) : 295.860
Run: 1, Backward Execution Time (us) : 297.502
Run: 2, Backward Execution Time (us) : 295.058

# Benchmarking PyTorch: learnable_kernel
# Mode: Eager
# Name: learnable_kernel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits4_cuda_bwd2
# Input: N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 4, device: cuda
Run: 0, Backward Execution Time (us) : 293.321
Run: 1, Backward Execution Time (us) : 295.597
Run: 2, Backward Execution Time (us) : 293.367

# Benchmarking PyTorch: learnable_kernel
# Mode: Eager
# Name: learnable_kernel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits4_cuda_bwd3
# Input: N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 4, device: cuda
Run: 0, Backward Execution Time (us) : 295.572
Run: 1, Backward Execution Time (us) : 294.794
Run: 2, Backward Execution Time (us) : 293.991

# Benchmarking PyTorch: learnable_kernel
# Mode: Eager
# Name: learnable_kernel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits8_cpu_bwdall
# Input: N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 8, device: cpu
Run: 0, Backward Execution Time (us) : 13877.030
Run: 1, Backward Execution Time (us) : 13791.718
Run: 2, Backward Execution Time (us) : 13822.485

# Benchmarking PyTorch: learnable_kernel
# Mode: Eager
# Name: learnable_kernel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits8_cpu_bwd1
# Input: N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 8, device: cpu
Run: 0, Backward Execution Time (us) : 13888.993
Run: 1, Backward Execution Time (us) : 13778.622
Run: 2, Backward Execution Time (us) : 13852.062

# Benchmarking PyTorch: learnable_kernel
# Mode: Eager
# Name: learnable_kernel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits8_cpu_bwd2
# Input: N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 8, device: cpu
Run: 0, Backward Execution Time (us) : 13802.235
Run: 1, Backward Execution Time (us) : 13824.680
Run: 2, Backward Execution Time (us) : 13786.243

# Benchmarking PyTorch: learnable_kernel
# Mode: Eager
# Name: learnable_kernel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits8_cpu_bwd3
# Input: N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 8, device: cpu
Run: 0, Backward Execution Time (us) : 13855.230
Run: 1, Backward Execution Time (us) : 13606.287
Run: 2, Backward Execution Time (us) : 13615.677

# Benchmarking PyTorch: learnable_kernel
# Mode: Eager
# Name: learnable_kernel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits8_cuda_bwdall
# Input: N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 8, device: cuda
Run: 0, Backward Execution Time (us) : 296.794
Run: 1, Backward Execution Time (us) : 297.737
Run: 2, Backward Execution Time (us) : 295.432

# Benchmarking PyTorch: learnable_kernel
# Mode: Eager
# Name: learnable_kernel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits8_cuda_bwd1
# Input: N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 8, device: cuda
Run: 0, Backward Execution Time (us) : 296.287
Run: 1, Backward Execution Time (us) : 298.223
Run: 2, Backward Execution Time (us) : 295.004

# Benchmarking PyTorch: learnable_kernel
# Mode: Eager
# Name: learnable_kernel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits8_cuda_bwd2
# Input: N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 8, device: cuda
Run: 0, Backward Execution Time (us) : 294.131
Run: 1, Backward Execution Time (us) : 294.341
Run: 2, Backward Execution Time (us) : 292.660

# Benchmarking PyTorch: learnable_kernel
# Mode: Eager
# Name: learnable_kernel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits8_cuda_bwd3
# Input: N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 8, device: cuda
Run: 0, Backward Execution Time (us) : 295.378
Run: 1, Backward Execution Time (us) : 293.651
Run: 2, Backward Execution Time (us) : 291.120

# Benchmarking PyTorch: original_kernel
# Mode: Eager
# Name: original_kernel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits4_cpu_bwdall
# Input: N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 4, device: cpu
Run: 0, Backward Execution Time (us) : 2241.105
Run: 1, Backward Execution Time (us) : 2246.037
Run: 2, Backward Execution Time (us) : 2233.981

# Benchmarking PyTorch: original_kernel
# Mode: Eager
# Name: original_kernel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits4_cpu_bwd1
# Input: N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 4, device: cpu
Run: 0, Backward Execution Time (us) : 2233.320
Run: 1, Backward Execution Time (us) : 2249.206
Run: 2, Backward Execution Time (us) : 2233.792

# Benchmarking PyTorch: original_kernel
# Mode: Eager
# Name: original_kernel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits4_cuda_bwdall
# Input: N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 4, device: cuda
Run: 0, Backward Execution Time (us) : 112.354
Run: 1, Backward Execution Time (us) : 112.542
Run: 2, Backward Execution Time (us) : 111.833

# Benchmarking PyTorch: original_kernel
# Mode: Eager
# Name: original_kernel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits4_cuda_bwd1
# Input: N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 4, device: cuda
Run: 0, Backward Execution Time (us) : 111.158
Run: 1, Backward Execution Time (us) : 112.170
Run: 2, Backward Execution Time (us) : 111.198

# Benchmarking PyTorch: original_kernel
# Mode: Eager
# Name: original_kernel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits8_cpu_bwdall
# Input: N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 8, device: cpu
Run: 0, Backward Execution Time (us) : 2228.243
Run: 1, Backward Execution Time (us) : 2272.161
Run: 2, Backward Execution Time (us) : 2251.840

# Benchmarking PyTorch: original_kernel
# Mode: Eager
# Name: original_kernel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits8_cpu_bwd1
# Input: N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 8, device: cpu
Run: 0, Backward Execution Time (us) : 2225.271
Run: 1, Backward Execution Time (us) : 2243.894
Run: 2, Backward Execution Time (us) : 2232.986

# Benchmarking PyTorch: original_kernel
# Mode: Eager
# Name: original_kernel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits8_cuda_bwdall
# Input: N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 8, device: cuda
Run: 0, Backward Execution Time (us) : 113.122
Run: 1, Backward Execution Time (us) : 112.043
Run: 2, Backward Execution Time (us) : 112.124

# Benchmarking PyTorch: original_kernel
# Mode: Eager
# Name: original_kernel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits8_cuda_bwd1
# Input: N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 8, device: cuda
Run: 0, Backward Execution Time (us) : 111.306
Run: 1, Backward Execution Time (us) : 112.377
Run: 2, Backward Execution Time (us) : 111.422

+ python -m pt.binary_test --num-runs 3 --iterations 100 --warmup-iterations 10
not use dipu
# ----------------------------------------
# PyTorch/Caffe2 Operator Micro-benchmarks
# ----------------------------------------
# Tag : short

# Benchmarking PyTorch: add
# Mode: Eager
# Name: add_in_one[64,1,64]_in_two[1,64,1]_cpu_dtypetorch.float32
# Input: in_one: [64, 1, 64], in_two: [1, 64, 1], device: cpu, dtype: torch.float32
Run: 0, Forward Execution Time (us) : 70.770
Run: 1, Forward Execution Time (us) : 64.267
Run: 2, Forward Execution Time (us) : 64.145

# Benchmarking PyTorch: add
# Mode: Eager
# Name: add_M1_N1_K1_cpu_dtype_onetorch.int32_dtype_twotorch.int32
# Input: M: 1, N: 1, K: 1, device: cpu, dtype_one: torch.int32, dtype_two: torch.int32
Run: 0, Forward Execution Time (us) : 7.694
Run: 1, Forward Execution Time (us) : 7.624
Run: 2, Forward Execution Time (us) : 7.583

# Benchmarking PyTorch: add
# Mode: Eager
# Name: add_M1_N1_K1_cuda_dtype_onetorch.int32_dtype_twotorch.int32
# Input: M: 1, N: 1, K: 1, device: cuda, dtype_one: torch.int32, dtype_two: torch.int32
Run: 0, Forward Execution Time (us) : 13.146
Run: 1, Forward Execution Time (us) : 15.769
Run: 2, Forward Execution Time (us) : 15.635

# Benchmarking PyTorch: add
# Mode: Eager
# Name: add_M64_N64_K64_cpu_dtype_onetorch.int32_dtype_twotorch.int32
# Input: M: 64, N: 64, K: 64, device: cpu, dtype_one: torch.int32, dtype_two: torch.int32
Run: 0, Forward Execution Time (us) : 132.862
Run: 1, Forward Execution Time (us) : 133.095
Run: 2, Forward Execution Time (us) : 132.969

# Benchmarking PyTorch: add
# Mode: Eager
# Name: add_M64_N64_K64_cuda_dtype_onetorch.int32_dtype_twotorch.int32
# Input: M: 64, N: 64, K: 64, device: cuda, dtype_one: torch.int32, dtype_two: torch.int32
Run: 0, Forward Execution Time (us) : 15.254
Run: 1, Forward Execution Time (us) : 15.503
Run: 2, Forward Execution Time (us) : 15.338

# Benchmarking PyTorch: add
# Mode: Eager
# Name: add_M64_N64_K128_cpu_dtype_onetorch.int32_dtype_twotorch.int32
# Input: M: 64, N: 64, K: 128, device: cpu, dtype_one: torch.int32, dtype_two: torch.int32
Run: 0, Forward Execution Time (us) : 256.323
Run: 1, Forward Execution Time (us) : 256.375
Run: 2, Forward Execution Time (us) : 256.379

# Benchmarking PyTorch: add
# Mode: Eager
# Name: add_M64_N64_K128_cuda_dtype_onetorch.int32_dtype_twotorch.int32
# Input: M: 64, N: 64, K: 128, device: cuda, dtype_one: torch.int32, dtype_two: torch.int32
Run: 0, Forward Execution Time (us) : 15.324
Run: 1, Forward Execution Time (us) : 15.582
Run: 2, Forward Execution Time (us) : 15.603

# Benchmarking PyTorch: copy_
# Mode: Eager
# Name: copy__M1_N1_K1_cpu_dtype_onetorch.int32_dtype_twotorch.int32
# Input: M: 1, N: 1, K: 1, device: cpu, dtype_one: torch.int32, dtype_two: torch.int32
Run: 0, Forward Execution Time (us) : 4.111
Run: 1, Forward Execution Time (us) : 4.992
Run: 2, Forward Execution Time (us) : 4.887

# Benchmarking PyTorch: copy_
# Mode: Eager
# Name: copy__M1_N1_K1_cuda_dtype_onetorch.int32_dtype_twotorch.int32
# Input: M: 1, N: 1, K: 1, device: cuda, dtype_one: torch.int32, dtype_two: torch.int32
Run: 0, Forward Execution Time (us) : 12.415
Run: 1, Forward Execution Time (us) : 12.479
Run: 2, Forward Execution Time (us) : 12.438

# Benchmarking PyTorch: copy_
# Mode: Eager
# Name: copy__M64_N64_K64_cpu_dtype_onetorch.int32_dtype_twotorch.int32
# Input: M: 64, N: 64, K: 64, device: cpu, dtype_one: torch.int32, dtype_two: torch.int32
Run: 0, Forward Execution Time (us) : 85.228
Run: 1, Forward Execution Time (us) : 87.738
Run: 2, Forward Execution Time (us) : 125.989

# Benchmarking PyTorch: copy_
# Mode: Eager
# Name: copy__M64_N64_K64_cuda_dtype_onetorch.int32_dtype_twotorch.int32
# Input: M: 64, N: 64, K: 64, device: cuda, dtype_one: torch.int32, dtype_two: torch.int32
Run: 0, Forward Execution Time (us) : 12.256
Run: 1, Forward Execution Time (us) : 12.460
Run: 2, Forward Execution Time (us) : 12.334

# Benchmarking PyTorch: copy_
# Mode: Eager
# Name: copy__M64_N64_K128_cpu_dtype_onetorch.int32_dtype_twotorch.int32
# Input: M: 64, N: 64, K: 128, device: cpu, dtype_one: torch.int32, dtype_two: torch.int32
Run: 0, Forward Execution Time (us) : 164.617
Run: 1, Forward Execution Time (us) : 168.222
Run: 2, Forward Execution Time (us) : 168.241

# Benchmarking PyTorch: copy_
# Mode: Eager
# Name: copy__M64_N64_K128_cuda_dtype_onetorch.int32_dtype_twotorch.int32
# Input: M: 64, N: 64, K: 128, device: cuda, dtype_one: torch.int32, dtype_two: torch.int32
Run: 0, Forward Execution Time (us) : 12.334
Run: 1, Forward Execution Time (us) : 12.404
Run: 2, Forward Execution Time (us) : 12.369

+ python -m pt.gelu_test --num-runs 3 --iterations 100 --warmup-iterations 10
not use dipu
# ----------------------------------------
# PyTorch/Caffe2 Operator Micro-benchmarks
# ----------------------------------------
# Tag : short

+ python -m pt.matmul_test --num-runs 3 --iterations 100 --warmup-iterations 10
not use dipu
# ----------------------------------------
# PyTorch/Caffe2 Operator Micro-benchmarks
# ----------------------------------------
# Tag : short

# Benchmarking PyTorch: matmul
# Mode: Eager
# Name: matmul_M1_N1_K1_trans_aTrue_trans_bFalse_cpu
# Input: M: 1, N: 1, K: 1, trans_a: True, trans_b: False, device: cpu
Run: 0, Forward Execution Time (us) : 7.355
Run: 1, Forward Execution Time (us) : 8.926
Run: 2, Forward Execution Time (us) : 8.796

# Benchmarking PyTorch: matmul
# Mode: Eager
# Name: matmul_M1_N1_K1_trans_aTrue_trans_bFalse_cuda
# Input: M: 1, N: 1, K: 1, trans_a: True, trans_b: False, device: cuda
Run: 0, Forward Execution Time (us) : 16.004
Run: 1, Forward Execution Time (us) : 18.715
Run: 2, Forward Execution Time (us) : 18.588

# Benchmarking PyTorch: matmul
# Mode: Eager
# Name: matmul_M128_N128_K128_trans_aTrue_trans_bFalse_cpu
# Input: M: 128, N: 128, K: 128, trans_a: True, trans_b: False, device: cpu
Run: 0, Forward Execution Time (us) : 85.484
Run: 1, Forward Execution Time (us) : 86.642
Run: 2, Forward Execution Time (us) : 85.797

# Benchmarking PyTorch: matmul
# Mode: Eager
# Name: matmul_M128_N128_K128_trans_aTrue_trans_bFalse_cuda
# Input: M: 128, N: 128, K: 128, trans_a: True, trans_b: False, device: cuda
Run: 0, Forward Execution Time (us) : 31.874
Run: 1, Forward Execution Time (us) : 32.012
Run: 2, Forward Execution Time (us) : 32.068

# Benchmarking PyTorch: matmul
# Mode: Eager
# Name: matmul_M256_N256_K256_trans_aFalse_trans_bTrue_cpu
# Input: M: 256, N: 256, K: 256, trans_a: False, trans_b: True, device: cpu
Run: 0, Forward Execution Time (us) : 526.027
Run: 1, Forward Execution Time (us) : 523.612
Run: 2, Forward Execution Time (us) : 522.887

# Benchmarking PyTorch: matmul
# Mode: Eager
# Name: matmul_M256_N256_K256_trans_aFalse_trans_bTrue_cuda
# Input: M: 256, N: 256, K: 256, trans_a: False, trans_b: True, device: cuda
Run: 0, Forward Execution Time (us) : 31.847
Run: 1, Forward Execution Time (us) : 32.000
Run: 2, Forward Execution Time (us) : 32.103

+ python -m pt.qembeddingbag_test --num-runs 3 --iterations 100 --warmup-iterations 10
not use dipu
# ----------------------------------------
# PyTorch/Caffe2 Operator Micro-benchmarks
# ----------------------------------------
# Tag : short

# Benchmarking PyTorch: qEmbeddingBag
# Mode: Eager
# Name: qEmbeddingBag_embeddingbags10_dim64_modesum_input_size8_offset0_sparseTrue_include_last_offsetTrue_cpu
# Input: embeddingbags: 10, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: True, include_last_offset: True, device: cpu
Run: 0, Forward Execution Time (us) : 59.083
Run: 1, Forward Execution Time (us) : 70.280
Run: 2, Forward Execution Time (us) : 70.109

# Benchmarking PyTorch: qEmbeddingBag
# Mode: Eager
# Name: qEmbeddingBag_embeddingbags10_dim64_modesum_input_size8_offset0_sparseTrue_include_last_offsetFalse_cpu
# Input: embeddingbags: 10, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: True, include_last_offset: False, device: cpu
Run: 0, Forward Execution Time (us) : 70.065
Run: 1, Forward Execution Time (us) : 70.030
Run: 2, Forward Execution Time (us) : 69.955

# Benchmarking PyTorch: qEmbeddingBag
# Mode: Eager
# Name: qEmbeddingBag_embeddingbags10_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetTrue_cpu
# Input: embeddingbags: 10, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: True, device: cpu
Run: 0, Forward Execution Time (us) : 69.737
Run: 1, Forward Execution Time (us) : 69.857
Run: 2, Forward Execution Time (us) : 69.836

# Benchmarking PyTorch: qEmbeddingBag
# Mode: Eager
# Name: qEmbeddingBag_embeddingbags10_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetFalse_cpu
# Input: embeddingbags: 10, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: False, device: cpu
Run: 0, Forward Execution Time (us) : 70.022
Run: 1, Forward Execution Time (us) : 69.983
Run: 2, Forward Execution Time (us) : 69.963

# Benchmarking PyTorch: qEmbeddingBag
# Mode: Eager
# Name: qEmbeddingBag_embeddingbags10_dim64_modesum_input_size16_offset0_sparseTrue_include_last_offsetTrue_cpu
# Input: embeddingbags: 10, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: True, include_last_offset: True, device: cpu
Run: 0, Forward Execution Time (us) : 76.753
Run: 1, Forward Execution Time (us) : 76.922
Run: 2, Forward Execution Time (us) : 76.701

# Benchmarking PyTorch: qEmbeddingBag
# Mode: Eager
# Name: qEmbeddingBag_embeddingbags10_dim64_modesum_input_size16_offset0_sparseTrue_include_last_offsetFalse_cpu
# Input: embeddingbags: 10, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: True, include_last_offset: False, device: cpu
Run: 0, Forward Execution Time (us) : 76.814
Run: 1, Forward Execution Time (us) : 77.163
Run: 2, Forward Execution Time (us) : 77.010

# Benchmarking PyTorch: qEmbeddingBag
# Mode: Eager
# Name: qEmbeddingBag_embeddingbags10_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetTrue_cpu
# Input: embeddingbags: 10, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: True, device: cpu
Run: 0, Forward Execution Time (us) : 76.518
Run: 1, Forward Execution Time (us) : 76.991
Run: 2, Forward Execution Time (us) : 76.744

# Benchmarking PyTorch: qEmbeddingBag
# Mode: Eager
# Name: qEmbeddingBag_embeddingbags10_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetFalse_cpu
# Input: embeddingbags: 10, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: False, device: cpu
Run: 0, Forward Execution Time (us) : 76.780
Run: 1, Forward Execution Time (us) : 77.016
Run: 2, Forward Execution Time (us) : 77.021

# Benchmarking PyTorch: qEmbeddingBag
# Mode: Eager
# Name: qEmbeddingBag_embeddingbags10_dim64_modesum_input_size64_offset0_sparseTrue_include_last_offsetTrue_cpu
# Input: embeddingbags: 10, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: True, include_last_offset: True, device: cpu
Run: 0, Forward Execution Time (us) : 119.036
Run: 1, Forward Execution Time (us) : 119.322
Run: 2, Forward Execution Time (us) : 119.176

# Benchmarking PyTorch: qEmbeddingBag
# Mode: Eager
# Name: qEmbeddingBag_embeddingbags10_dim64_modesum_input_size64_offset0_sparseTrue_include_last_offsetFalse_cpu
# Input: embeddingbags: 10, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: True, include_last_offset: False, device: cpu
Run: 0, Forward Execution Time (us) : 119.294
Run: 1, Forward Execution Time (us) : 119.413
Run: 2, Forward Execution Time (us) : 119.476

# Benchmarking PyTorch: qEmbeddingBag
# Mode: Eager
# Name: qEmbeddingBag_embeddingbags10_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetTrue_cpu
# Input: embeddingbags: 10, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: True, device: cpu
Run: 0, Forward Execution Time (us) : 119.141
Run: 1, Forward Execution Time (us) : 119.260
Run: 2, Forward Execution Time (us) : 119.192

# Benchmarking PyTorch: qEmbeddingBag
# Mode: Eager
# Name: qEmbeddingBag_embeddingbags10_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetFalse_cpu
# Input: embeddingbags: 10, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: False, device: cpu
Run: 0, Forward Execution Time (us) : 119.244
Run: 1, Forward Execution Time (us) : 119.477
Run: 2, Forward Execution Time (us) : 119.441

# Benchmarking PyTorch: qEmbeddingBag
# Mode: Eager
# Name: qEmbeddingBag_embeddingbags120_dim64_modesum_input_size8_offset0_sparseTrue_include_last_offsetTrue_cpu
# Input: embeddingbags: 120, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: True, include_last_offset: True, device: cpu
Run: 0, Forward Execution Time (us) : 69.589
Run: 1, Forward Execution Time (us) : 69.798
Run: 2, Forward Execution Time (us) : 69.719

# Benchmarking PyTorch: qEmbeddingBag
# Mode: Eager
# Name: qEmbeddingBag_embeddingbags120_dim64_modesum_input_size8_offset0_sparseTrue_include_last_offsetFalse_cpu
# Input: embeddingbags: 120, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: True, include_last_offset: False, device: cpu
Run: 0, Forward Execution Time (us) : 69.992
Run: 1, Forward Execution Time (us) : 70.188
Run: 2, Forward Execution Time (us) : 70.161

# Benchmarking PyTorch: qEmbeddingBag
# Mode: Eager
# Name: qEmbeddingBag_embeddingbags120_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetTrue_cpu
# Input: embeddingbags: 120, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: True, device: cpu
Run: 0, Forward Execution Time (us) : 69.688
Run: 1, Forward Execution Time (us) : 69.818
Run: 2, Forward Execution Time (us) : 69.658

# Benchmarking PyTorch: qEmbeddingBag
# Mode: Eager
# Name: qEmbeddingBag_embeddingbags120_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetFalse_cpu
# Input: embeddingbags: 120, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: False, device: cpu
Run: 0, Forward Execution Time (us) : 69.746
Run: 1, Forward Execution Time (us) : 70.061
Run: 2, Forward Execution Time (us) : 70.180

# Benchmarking PyTorch: qEmbeddingBag
# Mode: Eager
# Name: qEmbeddingBag_embeddingbags120_dim64_modesum_input_size16_offset0_sparseTrue_include_last_offsetTrue_cpu
# Input: embeddingbags: 120, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: True, include_last_offset: True, device: cpu
Run: 0, Forward Execution Time (us) : 76.753
Run: 1, Forward Execution Time (us) : 76.876
Run: 2, Forward Execution Time (us) : 76.759

# Benchmarking PyTorch: qEmbeddingBag
# Mode: Eager
# Name: qEmbeddingBag_embeddingbags120_dim64_modesum_input_size16_offset0_sparseTrue_include_last_offsetFalse_cpu
# Input: embeddingbags: 120, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: True, include_last_offset: False, device: cpu
Run: 0, Forward Execution Time (us) : 76.985
Run: 1, Forward Execution Time (us) : 77.153
Run: 2, Forward Execution Time (us) : 77.289

# Benchmarking PyTorch: qEmbeddingBag
# Mode: Eager
# Name: qEmbeddingBag_embeddingbags120_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetTrue_cpu
# Input: embeddingbags: 120, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: True, device: cpu
Run: 0, Forward Execution Time (us) : 76.811
Run: 1, Forward Execution Time (us) : 76.932
Run: 2, Forward Execution Time (us) : 76.785

# Benchmarking PyTorch: qEmbeddingBag
# Mode: Eager
# Name: qEmbeddingBag_embeddingbags120_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetFalse_cpu
# Input: embeddingbags: 120, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: False, device: cpu
Run: 0, Forward Execution Time (us) : 77.171
Run: 1, Forward Execution Time (us) : 77.134
Run: 2, Forward Execution Time (us) : 77.244

# Benchmarking PyTorch: qEmbeddingBag
# Mode: Eager
# Name: qEmbeddingBag_embeddingbags120_dim64_modesum_input_size64_offset0_sparseTrue_include_last_offsetTrue_cpu
# Input: embeddingbags: 120, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: True, include_last_offset: True, device: cpu
Run: 0, Forward Execution Time (us) : 119.132
Run: 1, Forward Execution Time (us) : 119.370
Run: 2, Forward Execution Time (us) : 119.275

# Benchmarking PyTorch: qEmbeddingBag
# Mode: Eager
# Name: qEmbeddingBag_embeddingbags120_dim64_modesum_input_size64_offset0_sparseTrue_include_last_offsetFalse_cpu
# Input: embeddingbags: 120, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: True, include_last_offset: False, device: cpu
Run: 0, Forward Execution Time (us) : 119.395
Run: 1, Forward Execution Time (us) : 119.586
Run: 2, Forward Execution Time (us) : 119.478

# Benchmarking PyTorch: qEmbeddingBag
# Mode: Eager
# Name: qEmbeddingBag_embeddingbags120_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetTrue_cpu
# Input: embeddingbags: 120, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: True, device: cpu
Run: 0, Forward Execution Time (us) : 119.071
Run: 1, Forward Execution Time (us) : 119.392
Run: 2, Forward Execution Time (us) : 119.177

# Benchmarking PyTorch: qEmbeddingBag
# Mode: Eager
# Name: qEmbeddingBag_embeddingbags120_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetFalse_cpu
# Input: embeddingbags: 120, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: False, device: cpu
Run: 0, Forward Execution Time (us) : 119.301
Run: 1, Forward Execution Time (us) : 119.501
Run: 2, Forward Execution Time (us) : 119.537

# Benchmarking PyTorch: qEmbeddingBag
# Mode: Eager
# Name: qEmbeddingBag_embeddingbags1000_dim64_modesum_input_size8_offset0_sparseTrue_include_last_offsetTrue_cpu
# Input: embeddingbags: 1000, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: True, include_last_offset: True, device: cpu
Run: 0, Forward Execution Time (us) : 69.762
Run: 1, Forward Execution Time (us) : 69.874
Run: 2, Forward Execution Time (us) : 69.842

# Benchmarking PyTorch: qEmbeddingBag
# Mode: Eager
# Name: qEmbeddingBag_embeddingbags1000_dim64_modesum_input_size8_offset0_sparseTrue_include_last_offsetFalse_cpu
# Input: embeddingbags: 1000, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: True, include_last_offset: False, device: cpu
Run: 0, Forward Execution Time (us) : 69.963
Run: 1, Forward Execution Time (us) : 70.149
Run: 2, Forward Execution Time (us) : 70.089

# Benchmarking PyTorch: qEmbeddingBag
# Mode: Eager
# Name: qEmbeddingBag_embeddingbags1000_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetTrue_cpu
# Input: embeddingbags: 1000, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: True, device: cpu
Run: 0, Forward Execution Time (us) : 69.676
Run: 1, Forward Execution Time (us) : 69.908
Run: 2, Forward Execution Time (us) : 69.994

# Benchmarking PyTorch: qEmbeddingBag
# Mode: Eager
# Name: qEmbeddingBag_embeddingbags1000_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetFalse_cpu
# Input: embeddingbags: 1000, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: False, device: cpu
Run: 0, Forward Execution Time (us) : 70.196
Run: 1, Forward Execution Time (us) : 70.117
Run: 2, Forward Execution Time (us) : 69.923

# Benchmarking PyTorch: qEmbeddingBag
# Mode: Eager
# Name: qEmbeddingBag_embeddingbags1000_dim64_modesum_input_size16_offset0_sparseTrue_include_last_offsetTrue_cpu
# Input: embeddingbags: 1000, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: True, include_last_offset: True, device: cpu
Run: 0, Forward Execution Time (us) : 76.767
Run: 1, Forward Execution Time (us) : 76.897
Run: 2, Forward Execution Time (us) : 76.789

# Benchmarking PyTorch: qEmbeddingBag
# Mode: Eager
# Name: qEmbeddingBag_embeddingbags1000_dim64_modesum_input_size16_offset0_sparseTrue_include_last_offsetFalse_cpu
# Input: embeddingbags: 1000, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: True, include_last_offset: False, device: cpu
Run: 0, Forward Execution Time (us) : 77.158
Run: 1, Forward Execution Time (us) : 77.471
Run: 2, Forward Execution Time (us) : 77.280

# Benchmarking PyTorch: qEmbeddingBag
# Mode: Eager
# Name: qEmbeddingBag_embeddingbags1000_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetTrue_cpu
# Input: embeddingbags: 1000, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: True, device: cpu
Run: 0, Forward Execution Time (us) : 76.793
Run: 1, Forward Execution Time (us) : 76.948
Run: 2, Forward Execution Time (us) : 76.830

# Benchmarking PyTorch: qEmbeddingBag
# Mode: Eager
# Name: qEmbeddingBag_embeddingbags1000_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetFalse_cpu
# Input: embeddingbags: 1000, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: False, device: cpu
Run: 0, Forward Execution Time (us) : 76.917
Run: 1, Forward Execution Time (us) : 77.144
Run: 2, Forward Execution Time (us) : 77.190

# Benchmarking PyTorch: qEmbeddingBag
# Mode: Eager
# Name: qEmbeddingBag_embeddingbags1000_dim64_modesum_input_size64_offset0_sparseTrue_include_last_offsetTrue_cpu
# Input: embeddingbags: 1000, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: True, include_last_offset: True, device: cpu
Run: 0, Forward Execution Time (us) : 119.244
Run: 1, Forward Execution Time (us) : 119.351
Run: 2, Forward Execution Time (us) : 119.423

# Benchmarking PyTorch: qEmbeddingBag
# Mode: Eager
# Name: qEmbeddingBag_embeddingbags1000_dim64_modesum_input_size64_offset0_sparseTrue_include_last_offsetFalse_cpu
# Input: embeddingbags: 1000, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: True, include_last_offset: False, device: cpu
Run: 0, Forward Execution Time (us) : 119.298
Run: 1, Forward Execution Time (us) : 119.461
Run: 2, Forward Execution Time (us) : 119.419

# Benchmarking PyTorch: qEmbeddingBag
# Mode: Eager
# Name: qEmbeddingBag_embeddingbags1000_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetTrue_cpu
# Input: embeddingbags: 1000, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: True, device: cpu
Run: 0, Forward Execution Time (us) : 119.164
Run: 1, Forward Execution Time (us) : 119.298
Run: 2, Forward Execution Time (us) : 119.117

# Benchmarking PyTorch: qEmbeddingBag
# Mode: Eager
# Name: qEmbeddingBag_embeddingbags1000_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetFalse_cpu
# Input: embeddingbags: 1000, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: False, device: cpu
Run: 0, Forward Execution Time (us) : 119.356
Run: 1, Forward Execution Time (us) : 119.589
Run: 2, Forward Execution Time (us) : 119.506

# Benchmarking PyTorch: qEmbeddingBag
# Mode: Eager
# Name: qEmbeddingBag_embeddingbags2300_dim64_modesum_input_size8_offset0_sparseTrue_include_last_offsetTrue_cpu
# Input: embeddingbags: 2300, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: True, include_last_offset: True, device: cpu
Run: 0, Forward Execution Time (us) : 69.612
Run: 1, Forward Execution Time (us) : 69.708
Run: 2, Forward Execution Time (us) : 69.686

# Benchmarking PyTorch: qEmbeddingBag
# Mode: Eager
# Name: qEmbeddingBag_embeddingbags2300_dim64_modesum_input_size8_offset0_sparseTrue_include_last_offsetFalse_cpu
# Input: embeddingbags: 2300, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: True, include_last_offset: False, device: cpu
Run: 0, Forward Execution Time (us) : 70.010
Run: 1, Forward Execution Time (us) : 70.177
Run: 2, Forward Execution Time (us) : 70.079

# Benchmarking PyTorch: qEmbeddingBag
# Mode: Eager
# Name: qEmbeddingBag_embeddingbags2300_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetTrue_cpu
# Input: embeddingbags: 2300, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: True, device: cpu
Run: 0, Forward Execution Time (us) : 69.837
Run: 1, Forward Execution Time (us) : 69.903
Run: 2, Forward Execution Time (us) : 69.938

# Benchmarking PyTorch: qEmbeddingBag
# Mode: Eager
# Name: qEmbeddingBag_embeddingbags2300_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetFalse_cpu
# Input: embeddingbags: 2300, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: False, device: cpu
Run: 0, Forward Execution Time (us) : 69.900
Run: 1, Forward Execution Time (us) : 70.278
Run: 2, Forward Execution Time (us) : 70.022

# Benchmarking PyTorch: qEmbeddingBag
# Mode: Eager
# Name: qEmbeddingBag_embeddingbags2300_dim64_modesum_input_size16_offset0_sparseTrue_include_last_offsetTrue_cpu
# Input: embeddingbags: 2300, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: True, include_last_offset: True, device: cpu
Run: 0, Forward Execution Time (us) : 76.815
Run: 1, Forward Execution Time (us) : 76.913
Run: 2, Forward Execution Time (us) : 76.795

# Benchmarking PyTorch: qEmbeddingBag
# Mode: Eager
# Name: qEmbeddingBag_embeddingbags2300_dim64_modesum_input_size16_offset0_sparseTrue_include_last_offsetFalse_cpu
# Input: embeddingbags: 2300, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: True, include_last_offset: False, device: cpu
Run: 0, Forward Execution Time (us) : 77.049
Run: 1, Forward Execution Time (us) : 77.218
Run: 2, Forward Execution Time (us) : 77.011

# Benchmarking PyTorch: qEmbeddingBag
# Mode: Eager
# Name: qEmbeddingBag_embeddingbags2300_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetTrue_cpu
# Input: embeddingbags: 2300, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: True, device: cpu
Run: 0, Forward Execution Time (us) : 76.748
Run: 1, Forward Execution Time (us) : 76.793
Run: 2, Forward Execution Time (us) : 76.905

# Benchmarking PyTorch: qEmbeddingBag
# Mode: Eager
# Name: qEmbeddingBag_embeddingbags2300_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetFalse_cpu
# Input: embeddingbags: 2300, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: False, device: cpu
Run: 0, Forward Execution Time (us) : 77.167
Run: 1, Forward Execution Time (us) : 77.209
Run: 2, Forward Execution Time (us) : 77.187

# Benchmarking PyTorch: qEmbeddingBag
# Mode: Eager
# Name: qEmbeddingBag_embeddingbags2300_dim64_modesum_input_size64_offset0_sparseTrue_include_last_offsetTrue_cpu
# Input: embeddingbags: 2300, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: True, include_last_offset: True, device: cpu
Run: 0, Forward Execution Time (us) : 118.976
Run: 1, Forward Execution Time (us) : 119.203
Run: 2, Forward Execution Time (us) : 119.282

# Benchmarking PyTorch: qEmbeddingBag
# Mode: Eager
# Name: qEmbeddingBag_embeddingbags2300_dim64_modesum_input_size64_offset0_sparseTrue_include_last_offsetFalse_cpu
# Input: embeddingbags: 2300, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: True, include_last_offset: False, device: cpu
Run: 0, Forward Execution Time (us) : 119.501
Run: 1, Forward Execution Time (us) : 119.638
Run: 2, Forward Execution Time (us) : 119.400

# Benchmarking PyTorch: qEmbeddingBag
# Mode: Eager
# Name: qEmbeddingBag_embeddingbags2300_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetTrue_cpu
# Input: embeddingbags: 2300, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: True, device: cpu
Run: 0, Forward Execution Time (us) : 119.011
Run: 1, Forward Execution Time (us) : 119.179
Run: 2, Forward Execution Time (us) : 119.136

# Benchmarking PyTorch: qEmbeddingBag
# Mode: Eager
# Name: qEmbeddingBag_embeddingbags2300_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetFalse_cpu
# Input: embeddingbags: 2300, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: False, device: cpu
Run: 0, Forward Execution Time (us) : 119.336
Run: 1, Forward Execution Time (us) : 119.529
Run: 2, Forward Execution Time (us) : 119.450

+ python -m pt.qunary_test --num-runs 3 --iterations 100 --warmup-iterations 10
not use dipu
# ----------------------------------------
# PyTorch/Caffe2 Operator Micro-benchmarks
# ----------------------------------------
# Tag : short

# Benchmarking PyTorch: q_argsort
# Mode: Eager
# Name: q_argsort_M512_N512_dtypetorch.quint8
# Input: M: 512, N: 512, dtype: torch.quint8
Run: 0, Forward Execution Time (us) : 4715.675
Run: 1, Forward Execution Time (us) : 4723.011
Run: 2, Forward Execution Time (us) : 4752.737

# Benchmarking PyTorch: q_clone
# Mode: Eager
# Name: q_clone_M512_N512_dtypetorch.quint8
# Input: M: 512, N: 512, dtype: torch.quint8
Run: 0, Forward Execution Time (us) : 16.983
Run: 1, Forward Execution Time (us) : 19.485
Run: 2, Forward Execution Time (us) : 19.398

# Benchmarking PyTorch: q_mean
# Mode: Eager
# Name: q_mean_M512_N512_dtypetorch.quint8
# Input: M: 512, N: 512, dtype: torch.quint8
Run: 0, Forward Execution Time (us) : 17.642
Run: 1, Forward Execution Time (us) : 17.160
Run: 2, Forward Execution Time (us) : 17.104

# Benchmarking PyTorch: q_relu
# Mode: Eager
# Name: q_relu_M512_N512_dtypetorch.quint8
# Input: M: 512, N: 512, dtype: torch.quint8
Run: 0, Forward Execution Time (us) : 16.404
Run: 1, Forward Execution Time (us) : 18.788
Run: 2, Forward Execution Time (us) : 18.745

# Benchmarking PyTorch: q_relu_
# Mode: Eager
# Name: q_relu__M512_N512_dtypetorch.quint8
# Input: M: 512, N: 512, dtype: torch.quint8
Run: 0, Forward Execution Time (us) : 214.139
Run: 1, Forward Execution Time (us) : 253.998
Run: 2, Forward Execution Time (us) : 217.358

# Benchmarking PyTorch: q_sort
# Mode: Eager
# Name: q_sort_M512_N512_dtypetorch.quint8
# Input: M: 512, N: 512, dtype: torch.quint8
Run: 0, Forward Execution Time (us) : 4229.363
Run: 1, Forward Execution Time (us) : 4229.414
Run: 2, Forward Execution Time (us) : 4285.262

# Benchmarking PyTorch: qtopk
# Mode: Eager
# Name: qtopk_M512_N512_k5_dtypetorch.quint8
# Input: M: 512, N: 512, k: 5, dtype: torch.quint8
Run: 0, Forward Execution Time (us) : 419.004
Run: 1, Forward Execution Time (us) : 493.577
Run: 2, Forward Execution Time (us) : 492.712

+ python -m pt.bmm_test --num-runs 3 --iterations 100 --warmup-iterations 10
not use dipu
# ----------------------------------------
# PyTorch/Caffe2 Operator Micro-benchmarks
# ----------------------------------------
# Tag : short

# Benchmarking PyTorch: bmm (actual op=bmm
# Mode: Eager
# Name: bmm(actualop=bmm_B2_M8_N256_K16_cpu_opbmm
# Input: B: 2, M: 8, N: 256, K: 16, device: cpu, op: bmm
Run: 0, Forward Execution Time (us) : 30.781
Run: 1, Forward Execution Time (us) : 30.547
Run: 2, Forward Execution Time (us) : 30.401

# Benchmarking PyTorch: bmm (actual op=matmul
# Mode: Eager
# Name: bmm(actualop=matmul_B2_M8_N256_K16_cpu_opmatmul
# Input: B: 2, M: 8, N: 256, K: 16, device: cpu, op: matmul
Run: 0, Forward Execution Time (us) : 47.679
Run: 1, Forward Execution Time (us) : 47.792
Run: 2, Forward Execution Time (us) : 47.753

# Benchmarking PyTorch: bmm (actual op=bmm
# Mode: Eager
# Name: bmm(actualop=bmm_B2_M8_N256_K32_cpu_opbmm
# Input: B: 2, M: 8, N: 256, K: 32, device: cpu, op: bmm
Run: 0, Forward Execution Time (us) : 37.372
Run: 1, Forward Execution Time (us) : 37.439
Run: 2, Forward Execution Time (us) : 37.367

# Benchmarking PyTorch: bmm (actual op=matmul
# Mode: Eager
# Name: bmm(actualop=matmul_B2_M8_N256_K32_cpu_opmatmul
# Input: B: 2, M: 8, N: 256, K: 32, device: cpu, op: matmul
Run: 0, Forward Execution Time (us) : 55.991
Run: 1, Forward Execution Time (us) : 54.737
Run: 2, Forward Execution Time (us) : 54.791

# Benchmarking PyTorch: bmm (actual op=bmm
# Mode: Eager
# Name: bmm(actualop=bmm_B2_M8_N16_K16_cpu_opbmm
# Input: B: 2, M: 8, N: 16, K: 16, device: cpu, op: bmm
Run: 0, Forward Execution Time (us) : 21.684
Run: 1, Forward Execution Time (us) : 21.704
Run: 2, Forward Execution Time (us) : 21.751

# Benchmarking PyTorch: bmm (actual op=matmul
# Mode: Eager
# Name: bmm(actualop=matmul_B2_M8_N16_K16_cpu_opmatmul
# Input: B: 2, M: 8, N: 16, K: 16, device: cpu, op: matmul
Run: 0, Forward Execution Time (us) : 39.054
Run: 1, Forward Execution Time (us) : 38.918
Run: 2, Forward Execution Time (us) : 38.926

# Benchmarking PyTorch: bmm (actual op=bmm
# Mode: Eager
# Name: bmm(actualop=bmm_B2_M8_N16_K32_cpu_opbmm
# Input: B: 2, M: 8, N: 16, K: 32, device: cpu, op: bmm
Run: 0, Forward Execution Time (us) : 22.419
Run: 1, Forward Execution Time (us) : 22.354
Run: 2, Forward Execution Time (us) : 22.307

# Benchmarking PyTorch: bmm (actual op=matmul
# Mode: Eager
# Name: bmm(actualop=matmul_B2_M8_N16_K32_cpu_opmatmul
# Input: B: 2, M: 8, N: 16, K: 32, device: cpu, op: matmul
Run: 0, Forward Execution Time (us) : 39.581
Run: 1, Forward Execution Time (us) : 39.576
Run: 2, Forward Execution Time (us) : 39.600

# Benchmarking PyTorch: bmm (actual op=bmm
# Mode: Eager
# Name: bmm(actualop=bmm_B2_M256_N256_K16_cpu_opbmm
# Input: B: 2, M: 256, N: 256, K: 16, device: cpu, op: bmm
Run: 0, Forward Execution Time (us) : 146.483
Run: 1, Forward Execution Time (us) : 146.739
Run: 2, Forward Execution Time (us) : 146.593

# Benchmarking PyTorch: bmm (actual op=matmul
# Mode: Eager
# Name: bmm(actualop=matmul_B2_M256_N256_K16_cpu_opmatmul
# Input: B: 2, M: 256, N: 256, K: 16, device: cpu, op: matmul
Run: 0, Forward Execution Time (us) : 164.765
Run: 1, Forward Execution Time (us) : 164.657
Run: 2, Forward Execution Time (us) : 164.909

# Benchmarking PyTorch: bmm (actual op=bmm
# Mode: Eager
# Name: bmm(actualop=bmm_B2_M256_N256_K32_cpu_opbmm
# Input: B: 2, M: 256, N: 256, K: 32, device: cpu, op: bmm
Run: 0, Forward Execution Time (us) : 209.435
Run: 1, Forward Execution Time (us) : 209.951
Run: 2, Forward Execution Time (us) : 210.030

# Benchmarking PyTorch: bmm (actual op=matmul
# Mode: Eager
# Name: bmm(actualop=matmul_B2_M256_N256_K32_cpu_opmatmul
# Input: B: 2, M: 256, N: 256, K: 32, device: cpu, op: matmul
Run: 0, Forward Execution Time (us) : 228.185
Run: 1, Forward Execution Time (us) : 228.634
Run: 2, Forward Execution Time (us) : 228.267

# Benchmarking PyTorch: bmm (actual op=bmm
# Mode: Eager
# Name: bmm(actualop=bmm_B2_M256_N16_K16_cpu_opbmm
# Input: B: 2, M: 256, N: 16, K: 16, device: cpu, op: bmm
Run: 0, Forward Execution Time (us) : 35.227
Run: 1, Forward Execution Time (us) : 35.445
Run: 2, Forward Execution Time (us) : 35.255

# Benchmarking PyTorch: bmm (actual op=matmul
# Mode: Eager
# Name: bmm(actualop=matmul_B2_M256_N16_K16_cpu_opmatmul
# Input: B: 2, M: 256, N: 16, K: 16, device: cpu, op: matmul
Run: 0, Forward Execution Time (us) : 52.840
Run: 1, Forward Execution Time (us) : 52.561
Run: 2, Forward Execution Time (us) : 52.504

# Benchmarking PyTorch: bmm (actual op=bmm
# Mode: Eager
# Name: bmm(actualop=bmm_B2_M256_N16_K32_cpu_opbmm
# Input: B: 2, M: 256, N: 16, K: 32, device: cpu, op: bmm
Run: 0, Forward Execution Time (us) : 43.346
Run: 1, Forward Execution Time (us) : 43.071
Run: 2, Forward Execution Time (us) : 43.003

# Benchmarking PyTorch: bmm (actual op=matmul
# Mode: Eager
# Name: bmm(actualop=matmul_B2_M256_N16_K32_cpu_opmatmul
# Input: B: 2, M: 256, N: 16, K: 32, device: cpu, op: matmul
Run: 0, Forward Execution Time (us) : 60.401
Run: 1, Forward Execution Time (us) : 60.466
Run: 2, Forward Execution Time (us) : 60.589

# Benchmarking PyTorch: bmm (actual op=bmm
# Mode: Eager
# Name: bmm(actualop=bmm_B100_M8_N256_K16_cpu_opbmm
# Input: B: 100, M: 8, N: 256, K: 16, device: cpu, op: bmm
Run: 0, Forward Execution Time (us) : 1160.169
Run: 1, Forward Execution Time (us) : 1160.858
Run: 2, Forward Execution Time (us) : 1161.800

# Benchmarking PyTorch: bmm (actual op=matmul
# Mode: Eager
# Name: bmm(actualop=matmul_B100_M8_N256_K16_cpu_opmatmul
# Input: B: 100, M: 8, N: 256, K: 16, device: cpu, op: matmul
Run: 0, Forward Execution Time (us) : 1191.854
Run: 1, Forward Execution Time (us) : 1193.005
Run: 2, Forward Execution Time (us) : 1190.707

# Benchmarking PyTorch: bmm (actual op=bmm
# Mode: Eager
# Name: bmm(actualop=bmm_B100_M8_N256_K32_cpu_opbmm
# Input: B: 100, M: 8, N: 256, K: 32, device: cpu, op: bmm
Run: 0, Forward Execution Time (us) : 1568.516
Run: 1, Forward Execution Time (us) : 1624.333
Run: 2, Forward Execution Time (us) : 1577.289

# Benchmarking PyTorch: bmm (actual op=matmul
# Mode: Eager
# Name: bmm(actualop=matmul_B100_M8_N256_K32_cpu_opmatmul
# Input: B: 100, M: 8, N: 256, K: 32, device: cpu, op: matmul
Run: 0, Forward Execution Time (us) : 1602.400
Run: 1, Forward Execution Time (us) : 1602.524
Run: 2, Forward Execution Time (us) : 1603.348

# Benchmarking PyTorch: bmm (actual op=bmm
# Mode: Eager
# Name: bmm(actualop=bmm_B100_M8_N16_K16_cpu_opbmm
# Input: B: 100, M: 8, N: 16, K: 16, device: cpu, op: bmm
Run: 0, Forward Execution Time (us) : 694.583
Run: 1, Forward Execution Time (us) : 695.468
Run: 2, Forward Execution Time (us) : 695.912

# Benchmarking PyTorch: bmm (actual op=matmul
# Mode: Eager
# Name: bmm(actualop=matmul_B100_M8_N16_K16_cpu_opmatmul
# Input: B: 100, M: 8, N: 16, K: 16, device: cpu, op: matmul
Run: 0, Forward Execution Time (us) : 713.792
Run: 1, Forward Execution Time (us) : 713.047
Run: 2, Forward Execution Time (us) : 712.630

# Benchmarking PyTorch: bmm (actual op=bmm
# Mode: Eager
# Name: bmm(actualop=bmm_B100_M8_N16_K32_cpu_opbmm
# Input: B: 100, M: 8, N: 16, K: 32, device: cpu, op: bmm
Run: 0, Forward Execution Time (us) : 722.336
Run: 1, Forward Execution Time (us) : 722.905
Run: 2, Forward Execution Time (us) : 721.053

# Benchmarking PyTorch: bmm (actual op=matmul
# Mode: Eager
# Name: bmm(actualop=matmul_B100_M8_N16_K32_cpu_opmatmul
# Input: B: 100, M: 8, N: 16, K: 32, device: cpu, op: matmul
Run: 0, Forward Execution Time (us) : 740.929
Run: 1, Forward Execution Time (us) : 739.772
Run: 2, Forward Execution Time (us) : 739.761

# Benchmarking PyTorch: bmm (actual op=bmm
# Mode: Eager
# Name: bmm(actualop=bmm_B100_M256_N256_K16_cpu_opbmm
# Input: B: 100, M: 256, N: 256, K: 16, device: cpu, op: bmm
Run: 0, Forward Execution Time (us) : 18735.606
Run: 1, Forward Execution Time (us) : 18670.771
Run: 2, Forward Execution Time (us) : 18611.644

# Benchmarking PyTorch: bmm (actual op=matmul
# Mode: Eager
# Name: bmm(actualop=matmul_B100_M256_N256_K16_cpu_opmatmul
# Input: B: 100, M: 256, N: 256, K: 16, device: cpu, op: matmul
Run: 0, Forward Execution Time (us) : 18633.369
Run: 1, Forward Execution Time (us) : 18777.444
Run: 2, Forward Execution Time (us) : 18550.776

# Benchmarking PyTorch: bmm (actual op=bmm
# Mode: Eager
# Name: bmm(actualop=bmm_B100_M256_N256_K32_cpu_opbmm
# Input: B: 100, M: 256, N: 256, K: 32, device: cpu, op: bmm
Run: 0, Forward Execution Time (us) : 10632.866
Run: 1, Forward Execution Time (us) : 10604.871
Run: 2, Forward Execution Time (us) : 10602.708

# Benchmarking PyTorch: bmm (actual op=matmul
# Mode: Eager
# Name: bmm(actualop=matmul_B100_M256_N256_K32_cpu_opmatmul
# Input: B: 100, M: 256, N: 256, K: 32, device: cpu, op: matmul
Run: 0, Forward Execution Time (us) : 22019.304
Run: 1, Forward Execution Time (us) : 22170.051
Run: 2, Forward Execution Time (us) : 21937.306

# Benchmarking PyTorch: bmm (actual op=bmm
# Mode: Eager
# Name: bmm(actualop=bmm_B100_M256_N16_K16_cpu_opbmm
# Input: B: 100, M: 256, N: 16, K: 16, device: cpu, op: bmm
Run: 0, Forward Execution Time (us) : 1396.942
Run: 1, Forward Execution Time (us) : 1394.379
Run: 2, Forward Execution Time (us) : 1394.493

# Benchmarking PyTorch: bmm (actual op=matmul
# Mode: Eager
# Name: bmm(actualop=matmul_B100_M256_N16_K16_cpu_opmatmul
# Input: B: 100, M: 256, N: 16, K: 16, device: cpu, op: matmul
Run: 0, Forward Execution Time (us) : 1419.192
Run: 1, Forward Execution Time (us) : 1418.743
Run: 2, Forward Execution Time (us) : 1419.879

# Benchmarking PyTorch: bmm (actual op=bmm
# Mode: Eager
# Name: bmm(actualop=bmm_B100_M256_N16_K32_cpu_opbmm
# Input: B: 100, M: 256, N: 16, K: 32, device: cpu, op: bmm
Run: 0, Forward Execution Time (us) : 1800.403
Run: 1, Forward Execution Time (us) : 1799.164
Run: 2, Forward Execution Time (us) : 1798.952

# Benchmarking PyTorch: bmm (actual op=matmul
# Mode: Eager
# Name: bmm(actualop=matmul_B100_M256_N16_K32_cpu_opmatmul
# Input: B: 100, M: 256, N: 16, K: 32, device: cpu, op: matmul
Run: 0, Forward Execution Time (us) : 1814.210
Run: 1, Forward Execution Time (us) : 1814.610
Run: 2, Forward Execution Time (us) : 1812.745

+ python -m pt.groupnorm_test --num-runs 3 --iterations 100 --warmup-iterations 10
not use dipu
# ----------------------------------------
# PyTorch/Caffe2 Operator Micro-benchmarks
# ----------------------------------------
# Tag : short

# Benchmarking PyTorch: GroupNormBenchmark
# Mode: Eager
# Name: GroupNormBenchmark_dims(32,8,16)_num_groups2
# Input: dims: (32, 8, 16), num_groups: 2
Run: 0, Forward Execution Time (us) : 28.046
Run: 1, Forward Execution Time (us) : 33.799
Run: 2, Forward Execution Time (us) : 33.245

# Benchmarking PyTorch: GroupNormBenchmark
# Mode: Eager
# Name: GroupNormBenchmark_dims(32,8,16)_num_groups4
# Input: dims: (32, 8, 16), num_groups: 4
Run: 0, Forward Execution Time (us) : 39.080
Run: 1, Forward Execution Time (us) : 39.201
Run: 2, Forward Execution Time (us) : 39.108

# Benchmarking PyTorch: GroupNormBenchmark
# Mode: Eager
# Name: GroupNormBenchmark_dims(32,8,56,56)_num_groups2
# Input: dims: (32, 8, 56, 56), num_groups: 2
Run: 0, Forward Execution Time (us) : 957.780
Run: 1, Forward Execution Time (us) : 956.678
Run: 2, Forward Execution Time (us) : 956.260

# Benchmarking PyTorch: GroupNormBenchmark
# Mode: Eager
# Name: GroupNormBenchmark_dims(32,8,56,56)_num_groups4
# Input: dims: (32, 8, 56, 56), num_groups: 4
Run: 0, Forward Execution Time (us) : 965.329
Run: 1, Forward Execution Time (us) : 966.811
Run: 2, Forward Execution Time (us) : 966.155

+ python -m pt.matrix_mult_test --num-runs 3 --iterations 100 --warmup-iterations 10
not use dipu
# ----------------------------------------
# PyTorch/Caffe2 Operator Micro-benchmarks
# ----------------------------------------
# Tag : short

# Benchmarking PyTorch: einsum_bmm
# Mode: Eager
# Name: einsum_bmm_B4_M5_N3_K2_cpu
# Input: B: 4, M: 5, N: 3, K: 2, device: cpu
Run: 0, Forward Execution Time (us) : 49.110
Run: 1, Forward Execution Time (us) : 51.114
Run: 2, Forward Execution Time (us) : 50.379

# Benchmarking PyTorch: einsum_bmm
# Mode: Eager
# Name: einsum_bmm_B4_M5_N3_K2_cuda
# Input: B: 4, M: 5, N: 3, K: 2, device: cuda
Run: 0, Forward Execution Time (us) : 62.054
Run: 1, Forward Execution Time (us) : 73.383
Run: 2, Forward Execution Time (us) : 72.714

# Benchmarking PyTorch: einsum_bmm
# Mode: Eager
# Name: einsum_bmm_B32_M25_N20_K30_cpu
# Input: B: 32, M: 25, N: 20, K: 30, device: cpu
Run: 0, Forward Execution Time (us) : 332.630
Run: 1, Forward Execution Time (us) : 331.728
Run: 2, Forward Execution Time (us) : 331.568

# Benchmarking PyTorch: einsum_bmm
# Mode: Eager
# Name: einsum_bmm_B32_M25_N20_K30_cuda
# Input: B: 32, M: 25, N: 20, K: 30, device: cuda
Run: 0, Forward Execution Time (us) : 75.416
Run: 1, Forward Execution Time (us) : 75.655
Run: 2, Forward Execution Time (us) : 75.647

# Benchmarking PyTorch: einsum_bmm
# Mode: Eager
# Name: einsum_bmm_B128_M100_N120_K110_cpu
# Input: B: 128, M: 100, N: 120, K: 110, device: cpu
Run: 0, Forward Execution Time (us) : 8711.811
Run: 1, Forward Execution Time (us) : 8636.519
Run: 2, Forward Execution Time (us) : 8678.786

# Benchmarking PyTorch: einsum_bmm
# Mode: Eager
# Name: einsum_bmm_B128_M100_N120_K110_cuda
# Input: B: 128, M: 100, N: 120, K: 110, device: cuda
Run: 0, Forward Execution Time (us) : 75.399
Run: 1, Forward Execution Time (us) : 75.418
Run: 2, Forward Execution Time (us) : 75.425

# Benchmarking PyTorch: bmm
# Mode: Eager
# Name: bmm_B4_M5_N3_K2_cpu
# Input: B: 4, M: 5, N: 3, K: 2, device: cpu
Run: 0, Forward Execution Time (us) : 7.470
Run: 1, Forward Execution Time (us) : 8.544
Run: 2, Forward Execution Time (us) : 8.230

# Benchmarking PyTorch: bmm
# Mode: Eager
# Name: bmm_B4_M5_N3_K2_cuda
# Input: B: 4, M: 5, N: 3, K: 2, device: cuda
Run: 0, Forward Execution Time (us) : 22.865
Run: 1, Forward Execution Time (us) : 19.620
Run: 2, Forward Execution Time (us) : 19.482

# Benchmarking PyTorch: bmm
# Mode: Eager
# Name: bmm_B32_M25_N20_K30_cpu
# Input: B: 32, M: 25, N: 20, K: 30, device: cpu
Run: 0, Forward Execution Time (us) : 281.283
Run: 1, Forward Execution Time (us) : 281.516
Run: 2, Forward Execution Time (us) : 281.870

# Benchmarking PyTorch: bmm
# Mode: Eager
# Name: bmm_B32_M25_N20_K30_cuda
# Input: B: 32, M: 25, N: 20, K: 30, device: cuda
Run: 0, Forward Execution Time (us) : 22.205
Run: 1, Forward Execution Time (us) : 22.432
Run: 2, Forward Execution Time (us) : 22.324

# Benchmarking PyTorch: bmm
# Mode: Eager
# Name: bmm_B128_M100_N120_K110_cpu
# Input: B: 128, M: 100, N: 120, K: 110, device: cpu
Run: 0, Forward Execution Time (us) : 8692.783
Run: 1, Forward Execution Time (us) : 8681.196
Run: 2, Forward Execution Time (us) : 8634.892

# Benchmarking PyTorch: bmm
# Mode: Eager
# Name: bmm_B128_M100_N120_K110_cuda
# Input: B: 128, M: 100, N: 120, K: 110, device: cuda
Run: 0, Forward Execution Time (us) : 62.361
Run: 1, Forward Execution Time (us) : 62.647
Run: 2, Forward Execution Time (us) : 62.453

# Benchmarking PyTorch: einsum_elementwise
# Mode: Eager
# Name: einsum_elementwise_B4_M5_N3_cpu
# Input: B: 4, M: 5, N: 3, device: cpu
Run: 0, Forward Execution Time (us) : 28.316
Run: 1, Forward Execution Time (us) : 29.719
Run: 2, Forward Execution Time (us) : 29.633

# Benchmarking PyTorch: einsum_elementwise
# Mode: Eager
# Name: einsum_elementwise_B4_M5_N3_cuda
# Input: B: 4, M: 5, N: 3, device: cuda
Run: 0, Forward Execution Time (us) : 39.875
Run: 1, Forward Execution Time (us) : 40.114
Run: 2, Forward Execution Time (us) : 40.442

# Benchmarking PyTorch: einsum_elementwise
# Mode: Eager
# Name: einsum_elementwise_B32_M25_N20_cpu
# Input: B: 32, M: 25, N: 20, device: cpu
Run: 0, Forward Execution Time (us) : 31.763
Run: 1, Forward Execution Time (us) : 31.919
Run: 2, Forward Execution Time (us) : 31.605

# Benchmarking PyTorch: einsum_elementwise
# Mode: Eager
# Name: einsum_elementwise_B32_M25_N20_cuda
# Input: B: 32, M: 25, N: 20, device: cuda
Run: 0, Forward Execution Time (us) : 40.054
Run: 1, Forward Execution Time (us) : 40.248
Run: 2, Forward Execution Time (us) : 33.496

# Benchmarking PyTorch: einsum_elementwise
# Mode: Eager
# Name: einsum_elementwise_B100_M90_N110_cpu
# Input: B: 100, M: 90, N: 110, device: cpu
Run: 0, Forward Execution Time (us) : 526.115
Run: 1, Forward Execution Time (us) : 524.637
Run: 2, Forward Execution Time (us) : 523.321

# Benchmarking PyTorch: einsum_elementwise
# Mode: Eager
# Name: einsum_elementwise_B100_M90_N110_cuda
# Input: B: 100, M: 90, N: 110, device: cuda
Run: 0, Forward Execution Time (us) : 35.163
Run: 1, Forward Execution Time (us) : 34.916
Run: 2, Forward Execution Time (us) : 35.489

# Benchmarking PyTorch: mul
# Mode: Eager
# Name: mul_B4_M5_N3_cpu
# Input: B: 4, M: 5, N: 3, device: cpu
Run: 0, Forward Execution Time (us) : 7.229
Run: 1, Forward Execution Time (us) : 8.577
Run: 2, Forward Execution Time (us) : 14.002

# Benchmarking PyTorch: mul
# Mode: Eager
# Name: mul_B4_M5_N3_cuda
# Input: B: 4, M: 5, N: 3, device: cuda
Run: 0, Forward Execution Time (us) : 16.607
Run: 1, Forward Execution Time (us) : 16.909
Run: 2, Forward Execution Time (us) : 16.785

# Benchmarking PyTorch: mul
# Mode: Eager
# Name: mul_B32_M25_N20_cpu
# Input: B: 32, M: 25, N: 20, device: cpu
Run: 0, Forward Execution Time (us) : 10.578
Run: 1, Forward Execution Time (us) : 10.643
Run: 2, Forward Execution Time (us) : 10.675

# Benchmarking PyTorch: mul
# Mode: Eager
# Name: mul_B32_M25_N20_cuda
# Input: B: 32, M: 25, N: 20, device: cuda
Run: 0, Forward Execution Time (us) : 16.512
Run: 1, Forward Execution Time (us) : 16.980
Run: 2, Forward Execution Time (us) : 16.568

# Benchmarking PyTorch: mul
# Mode: Eager
# Name: mul_B100_M90_N110_cpu
# Input: B: 100, M: 90, N: 110, device: cpu
Run: 0, Forward Execution Time (us) : 495.792
Run: 1, Forward Execution Time (us) : 495.753
Run: 2, Forward Execution Time (us) : 496.398

# Benchmarking PyTorch: mul
# Mode: Eager
# Name: mul_B100_M90_N110_cuda
# Input: B: 100, M: 90, N: 110, device: cuda
Run: 0, Forward Execution Time (us) : 17.603
Run: 1, Forward Execution Time (us) : 17.789
Run: 2, Forward Execution Time (us) : 17.718

+ python -m pt.qembedding_pack_test --num-runs 3 --iterations 100 --warmup-iterations 10
not use dipu
# ----------------------------------------
# PyTorch/Caffe2 Operator Micro-benchmarks
# ----------------------------------------
# Tag : short

# Benchmarking PyTorch: qembeddingbag_byte_prepack
# Mode: Eager
# Name: qembeddingbag_byte_prepack_num_embeddings80_embedding_dim128
# Input: num_embeddings: 80, embedding_dim: 128
Run: 0, Forward Execution Time (us) : 40.428
Run: 1, Forward Execution Time (us) : 42.067
Run: 2, Forward Execution Time (us) : 41.591

# Benchmarking PyTorch: qembeddingbag_byte_prepack
# Mode: Eager
# Name: qembeddingbag_byte_prepack_num_embeddings80_embedding_dim256
# Input: num_embeddings: 80, embedding_dim: 256
Run: 0, Forward Execution Time (us) : 75.177
Run: 1, Forward Execution Time (us) : 75.524
Run: 2, Forward Execution Time (us) : 74.507

# Benchmarking PyTorch: qembeddingbag_byte_prepack
# Mode: Eager
# Name: qembeddingbag_byte_prepack_num_embeddings80_embedding_dim512
# Input: num_embeddings: 80, embedding_dim: 512
Run: 0, Forward Execution Time (us) : 139.331
Run: 1, Forward Execution Time (us) : 140.610
Run: 2, Forward Execution Time (us) : 140.475

# Benchmarking PyTorch: qembeddingbag_4bit_prepack
# Mode: Eager
# Name: qembeddingbag_4bit_prepack_num_embeddings80_embedding_dim128
# Input: num_embeddings: 80, embedding_dim: 128
Run: 0, Forward Execution Time (us) : 160.770
Run: 1, Forward Execution Time (us) : 162.003
Run: 2, Forward Execution Time (us) : 162.709

# Benchmarking PyTorch: qembeddingbag_4bit_prepack
# Mode: Eager
# Name: qembeddingbag_4bit_prepack_num_embeddings80_embedding_dim256
# Input: num_embeddings: 80, embedding_dim: 256
Run: 0, Forward Execution Time (us) : 311.541
Run: 1, Forward Execution Time (us) : 313.656
Run: 2, Forward Execution Time (us) : 312.992

# Benchmarking PyTorch: qembeddingbag_4bit_prepack
# Mode: Eager
# Name: qembeddingbag_4bit_prepack_num_embeddings80_embedding_dim512
# Input: num_embeddings: 80, embedding_dim: 512
Run: 0, Forward Execution Time (us) : 612.430
Run: 1, Forward Execution Time (us) : 614.352
Run: 2, Forward Execution Time (us) : 613.657

# Benchmarking PyTorch: qembeddingbag_2bit_prepack
# Mode: Eager
# Name: qembeddingbag_2bit_prepack_num_embeddings80_embedding_dim128
# Input: num_embeddings: 80, embedding_dim: 128
Run: 0, Forward Execution Time (us) : 173.440
Run: 1, Forward Execution Time (us) : 175.045
Run: 2, Forward Execution Time (us) : 175.311

# Benchmarking PyTorch: qembeddingbag_2bit_prepack
# Mode: Eager
# Name: qembeddingbag_2bit_prepack_num_embeddings80_embedding_dim256
# Input: num_embeddings: 80, embedding_dim: 256
Run: 0, Forward Execution Time (us) : 340.914
Run: 1, Forward Execution Time (us) : 342.053
Run: 2, Forward Execution Time (us) : 342.655

# Benchmarking PyTorch: qembeddingbag_2bit_prepack
# Mode: Eager
# Name: qembeddingbag_2bit_prepack_num_embeddings80_embedding_dim512
# Input: num_embeddings: 80, embedding_dim: 512
Run: 0, Forward Execution Time (us) : 677.413
Run: 1, Forward Execution Time (us) : 678.482
Run: 2, Forward Execution Time (us) : 679.661

# Benchmarking PyTorch: qembeddingbag_byte_unpack
# Mode: Eager
# Name: qembeddingbag_byte_unpack_num_embeddings80_embedding_dim128
# Input: num_embeddings: 80, embedding_dim: 128
Run: 0, Forward Execution Time (us) : 59.487
Run: 1, Forward Execution Time (us) : 60.509
Run: 2, Forward Execution Time (us) : 61.469

# Benchmarking PyTorch: qembeddingbag_byte_unpack
# Mode: Eager
# Name: qembeddingbag_byte_unpack_num_embeddings80_embedding_dim256
# Input: num_embeddings: 80, embedding_dim: 256
Run: 0, Forward Execution Time (us) : 129.810
Run: 1, Forward Execution Time (us) : 132.057
Run: 2, Forward Execution Time (us) : 131.341

# Benchmarking PyTorch: qembeddingbag_byte_unpack
# Mode: Eager
# Name: qembeddingbag_byte_unpack_num_embeddings80_embedding_dim512
# Input: num_embeddings: 80, embedding_dim: 512
Run: 0, Forward Execution Time (us) : 239.162
Run: 1, Forward Execution Time (us) : 241.219
Run: 2, Forward Execution Time (us) : 240.458

# Benchmarking PyTorch: qembeddingbag_4bit_unpack
# Mode: Eager
# Name: qembeddingbag_4bit_unpack_num_embeddings80_embedding_dim128
# Input: num_embeddings: 80, embedding_dim: 128
Run: 0, Forward Execution Time (us) : 235.062
Run: 1, Forward Execution Time (us) : 236.196
Run: 2, Forward Execution Time (us) : 236.114

# Benchmarking PyTorch: qembeddingbag_4bit_unpack
# Mode: Eager
# Name: qembeddingbag_4bit_unpack_num_embeddings80_embedding_dim256
# Input: num_embeddings: 80, embedding_dim: 256
Run: 0, Forward Execution Time (us) : 454.667
Run: 1, Forward Execution Time (us) : 491.817
Run: 2, Forward Execution Time (us) : 456.320

# Benchmarking PyTorch: qembeddingbag_4bit_unpack
# Mode: Eager
# Name: qembeddingbag_4bit_unpack_num_embeddings80_embedding_dim512
# Input: num_embeddings: 80, embedding_dim: 512
Run: 0, Forward Execution Time (us) : 894.450
Run: 1, Forward Execution Time (us) : 896.482
Run: 2, Forward Execution Time (us) : 896.231

# Benchmarking PyTorch: qembeddingbag_2bit_unpack
# Mode: Eager
# Name: qembeddingbag_2bit_unpack_num_embeddings80_embedding_dim128
# Input: num_embeddings: 80, embedding_dim: 128
Run: 0, Forward Execution Time (us) : 461.215
Run: 1, Forward Execution Time (us) : 462.651
Run: 2, Forward Execution Time (us) : 463.113

# Benchmarking PyTorch: qembeddingbag_2bit_unpack
# Mode: Eager
# Name: qembeddingbag_2bit_unpack_num_embeddings80_embedding_dim256
# Input: num_embeddings: 80, embedding_dim: 256
Run: 0, Forward Execution Time (us) : 900.547
Run: 1, Forward Execution Time (us) : 902.302
Run: 2, Forward Execution Time (us) : 901.911

# Benchmarking PyTorch: qembeddingbag_2bit_unpack
# Mode: Eager
# Name: qembeddingbag_2bit_unpack_num_embeddings80_embedding_dim512
# Input: num_embeddings: 80, embedding_dim: 512
Run: 0, Forward Execution Time (us) : 1781.881
Run: 1, Forward Execution Time (us) : 1784.206
Run: 2, Forward Execution Time (us) : 1782.744

# Benchmarking PyTorch: qembeddingbag_byte_prepack
# Mode: Eager
# Name: qembeddingbag_byte_prepack_num_embeddings80_embedding_dim128_batch_size10
# Input: num_embeddings: 80, embedding_dim: 128, batch_size: 10
Run: 0, Forward Execution Time (us) : 341.411
Run: 1, Forward Execution Time (us) : 343.451
Run: 2, Forward Execution Time (us) : 342.489

# Benchmarking PyTorch: qembeddingbag_byte_prepack
# Mode: Eager
# Name: qembeddingbag_byte_prepack_num_embeddings80_embedding_dim256_batch_size10
# Input: num_embeddings: 80, embedding_dim: 256, batch_size: 10
Run: 0, Forward Execution Time (us) : 676.462
Run: 1, Forward Execution Time (us) : 677.250
Run: 2, Forward Execution Time (us) : 677.144

# Benchmarking PyTorch: qembeddingbag_byte_prepack
# Mode: Eager
# Name: qembeddingbag_byte_prepack_num_embeddings80_embedding_dim512_batch_size10
# Input: num_embeddings: 80, embedding_dim: 512, batch_size: 10
Run: 0, Forward Execution Time (us) : 1336.982
Run: 1, Forward Execution Time (us) : 1370.998
Run: 2, Forward Execution Time (us) : 1338.404

# Benchmarking PyTorch: qembeddingbag_4bit_prepack
# Mode: Eager
# Name: qembeddingbag_4bit_prepack_num_embeddings80_embedding_dim128_batch_size10
# Input: num_embeddings: 80, embedding_dim: 128, batch_size: 10
Run: 0, Forward Execution Time (us) : 19.815
Run: 1, Forward Execution Time (us) : 21.932
Run: 2, Forward Execution Time (us) : 21.183

# Benchmarking PyTorch: qembeddingbag_4bit_prepack
# Mode: Eager
# Name: qembeddingbag_4bit_prepack_num_embeddings80_embedding_dim256_batch_size10
# Input: num_embeddings: 80, embedding_dim: 256, batch_size: 10
Run: 0, Forward Execution Time (us) : 21.079
Run: 1, Forward Execution Time (us) : 20.826
Run: 2, Forward Execution Time (us) : 21.424

# Benchmarking PyTorch: qembeddingbag_4bit_prepack
# Mode: Eager
# Name: qembeddingbag_4bit_prepack_num_embeddings80_embedding_dim512_batch_size10
# Input: num_embeddings: 80, embedding_dim: 512, batch_size: 10
Run: 0, Forward Execution Time (us) : 19.899
Run: 1, Forward Execution Time (us) : 20.815
Run: 2, Forward Execution Time (us) : 21.573

# Benchmarking PyTorch: qembeddingbag_2bit_prepack
# Mode: Eager
# Name: qembeddingbag_2bit_prepack_num_embeddings80_embedding_dim128_batch_size10
# Input: num_embeddings: 80, embedding_dim: 128, batch_size: 10
Run: 0, Forward Execution Time (us) : 20.296
Run: 1, Forward Execution Time (us) : 21.508
Run: 2, Forward Execution Time (us) : 20.769

# Benchmarking PyTorch: qembeddingbag_2bit_prepack
# Mode: Eager
# Name: qembeddingbag_2bit_prepack_num_embeddings80_embedding_dim256_batch_size10
# Input: num_embeddings: 80, embedding_dim: 256, batch_size: 10
Run: 0, Forward Execution Time (us) : 19.930
Run: 1, Forward Execution Time (us) : 21.451
Run: 2, Forward Execution Time (us) : 20.876

# Benchmarking PyTorch: qembeddingbag_2bit_prepack
# Mode: Eager
# Name: qembeddingbag_2bit_prepack_num_embeddings80_embedding_dim512_batch_size10
# Input: num_embeddings: 80, embedding_dim: 512, batch_size: 10
Run: 0, Forward Execution Time (us) : 19.924
Run: 1, Forward Execution Time (us) : 21.562
Run: 2, Forward Execution Time (us) : 20.910

# Benchmarking PyTorch: qembeddingbag_byte_unpack
# Mode: Eager
# Name: qembeddingbag_byte_unpack_num_embeddings80_embedding_dim128_batch_size10
# Input: num_embeddings: 80, embedding_dim: 128, batch_size: 10
Run: 0, Forward Execution Time (us) : 569.790
Run: 1, Forward Execution Time (us) : 570.758
Run: 2, Forward Execution Time (us) : 571.523

# Benchmarking PyTorch: qembeddingbag_byte_unpack
# Mode: Eager
# Name: qembeddingbag_byte_unpack_num_embeddings80_embedding_dim256_batch_size10
# Input: num_embeddings: 80, embedding_dim: 256, batch_size: 10
Run: 0, Forward Execution Time (us) : 1159.579
Run: 1, Forward Execution Time (us) : 1159.127
Run: 2, Forward Execution Time (us) : 1159.613

# Benchmarking PyTorch: qembeddingbag_byte_unpack
# Mode: Eager
# Name: qembeddingbag_byte_unpack_num_embeddings80_embedding_dim512_batch_size10
# Input: num_embeddings: 80, embedding_dim: 512, batch_size: 10
Run: 0, Forward Execution Time (us) : 2221.712
Run: 1, Forward Execution Time (us) : 2223.339
Run: 2, Forward Execution Time (us) : 2223.808

# Benchmarking PyTorch: qembeddingbag_4bit_unpack
# Mode: Eager
# Name: qembeddingbag_4bit_unpack_num_embeddings80_embedding_dim128_batch_size10
# Input: num_embeddings: 80, embedding_dim: 128, batch_size: 10
Run: 0, Forward Execution Time (us) : 23.719
Run: 1, Forward Execution Time (us) : 25.768
Run: 2, Forward Execution Time (us) : 24.872

# Benchmarking PyTorch: qembeddingbag_4bit_unpack
# Mode: Eager
# Name: qembeddingbag_4bit_unpack_num_embeddings80_embedding_dim256_batch_size10
# Input: num_embeddings: 80, embedding_dim: 256, batch_size: 10
Run: 0, Forward Execution Time (us) : 23.642
Run: 1, Forward Execution Time (us) : 25.202
Run: 2, Forward Execution Time (us) : 25.463

# Benchmarking PyTorch: qembeddingbag_4bit_unpack
# Mode: Eager
# Name: qembeddingbag_4bit_unpack_num_embeddings80_embedding_dim512_batch_size10
# Input: num_embeddings: 80, embedding_dim: 512, batch_size: 10
Run: 0, Forward Execution Time (us) : 23.711
Run: 1, Forward Execution Time (us) : 25.778
Run: 2, Forward Execution Time (us) : 24.861

# Benchmarking PyTorch: qembeddingbag_2bit_unpack
# Mode: Eager
# Name: qembeddingbag_2bit_unpack_num_embeddings80_embedding_dim128_batch_size10
# Input: num_embeddings: 80, embedding_dim: 128, batch_size: 10
Run: 0, Forward Execution Time (us) : 39.965
Run: 1, Forward Execution Time (us) : 41.141
Run: 2, Forward Execution Time (us) : 42.071

# Benchmarking PyTorch: qembeddingbag_2bit_unpack
# Mode: Eager
# Name: qembeddingbag_2bit_unpack_num_embeddings80_embedding_dim256_batch_size10
# Input: num_embeddings: 80, embedding_dim: 256, batch_size: 10
Run: 0, Forward Execution Time (us) : 39.926
Run: 1, Forward Execution Time (us) : 42.003
Run: 2, Forward Execution Time (us) : 41.296

# Benchmarking PyTorch: qembeddingbag_2bit_unpack
# Mode: Eager
# Name: qembeddingbag_2bit_unpack_num_embeddings80_embedding_dim512_batch_size10
# Input: num_embeddings: 80, embedding_dim: 512, batch_size: 10
Run: 0, Forward Execution Time (us) : 39.978
Run: 1, Forward Execution Time (us) : 41.182
Run: 2, Forward Execution Time (us) : 41.716

+ python -m pt.remainder_test --num-runs 3 --iterations 100 --warmup-iterations 10
not use dipu
# ----------------------------------------
# PyTorch/Caffe2 Operator Micro-benchmarks
# ----------------------------------------
# Tag : short

# Benchmarking PyTorch: fmod
# Mode: Eager
# Name: fmod_M1_N1_K1_cpu_dtypetorch.int32
# Input: M: 1, N: 1, K: 1, device: cpu, dtype: torch.int32
Run: 0, Forward Execution Time (us) : 6.552
Run: 1, Forward Execution Time (us) : 7.697
Run: 2, Forward Execution Time (us) : 7.577

# Benchmarking PyTorch: fmod
# Mode: Eager
# Name: fmod_M1_N1_K1_cpu_dtypetorch.float32
# Input: M: 1, N: 1, K: 1, device: cpu, dtype: torch.float32
Run: 0, Forward Execution Time (us) : 7.495
Run: 1, Forward Execution Time (us) : 7.618
Run: 2, Forward Execution Time (us) : 7.602

# Benchmarking PyTorch: fmod
# Mode: Eager
# Name: fmod_M1_N1_K1_cpu_dtypetorch.float64
# Input: M: 1, N: 1, K: 1, device: cpu, dtype: torch.float64
Run: 0, Forward Execution Time (us) : 7.635
Run: 1, Forward Execution Time (us) : 7.663
Run: 2, Forward Execution Time (us) : 7.619

# Benchmarking PyTorch: fmod
# Mode: Eager
# Name: fmod_M1_N1_K1_cuda_dtypetorch.int32
# Input: M: 1, N: 1, K: 1, device: cuda, dtype: torch.int32
Run: 0, Forward Execution Time (us) : 13.147
Run: 1, Forward Execution Time (us) : 15.535
Run: 2, Forward Execution Time (us) : 15.515

# Benchmarking PyTorch: fmod
# Mode: Eager
# Name: fmod_M1_N1_K1_cuda_dtypetorch.float32
# Input: M: 1, N: 1, K: 1, device: cuda, dtype: torch.float32
Run: 0, Forward Execution Time (us) : 15.394
Run: 1, Forward Execution Time (us) : 15.650
Run: 2, Forward Execution Time (us) : 15.553

# Benchmarking PyTorch: fmod
# Mode: Eager
# Name: fmod_M1_N1_K1_cuda_dtypetorch.float64
# Input: M: 1, N: 1, K: 1, device: cuda, dtype: torch.float64
Run: 0, Forward Execution Time (us) : 15.044
Run: 1, Forward Execution Time (us) : 15.194
Run: 2, Forward Execution Time (us) : 15.212

# Benchmarking PyTorch: fmod
# Mode: Eager
# Name: fmod_M64_N64_K64_cpu_dtypetorch.int32
# Input: M: 64, N: 64, K: 64, device: cpu, dtype: torch.int32
Run: 0, Forward Execution Time (us) : 650.512
Run: 1, Forward Execution Time (us) : 654.522
Run: 2, Forward Execution Time (us) : 652.634

# Benchmarking PyTorch: fmod
# Mode: Eager
# Name: fmod_M64_N64_K64_cpu_dtypetorch.float32
# Input: M: 64, N: 64, K: 64, device: cpu, dtype: torch.float32
Run: 0, Forward Execution Time (us) : 849.816
Run: 1, Forward Execution Time (us) : 964.136
Run: 2, Forward Execution Time (us) : 851.144

# Benchmarking PyTorch: fmod
# Mode: Eager
# Name: fmod_M64_N64_K64_cpu_dtypetorch.float64
# Input: M: 64, N: 64, K: 64, device: cpu, dtype: torch.float64
Run: 0, Forward Execution Time (us) : 1672.033
Run: 1, Forward Execution Time (us) : 1671.590
Run: 2, Forward Execution Time (us) : 1673.305

# Benchmarking PyTorch: fmod
# Mode: Eager
# Name: fmod_M64_N64_K64_cuda_dtypetorch.int32
# Input: M: 64, N: 64, K: 64, device: cuda, dtype: torch.int32
Run: 0, Forward Execution Time (us) : 15.156
Run: 1, Forward Execution Time (us) : 15.300
Run: 2, Forward Execution Time (us) : 15.282

# Benchmarking PyTorch: fmod
# Mode: Eager
# Name: fmod_M64_N64_K64_cuda_dtypetorch.float32
# Input: M: 64, N: 64, K: 64, device: cuda, dtype: torch.float32
Run: 0, Forward Execution Time (us) : 15.167
Run: 1, Forward Execution Time (us) : 15.314
Run: 2, Forward Execution Time (us) : 15.237

# Benchmarking PyTorch: fmod
# Mode: Eager
# Name: fmod_M64_N64_K64_cuda_dtypetorch.float64
# Input: M: 64, N: 64, K: 64, device: cuda, dtype: torch.float64
Run: 0, Forward Execution Time (us) : 15.427
Run: 1, Forward Execution Time (us) : 15.514
Run: 2, Forward Execution Time (us) : 15.488

# Benchmarking PyTorch: fmod
# Mode: Eager
# Name: fmod_M64_N64_K128_cpu_dtypetorch.int32
# Input: M: 64, N: 64, K: 128, device: cpu, dtype: torch.int32
Run: 0, Forward Execution Time (us) : 1285.307
Run: 1, Forward Execution Time (us) : 1286.088
Run: 2, Forward Execution Time (us) : 1285.834

# Benchmarking PyTorch: fmod
# Mode: Eager
# Name: fmod_M64_N64_K128_cpu_dtypetorch.float32
# Input: M: 64, N: 64, K: 128, device: cpu, dtype: torch.float32
Run: 0, Forward Execution Time (us) : 1698.157
Run: 1, Forward Execution Time (us) : 1700.085
Run: 2, Forward Execution Time (us) : 1699.660

# Benchmarking PyTorch: fmod
# Mode: Eager
# Name: fmod_M64_N64_K128_cpu_dtypetorch.float64
# Input: M: 64, N: 64, K: 128, device: cpu, dtype: torch.float64
Run: 0, Forward Execution Time (us) : 3329.942
Run: 1, Forward Execution Time (us) : 3386.813
Run: 2, Forward Execution Time (us) : 3344.703

# Benchmarking PyTorch: fmod
# Mode: Eager
# Name: fmod_M64_N64_K128_cuda_dtypetorch.int32
# Input: M: 64, N: 64, K: 128, device: cuda, dtype: torch.int32
Run: 0, Forward Execution Time (us) : 15.381
Run: 1, Forward Execution Time (us) : 15.625
Run: 2, Forward Execution Time (us) : 15.435

# Benchmarking PyTorch: fmod
# Mode: Eager
# Name: fmod_M64_N64_K128_cuda_dtypetorch.float32
# Input: M: 64, N: 64, K: 128, device: cuda, dtype: torch.float32
Run: 0, Forward Execution Time (us) : 15.418
Run: 1, Forward Execution Time (us) : 15.690
Run: 2, Forward Execution Time (us) : 15.679

# Benchmarking PyTorch: fmod
# Mode: Eager
# Name: fmod_M64_N64_K128_cuda_dtypetorch.float64
# Input: M: 64, N: 64, K: 128, device: cuda, dtype: torch.float64
Run: 0, Forward Execution Time (us) : 25.594
Run: 1, Forward Execution Time (us) : 25.707
Run: 2, Forward Execution Time (us) : 25.605

# Benchmarking PyTorch: remainder
# Mode: Eager
# Name: remainder_M1_N1_K1_cpu_dtypetorch.int32
# Input: M: 1, N: 1, K: 1, device: cpu, dtype: torch.int32
Run: 0, Forward Execution Time (us) : 6.351
Run: 1, Forward Execution Time (us) : 7.775
Run: 2, Forward Execution Time (us) : 7.641

# Benchmarking PyTorch: remainder
# Mode: Eager
# Name: remainder_M1_N1_K1_cpu_dtypetorch.float32
# Input: M: 1, N: 1, K: 1, device: cpu, dtype: torch.float32
Run: 0, Forward Execution Time (us) : 7.666
Run: 1, Forward Execution Time (us) : 7.668
Run: 2, Forward Execution Time (us) : 7.624

# Benchmarking PyTorch: remainder
# Mode: Eager
# Name: remainder_M1_N1_K1_cpu_dtypetorch.float64
# Input: M: 1, N: 1, K: 1, device: cpu, dtype: torch.float64
Run: 0, Forward Execution Time (us) : 7.630
Run: 1, Forward Execution Time (us) : 7.680
Run: 2, Forward Execution Time (us) : 7.638

# Benchmarking PyTorch: remainder
# Mode: Eager
# Name: remainder_M1_N1_K1_cuda_dtypetorch.int32
# Input: M: 1, N: 1, K: 1, device: cuda, dtype: torch.int32
Run: 0, Forward Execution Time (us) : 15.384
Run: 1, Forward Execution Time (us) : 15.705
Run: 2, Forward Execution Time (us) : 15.628

# Benchmarking PyTorch: remainder
# Mode: Eager
# Name: remainder_M1_N1_K1_cuda_dtypetorch.float32
# Input: M: 1, N: 1, K: 1, device: cuda, dtype: torch.float32
Run: 0, Forward Execution Time (us) : 15.324
Run: 1, Forward Execution Time (us) : 15.566
Run: 2, Forward Execution Time (us) : 15.541

# Benchmarking PyTorch: remainder
# Mode: Eager
# Name: remainder_M1_N1_K1_cuda_dtypetorch.float64
# Input: M: 1, N: 1, K: 1, device: cuda, dtype: torch.float64
Run: 0, Forward Execution Time (us) : 15.253
Run: 1, Forward Execution Time (us) : 15.326
Run: 2, Forward Execution Time (us) : 15.268

# Benchmarking PyTorch: remainder
# Mode: Eager
# Name: remainder_M64_N64_K64_cpu_dtypetorch.int32
# Input: M: 64, N: 64, K: 64, device: cpu, dtype: torch.int32
Run: 0, Forward Execution Time (us) : 1183.141
Run: 1, Forward Execution Time (us) : 1181.049
Run: 2, Forward Execution Time (us) : 1189.618

# Benchmarking PyTorch: remainder
# Mode: Eager
# Name: remainder_M64_N64_K64_cpu_dtypetorch.float32
# Input: M: 64, N: 64, K: 64, device: cpu, dtype: torch.float32
Run: 0, Forward Execution Time (us) : 1035.191
Run: 1, Forward Execution Time (us) : 1018.233
Run: 2, Forward Execution Time (us) : 1021.716

# Benchmarking PyTorch: remainder
# Mode: Eager
# Name: remainder_M64_N64_K64_cpu_dtypetorch.float64
# Input: M: 64, N: 64, K: 64, device: cpu, dtype: torch.float64
Run: 0, Forward Execution Time (us) : 1825.473
Run: 1, Forward Execution Time (us) : 1831.792
Run: 2, Forward Execution Time (us) : 1830.371

# Benchmarking PyTorch: remainder
# Mode: Eager
# Name: remainder_M64_N64_K64_cuda_dtypetorch.int32
# Input: M: 64, N: 64, K: 64, device: cuda, dtype: torch.int32
Run: 0, Forward Execution Time (us) : 15.151
Run: 1, Forward Execution Time (us) : 15.253
Run: 2, Forward Execution Time (us) : 15.235

# Benchmarking PyTorch: remainder
# Mode: Eager
# Name: remainder_M64_N64_K64_cuda_dtypetorch.float32
# Input: M: 64, N: 64, K: 64, device: cuda, dtype: torch.float32
Run: 0, Forward Execution Time (us) : 15.620
Run: 1, Forward Execution Time (us) : 15.330
Run: 2, Forward Execution Time (us) : 15.376

# Benchmarking PyTorch: remainder
# Mode: Eager
# Name: remainder_M64_N64_K64_cuda_dtypetorch.float64
# Input: M: 64, N: 64, K: 64, device: cuda, dtype: torch.float64
Run: 0, Forward Execution Time (us) : 15.629
Run: 1, Forward Execution Time (us) : 15.861
Run: 2, Forward Execution Time (us) : 15.781

# Benchmarking PyTorch: remainder
# Mode: Eager
# Name: remainder_M64_N64_K128_cpu_dtypetorch.int32
# Input: M: 64, N: 64, K: 128, device: cpu, dtype: torch.int32
Run: 0, Forward Execution Time (us) : 2361.806
Run: 1, Forward Execution Time (us) : 2364.160
Run: 2, Forward Execution Time (us) : 2366.278

# Benchmarking PyTorch: remainder
# Mode: Eager
# Name: remainder_M64_N64_K128_cpu_dtypetorch.float32
# Input: M: 64, N: 64, K: 128, device: cpu, dtype: torch.float32
Run: 0, Forward Execution Time (us) : 2032.978
Run: 1, Forward Execution Time (us) : 2041.935
Run: 2, Forward Execution Time (us) : 2020.849

# Benchmarking PyTorch: remainder
# Mode: Eager
# Name: remainder_M64_N64_K128_cpu_dtypetorch.float64
# Input: M: 64, N: 64, K: 128, device: cpu, dtype: torch.float64
Run: 0, Forward Execution Time (us) : 3622.671
Run: 1, Forward Execution Time (us) : 3632.296
Run: 2, Forward Execution Time (us) : 3657.549

# Benchmarking PyTorch: remainder
# Mode: Eager
# Name: remainder_M64_N64_K128_cuda_dtypetorch.int32
# Input: M: 64, N: 64, K: 128, device: cuda, dtype: torch.int32
Run: 0, Forward Execution Time (us) : 15.515
Run: 1, Forward Execution Time (us) : 15.770
Run: 2, Forward Execution Time (us) : 15.646

# Benchmarking PyTorch: remainder
# Mode: Eager
# Name: remainder_M64_N64_K128_cuda_dtypetorch.float32
# Input: M: 64, N: 64, K: 128, device: cuda, dtype: torch.float32
Run: 0, Forward Execution Time (us) : 15.557
Run: 1, Forward Execution Time (us) : 15.736
Run: 2, Forward Execution Time (us) : 15.840

# Benchmarking PyTorch: remainder
# Mode: Eager
# Name: remainder_M64_N64_K128_cuda_dtypetorch.float64
# Input: M: 64, N: 64, K: 128, device: cuda, dtype: torch.float64
Run: 0, Forward Execution Time (us) : 25.944
Run: 1, Forward Execution Time (us) : 26.010
Run: 2, Forward Execution Time (us) : 26.017

+ python -m pt.cat_test --num-runs 3 --iterations 100 --warmup-iterations 10
not use dipu
# ----------------------------------------
# PyTorch/Caffe2 Operator Micro-benchmarks
# ----------------------------------------
# Tag : short

# Benchmarking PyTorch: cat
# Mode: Eager
# Name: cat_sizes(1,1,1)_N2_dim0_cpu
# Input: sizes: (1, 1, 1), N: 2, dim: 0, device: cpu
Run: 0, Forward Execution Time (us) : 5.741
Run: 1, Forward Execution Time (us) : 6.876
Run: 2, Forward Execution Time (us) : 6.715

# Benchmarking PyTorch: cat
# Mode: Eager
# Name: cat_sizes(1,1,1)_N2_dim0_cuda
# Input: sizes: (1, 1, 1), N: 2, dim: 0, device: cuda
Run: 0, Forward Execution Time (us) : 13.362
Run: 1, Forward Execution Time (us) : 16.006
Run: 2, Forward Execution Time (us) : 15.642

# Benchmarking PyTorch: cat
# Mode: Eager
# Name: cat_sizes(512,512,2)_N2_dim1_cpu
# Input: sizes: (512, 512, 2), N: 2, dim: 1, device: cpu
Run: 0, Forward Execution Time (us) : 328.486
Run: 1, Forward Execution Time (us) : 336.377
Run: 2, Forward Execution Time (us) : 336.250

# Benchmarking PyTorch: cat
# Mode: Eager
# Name: cat_sizes(512,512,2)_N2_dim1_cuda
# Input: sizes: (512, 512, 2), N: 2, dim: 1, device: cuda
Run: 0, Forward Execution Time (us) : 15.587
Run: 1, Forward Execution Time (us) : 15.783
Run: 2, Forward Execution Time (us) : 15.640

# Benchmarking PyTorch: cat
# Mode: Eager
# Name: cat_sizes(128,1024,2)_N2_dim1_cpu
# Input: sizes: (128, 1024, 2), N: 2, dim: 1, device: cpu
Run: 0, Forward Execution Time (us) : 169.135
Run: 1, Forward Execution Time (us) : 173.585
Run: 2, Forward Execution Time (us) : 173.618

# Benchmarking PyTorch: cat
# Mode: Eager
# Name: cat_sizes(128,1024,2)_N2_dim1_cuda
# Input: sizes: (128, 1024, 2), N: 2, dim: 1, device: cuda
Run: 0, Forward Execution Time (us) : 15.398
Run: 1, Forward Execution Time (us) : 15.509
Run: 2, Forward Execution Time (us) : 15.445

+ python -m pt.hardsigmoid_test --num-runs 3 --iterations 100 --warmup-iterations 10
not use dipu
# ----------------------------------------
# PyTorch/Caffe2 Operator Micro-benchmarks
# ----------------------------------------
# Tag : short

# Benchmarking PyTorch: Hardsigmoid
# Mode: Eager
# Name: Hardsigmoid_N1_C3_H256_W256_cpu
# Input: N: 1, C: 3, H: 256, W: 256, device: cpu
Run: 0, Forward Execution Time (us) : 96.088
Run: 1, Forward Execution Time (us) : 91.257
Run: 2, Forward Execution Time (us) : 91.442

# Benchmarking PyTorch: Hardsigmoid
# Mode: Eager
# Name: Hardsigmoid_N4_C3_H256_W256_cpu
# Input: N: 4, C: 3, H: 256, W: 256, device: cpu
Run: 0, Forward Execution Time (us) : 1188.787
Run: 1, Forward Execution Time (us) : 1164.707
Run: 2, Forward Execution Time (us) : 1169.660

+ python -m pt.nan_to_num_test --num-runs 3 --iterations 100 --warmup-iterations 10
not use dipu
# ----------------------------------------
# PyTorch/Caffe2 Operator Micro-benchmarks
# ----------------------------------------
# Tag : short

# Benchmarking PyTorch: nan_to_num
# Mode: Eager
# Name: nan_to_num_M16_N64_dtypetorch.float32_replace_infTrue
# Input: M: 16, N: 64, dtype: torch.float32, replace_inf: True
Run: 0, Forward Execution Time (us) : 8.916
Run: 1, Forward Execution Time (us) : 10.782
Run: 2, Forward Execution Time (us) : 10.722

# Benchmarking PyTorch: nan_to_num
# Mode: Eager
# Name: nan_to_num_M16_N64_dtypetorch.float32_replace_infFalse
# Input: M: 16, N: 64, dtype: torch.float32, replace_inf: False
Run: 0, Forward Execution Time (us) : 11.269
Run: 1, Forward Execution Time (us) : 11.322
Run: 2, Forward Execution Time (us) : 11.340

# Benchmarking PyTorch: nan_to_num
# Mode: Eager
# Name: nan_to_num_M16_N64_dtypetorch.float64_replace_infTrue
# Input: M: 16, N: 64, dtype: torch.float64, replace_inf: True
Run: 0, Forward Execution Time (us) : 10.937
Run: 1, Forward Execution Time (us) : 11.023
Run: 2, Forward Execution Time (us) : 10.886

# Benchmarking PyTorch: nan_to_num
# Mode: Eager
# Name: nan_to_num_M16_N64_dtypetorch.float64_replace_infFalse
# Input: M: 16, N: 64, dtype: torch.float64, replace_inf: False
Run: 0, Forward Execution Time (us) : 11.579
Run: 1, Forward Execution Time (us) : 11.559
Run: 2, Forward Execution Time (us) : 11.569

# Benchmarking PyTorch: nan_to_num
# Mode: Eager
# Name: nan_to_num_M16_N64_dtypetorch.float32_replace_infTrue
# Input: M: 16, N: 64, dtype: torch.float32, replace_inf: True
Run: 0, Forward Execution Time (us) : 10.712
Run: 1, Forward Execution Time (us) : 10.782
Run: 2, Forward Execution Time (us) : 10.731

# Benchmarking PyTorch: nan_to_num
# Mode: Eager
# Name: nan_to_num_M16_N64_dtypetorch.float32_replace_infFalse
# Input: M: 16, N: 64, dtype: torch.float32, replace_inf: False
Run: 0, Forward Execution Time (us) : 11.331
Run: 1, Forward Execution Time (us) : 11.318
Run: 2, Forward Execution Time (us) : 11.322

# Benchmarking PyTorch: nan_to_num
# Mode: Eager
# Name: nan_to_num_M16_N64_dtypetorch.float64_replace_infTrue
# Input: M: 16, N: 64, dtype: torch.float64, replace_inf: True
Run: 0, Forward Execution Time (us) : 10.944
Run: 1, Forward Execution Time (us) : 10.996
Run: 2, Forward Execution Time (us) : 10.998

# Benchmarking PyTorch: nan_to_num
# Mode: Eager
# Name: nan_to_num_M16_N64_dtypetorch.float64_replace_infFalse
# Input: M: 16, N: 64, dtype: torch.float64, replace_inf: False
Run: 0, Forward Execution Time (us) : 11.608
Run: 1, Forward Execution Time (us) : 11.585
Run: 2, Forward Execution Time (us) : 11.532

# Benchmarking PyTorch: nan_to_num
# Mode: Eager
# Name: nan_to_num_M64_N64_dtypetorch.float32_replace_infTrue
# Input: M: 64, N: 64, dtype: torch.float32, replace_inf: True
Run: 0, Forward Execution Time (us) : 15.239
Run: 1, Forward Execution Time (us) : 15.291
Run: 2, Forward Execution Time (us) : 15.344

# Benchmarking PyTorch: nan_to_num
# Mode: Eager
# Name: nan_to_num_M64_N64_dtypetorch.float32_replace_infFalse
# Input: M: 64, N: 64, dtype: torch.float32, replace_inf: False
Run: 0, Forward Execution Time (us) : 15.870
Run: 1, Forward Execution Time (us) : 15.895
Run: 2, Forward Execution Time (us) : 15.931

# Benchmarking PyTorch: nan_to_num
# Mode: Eager
# Name: nan_to_num_M64_N64_dtypetorch.float64_replace_infTrue
# Input: M: 64, N: 64, dtype: torch.float64, replace_inf: True
Run: 0, Forward Execution Time (us) : 15.847
Run: 1, Forward Execution Time (us) : 15.986
Run: 2, Forward Execution Time (us) : 15.958

# Benchmarking PyTorch: nan_to_num
# Mode: Eager
# Name: nan_to_num_M64_N64_dtypetorch.float64_replace_infFalse
# Input: M: 64, N: 64, dtype: torch.float64, replace_inf: False
Run: 0, Forward Execution Time (us) : 16.476
Run: 1, Forward Execution Time (us) : 16.599
Run: 2, Forward Execution Time (us) : 16.554

# Benchmarking PyTorch: nan_to_num
# Mode: Eager
# Name: nan_to_num_M64_N64_dtypetorch.float32_replace_infTrue
# Input: M: 64, N: 64, dtype: torch.float32, replace_inf: True
Run: 0, Forward Execution Time (us) : 15.231
Run: 1, Forward Execution Time (us) : 15.319
Run: 2, Forward Execution Time (us) : 15.265

# Benchmarking PyTorch: nan_to_num
# Mode: Eager
# Name: nan_to_num_M64_N64_dtypetorch.float32_replace_infFalse
# Input: M: 64, N: 64, dtype: torch.float32, replace_inf: False
Run: 0, Forward Execution Time (us) : 15.814
Run: 1, Forward Execution Time (us) : 15.909
Run: 2, Forward Execution Time (us) : 15.902

# Benchmarking PyTorch: nan_to_num
# Mode: Eager
# Name: nan_to_num_M64_N64_dtypetorch.float64_replace_infTrue
# Input: M: 64, N: 64, dtype: torch.float64, replace_inf: True
Run: 0, Forward Execution Time (us) : 15.820
Run: 1, Forward Execution Time (us) : 15.972
Run: 2, Forward Execution Time (us) : 15.918

# Benchmarking PyTorch: nan_to_num
# Mode: Eager
# Name: nan_to_num_M64_N64_dtypetorch.float64_replace_infFalse
# Input: M: 64, N: 64, dtype: torch.float64, replace_inf: False
Run: 0, Forward Execution Time (us) : 16.470
Run: 1, Forward Execution Time (us) : 16.586
Run: 2, Forward Execution Time (us) : 16.558

# Benchmarking PyTorch: nan_to_num
# Mode: Eager
# Name: nan_to_num_M16_N64_dtypetorch.float32_replace_infTrue
# Input: M: 16, N: 64, dtype: torch.float32, replace_inf: True
Run: 0, Forward Execution Time (us) : 6.142
Run: 1, Forward Execution Time (us) : 6.917
Run: 2, Forward Execution Time (us) : 6.816

# Benchmarking PyTorch: nan_to_num
# Mode: Eager
# Name: nan_to_num_M16_N64_dtypetorch.float32_replace_infFalse
# Input: M: 16, N: 64, dtype: torch.float32, replace_inf: False
Run: 0, Forward Execution Time (us) : 7.316
Run: 1, Forward Execution Time (us) : 7.339
Run: 2, Forward Execution Time (us) : 7.315

# Benchmarking PyTorch: nan_to_num
# Mode: Eager
# Name: nan_to_num_M16_N64_dtypetorch.float64_replace_infTrue
# Input: M: 16, N: 64, dtype: torch.float64, replace_inf: True
Run: 0, Forward Execution Time (us) : 6.979
Run: 1, Forward Execution Time (us) : 7.009
Run: 2, Forward Execution Time (us) : 6.945

# Benchmarking PyTorch: nan_to_num
# Mode: Eager
# Name: nan_to_num_M16_N64_dtypetorch.float64_replace_infFalse
# Input: M: 16, N: 64, dtype: torch.float64, replace_inf: False
Run: 0, Forward Execution Time (us) : 7.531
Run: 1, Forward Execution Time (us) : 7.496
Run: 2, Forward Execution Time (us) : 7.496

# Benchmarking PyTorch: nan_to_num
# Mode: Eager
# Name: nan_to_num_M16_N64_dtypetorch.float32_replace_infTrue
# Input: M: 16, N: 64, dtype: torch.float32, replace_inf: True
Run: 0, Forward Execution Time (us) : 6.847
Run: 1, Forward Execution Time (us) : 6.889
Run: 2, Forward Execution Time (us) : 6.870

# Benchmarking PyTorch: nan_to_num
# Mode: Eager
# Name: nan_to_num_M16_N64_dtypetorch.float32_replace_infFalse
# Input: M: 16, N: 64, dtype: torch.float32, replace_inf: False
Run: 0, Forward Execution Time (us) : 7.275
Run: 1, Forward Execution Time (us) : 7.405
Run: 2, Forward Execution Time (us) : 7.392

# Benchmarking PyTorch: nan_to_num
# Mode: Eager
# Name: nan_to_num_M16_N64_dtypetorch.float64_replace_infTrue
# Input: M: 16, N: 64, dtype: torch.float64, replace_inf: True
Run: 0, Forward Execution Time (us) : 6.933
Run: 1, Forward Execution Time (us) : 6.972
Run: 2, Forward Execution Time (us) : 6.975

# Benchmarking PyTorch: nan_to_num
# Mode: Eager
# Name: nan_to_num_M16_N64_dtypetorch.float64_replace_infFalse
# Input: M: 16, N: 64, dtype: torch.float64, replace_inf: False
Run: 0, Forward Execution Time (us) : 7.544
Run: 1, Forward Execution Time (us) : 7.592
Run: 2, Forward Execution Time (us) : 7.586

# Benchmarking PyTorch: nan_to_num
# Mode: Eager
# Name: nan_to_num_M64_N64_dtypetorch.float32_replace_infTrue
# Input: M: 64, N: 64, dtype: torch.float32, replace_inf: True
Run: 0, Forward Execution Time (us) : 11.403
Run: 1, Forward Execution Time (us) : 11.431
Run: 2, Forward Execution Time (us) : 11.472

# Benchmarking PyTorch: nan_to_num
# Mode: Eager
# Name: nan_to_num_M64_N64_dtypetorch.float32_replace_infFalse
# Input: M: 64, N: 64, dtype: torch.float32, replace_inf: False
Run: 0, Forward Execution Time (us) : 12.039
Run: 1, Forward Execution Time (us) : 12.015
Run: 2, Forward Execution Time (us) : 12.034

# Benchmarking PyTorch: nan_to_num
# Mode: Eager
# Name: nan_to_num_M64_N64_dtypetorch.float64_replace_infTrue
# Input: M: 64, N: 64, dtype: torch.float64, replace_inf: True
Run: 0, Forward Execution Time (us) : 12.068
Run: 1, Forward Execution Time (us) : 12.143
Run: 2, Forward Execution Time (us) : 12.118

# Benchmarking PyTorch: nan_to_num
# Mode: Eager
# Name: nan_to_num_M64_N64_dtypetorch.float64_replace_infFalse
# Input: M: 64, N: 64, dtype: torch.float64, replace_inf: False
Run: 0, Forward Execution Time (us) : 12.662
Run: 1, Forward Execution Time (us) : 12.632
Run: 2, Forward Execution Time (us) : 12.651

# Benchmarking PyTorch: nan_to_num
# Mode: Eager
# Name: nan_to_num_M64_N64_dtypetorch.float32_replace_infTrue
# Input: M: 64, N: 64, dtype: torch.float32, replace_inf: True
Run: 0, Forward Execution Time (us) : 11.475
Run: 1, Forward Execution Time (us) : 11.430
Run: 2, Forward Execution Time (us) : 11.437

# Benchmarking PyTorch: nan_to_num
# Mode: Eager
# Name: nan_to_num_M64_N64_dtypetorch.float32_replace_infFalse
# Input: M: 64, N: 64, dtype: torch.float32, replace_inf: False
Run: 0, Forward Execution Time (us) : 11.951
Run: 1, Forward Execution Time (us) : 11.955
Run: 2, Forward Execution Time (us) : 12.023

# Benchmarking PyTorch: nan_to_num
# Mode: Eager
# Name: nan_to_num_M64_N64_dtypetorch.float64_replace_infTrue
# Input: M: 64, N: 64, dtype: torch.float64, replace_inf: True
Run: 0, Forward Execution Time (us) : 12.052
Run: 1, Forward Execution Time (us) : 12.030
Run: 2, Forward Execution Time (us) : 12.040

# Benchmarking PyTorch: nan_to_num
# Mode: Eager
# Name: nan_to_num_M64_N64_dtypetorch.float64_replace_infFalse
# Input: M: 64, N: 64, dtype: torch.float64, replace_inf: False
Run: 0, Forward Execution Time (us) : 12.589
Run: 1, Forward Execution Time (us) : 12.564
Run: 2, Forward Execution Time (us) : 12.606

+ python -m pt.qgroupnorm_test --num-runs 3 --iterations 100 --warmup-iterations 10
not use dipu
# ----------------------------------------
# PyTorch/Caffe2 Operator Micro-benchmarks
# ----------------------------------------
# Tag : short

# Benchmarking PyTorch: QGroupNormBenchmark
# Mode: Eager
# Name: QGroupNormBenchmark_dims(32,8,16)_num_groups2_dtypetorch.qint8
# Input: dims: (32, 8, 16), num_groups: 2, dtype: torch.qint8
Run: 0, Forward Execution Time (us) : 33.304
Run: 1, Forward Execution Time (us) : 39.724
Run: 2, Forward Execution Time (us) : 39.354

# Benchmarking PyTorch: QGroupNormBenchmark
# Mode: Eager
# Name: QGroupNormBenchmark_dims(32,8,16)_num_groups4_dtypetorch.qint8
# Input: dims: (32, 8, 16), num_groups: 4, dtype: torch.qint8
Run: 0, Forward Execution Time (us) : 41.867
Run: 1, Forward Execution Time (us) : 41.811
Run: 2, Forward Execution Time (us) : 41.909

# Benchmarking PyTorch: QGroupNormBenchmark
# Mode: Eager
# Name: QGroupNormBenchmark_dims(32,8,56,56)_num_groups2_dtypetorch.qint8
# Input: dims: (32, 8, 56, 56), num_groups: 2, dtype: torch.qint8
Run: 0, Forward Execution Time (us) : 451.757
Run: 1, Forward Execution Time (us) : 452.229
Run: 2, Forward Execution Time (us) : 452.113

# Benchmarking PyTorch: QGroupNormBenchmark
# Mode: Eager
# Name: QGroupNormBenchmark_dims(32,8,56,56)_num_groups4_dtypetorch.qint8
# Input: dims: (32, 8, 56, 56), num_groups: 4, dtype: torch.qint8
Run: 0, Forward Execution Time (us) : 454.497
Run: 1, Forward Execution Time (us) : 454.816
Run: 2, Forward Execution Time (us) : 457.102

+ python -m pt.softmax_test --num-runs 3 --iterations 100 --warmup-iterations 10
/nvme/share/share/platform/env/miniconda3.8/envs/pt2.0_diopi/pytorch2.0/benchmarks/operator_benchmark/pt/softmax_test.py:79: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return self.op_func(input)
/nvme/share/share/platform/env/miniconda3.8/envs/pt2.0_diopi/pytorch2.0/benchmarks/operator_benchmark/pt/softmax_test.py:79: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  return self.op_func(input)
not use dipu
# ----------------------------------------
# PyTorch/Caffe2 Operator Micro-benchmarks
# ----------------------------------------
# Tag : short

# Benchmarking PyTorch: Softmax
# Mode: Eager
# Name: Softmax_N1_C3_H256_W256_cpu
# Input: N: 1, C: 3, H: 256, W: 256, device: cpu
Run: 0, Forward Execution Time (us) : 705.746
Run: 1, Forward Execution Time (us) : 702.626
Run: 2, Forward Execution Time (us) : 702.277

# Benchmarking PyTorch: Softmax
# Mode: Eager
# Name: Softmax_N1_C3_H256_W256_cuda
# Input: N: 1, C: 3, H: 256, W: 256, device: cuda
Run: 0, Forward Execution Time (us) : 22.216
Run: 1, Forward Execution Time (us) : 26.580
Run: 2, Forward Execution Time (us) : 26.219

# Benchmarking PyTorch: Softmax
# Mode: Eager
# Name: Softmax_N4_C3_H256_W256_cpu
# Input: N: 4, C: 3, H: 256, W: 256, device: cpu
Run: 0, Forward Execution Time (us) : 2885.479
Run: 1, Forward Execution Time (us) : 2844.625
Run: 2, Forward Execution Time (us) : 2845.104

# Benchmarking PyTorch: Softmax
# Mode: Eager
# Name: Softmax_N4_C3_H256_W256_cuda
# Input: N: 4, C: 3, H: 256, W: 256, device: cuda
Run: 0, Forward Execution Time (us) : 25.771
Run: 1, Forward Execution Time (us) : 25.748
Run: 2, Forward Execution Time (us) : 25.740

# Benchmarking PyTorch: Softmax2d
# Mode: Eager
# Name: Softmax2d_N1_C3_H256_W256_cpu
# Input: N: 1, C: 3, H: 256, W: 256, device: cpu
Run: 0, Forward Execution Time (us) : 725.073
Run: 1, Forward Execution Time (us) : 726.190
Run: 2, Forward Execution Time (us) : 726.237

# Benchmarking PyTorch: Softmax2d
# Mode: Eager
# Name: Softmax2d_N1_C3_H256_W256_cuda
# Input: N: 1, C: 3, H: 256, W: 256, device: cuda
Run: 0, Forward Execution Time (us) : 23.003
Run: 1, Forward Execution Time (us) : 23.362
Run: 2, Forward Execution Time (us) : 23.315

# Benchmarking PyTorch: Softmax2d
# Mode: Eager
# Name: Softmax2d_N4_C3_H256_W256_cpu
# Input: N: 4, C: 3, H: 256, W: 256, device: cpu
Run: 0, Forward Execution Time (us) : 2817.818
Run: 1, Forward Execution Time (us) : 2817.445
Run: 2, Forward Execution Time (us) : 2880.561

# Benchmarking PyTorch: Softmax2d
# Mode: Eager
# Name: Softmax2d_N4_C3_H256_W256_cuda
# Input: N: 4, C: 3, H: 256, W: 256, device: cuda
Run: 0, Forward Execution Time (us) : 22.724
Run: 1, Forward Execution Time (us) : 22.688
Run: 2, Forward Execution Time (us) : 22.682

# Benchmarking PyTorch: LogSoftmax
# Mode: Eager
# Name: LogSoftmax_N1_C3_H256_W256_cpu
# Input: N: 1, C: 3, H: 256, W: 256, device: cpu
Run: 0, Forward Execution Time (us) : 859.002
Run: 1, Forward Execution Time (us) : 899.233
Run: 2, Forward Execution Time (us) : 859.452

# Benchmarking PyTorch: LogSoftmax
# Mode: Eager
# Name: LogSoftmax_N1_C3_H256_W256_cuda
# Input: N: 1, C: 3, H: 256, W: 256, device: cuda
Run: 0, Forward Execution Time (us) : 25.882
Run: 1, Forward Execution Time (us) : 26.038
Run: 2, Forward Execution Time (us) : 26.011

# Benchmarking PyTorch: LogSoftmax
# Mode: Eager
# Name: LogSoftmax_N4_C3_H256_W256_cpu
# Input: N: 4, C: 3, H: 256, W: 256, device: cpu
Run: 0, Forward Execution Time (us) : 3305.567
Run: 1, Forward Execution Time (us) : 3310.049
Run: 2, Forward Execution Time (us) : 3309.419

# Benchmarking PyTorch: LogSoftmax
# Mode: Eager
# Name: LogSoftmax_N4_C3_H256_W256_cuda
# Input: N: 4, C: 3, H: 256, W: 256, device: cuda
Run: 0, Forward Execution Time (us) : 25.694
Run: 1, Forward Execution Time (us) : 25.729
Run: 2, Forward Execution Time (us) : 25.668

+ python -m pt.channel_shuffle_test --num-runs 3 --iterations 100 --warmup-iterations 10
not use dipu
# ----------------------------------------
# PyTorch/Caffe2 Operator Micro-benchmarks
# ----------------------------------------
# Tag : short

# Benchmarking PyTorch: channel_shuffle
# Mode: Eager
# Name: channel_shuffle_batch_size2_channels_per_group16_height16_width16_groups2_channel_lastTrue
# Input: batch_size: 2, channels_per_group: 16, height: 16, width: 16, groups: 2, channel_last: True
Run: 0, Forward Execution Time (us) : 18.679
Run: 1, Forward Execution Time (us) : 19.802
Run: 2, Forward Execution Time (us) : 20.708

# Benchmarking PyTorch: channel_shuffle
# Mode: Eager
# Name: channel_shuffle_batch_size2_channels_per_group16_height16_width16_groups2_channel_lastFalse
# Input: batch_size: 2, channels_per_group: 16, height: 16, width: 16, groups: 2, channel_last: False
Run: 0, Forward Execution Time (us) : 10.805
Run: 1, Forward Execution Time (us) : 10.436
Run: 2, Forward Execution Time (us) : 10.275

# Benchmarking PyTorch: channel_shuffle
# Mode: Eager
# Name: channel_shuffle_batch_size2_channels_per_group32_height32_width32_groups2_channel_lastTrue
# Input: batch_size: 2, channels_per_group: 32, height: 32, width: 32, groups: 2, channel_last: True
Run: 0, Forward Execution Time (us) : 102.606
Run: 1, Forward Execution Time (us) : 102.808
Run: 2, Forward Execution Time (us) : 102.945

# Benchmarking PyTorch: channel_shuffle
# Mode: Eager
# Name: channel_shuffle_batch_size2_channels_per_group32_height32_width32_groups2_channel_lastFalse
# Input: batch_size: 2, channels_per_group: 32, height: 32, width: 32, groups: 2, channel_last: False
Run: 0, Forward Execution Time (us) : 40.091
Run: 1, Forward Execution Time (us) : 40.996
Run: 2, Forward Execution Time (us) : 40.652

# Benchmarking PyTorch: channel_shuffle
# Mode: Eager
# Name: channel_shuffle_batch_size4_channels_per_group32_height32_width32_groups4_channel_lastTrue
# Input: batch_size: 4, channels_per_group: 32, height: 32, width: 32, groups: 4, channel_last: True
Run: 0, Forward Execution Time (us) : 1007.942
Run: 1, Forward Execution Time (us) : 1007.861
Run: 2, Forward Execution Time (us) : 1003.230

# Benchmarking PyTorch: channel_shuffle
# Mode: Eager
# Name: channel_shuffle_batch_size4_channels_per_group32_height32_width32_groups4_channel_lastFalse
# Input: batch_size: 4, channels_per_group: 32, height: 32, width: 32, groups: 4, channel_last: False
Run: 0, Forward Execution Time (us) : 725.379
Run: 1, Forward Execution Time (us) : 725.326
Run: 2, Forward Execution Time (us) : 721.268

# Benchmarking PyTorch: channel_shuffle
# Mode: Eager
# Name: channel_shuffle_batch_size4_channels_per_group64_height64_width64_groups4_channel_lastTrue
# Input: batch_size: 4, channels_per_group: 64, height: 64, width: 64, groups: 4, channel_last: True
Run: 0, Forward Execution Time (us) : 9468.090
Run: 1, Forward Execution Time (us) : 9326.591
Run: 2, Forward Execution Time (us) : 9292.474

# Benchmarking PyTorch: channel_shuffle
# Mode: Eager
# Name: channel_shuffle_batch_size4_channels_per_group64_height64_width64_groups4_channel_lastFalse
# Input: batch_size: 4, channels_per_group: 64, height: 64, width: 64, groups: 4, channel_last: False
Run: 0, Forward Execution Time (us) : 7877.694
Run: 1, Forward Execution Time (us) : 7781.831
Run: 2, Forward Execution Time (us) : 7719.551

# Benchmarking PyTorch: channel_shuffle
# Mode: Eager
# Name: channel_shuffle_batch_size8_channels_per_group64_height64_width64_groups8_channel_lastTrue
# Input: batch_size: 8, channels_per_group: 64, height: 64, width: 64, groups: 8, channel_last: True
Run: 0, Forward Execution Time (us) : 55629.900
Run: 1, Forward Execution Time (us) : 55561.801
Run: 2, Forward Execution Time (us) : 55651.952

# Benchmarking PyTorch: channel_shuffle
# Mode: Eager
# Name: channel_shuffle_batch_size8_channels_per_group64_height64_width64_groups8_channel_lastFalse
# Input: batch_size: 8, channels_per_group: 64, height: 64, width: 64, groups: 8, channel_last: False
Run: 0, Forward Execution Time (us) : 47617.762
Run: 1, Forward Execution Time (us) : 47811.930
Run: 2, Forward Execution Time (us) : 47703.705

# Benchmarking PyTorch: channel_shuffle
# Mode: Eager
# Name: channel_shuffle_batch_size16_channels_per_group64_height64_width64_groups16_channel_lastTrue
# Input: batch_size: 16, channels_per_group: 64, height: 64, width: 64, groups: 16, channel_last: True
Run: 0, Forward Execution Time (us) : 230485.215
Run: 1, Forward Execution Time (us) : 230362.079
Run: 2, Forward Execution Time (us) : 230169.514

# Benchmarking PyTorch: channel_shuffle
# Mode: Eager
# Name: channel_shuffle_batch_size16_channels_per_group64_height64_width64_groups16_channel_lastFalse
# Input: batch_size: 16, channels_per_group: 64, height: 64, width: 64, groups: 16, channel_last: False
Run: 0, Forward Execution Time (us) : 195070.758
Run: 1, Forward Execution Time (us) : 195867.721
Run: 2, Forward Execution Time (us) : 195186.360

+ python -m pt.hardswish_test --num-runs 3 --iterations 100 --warmup-iterations 10
not use dipu
# ----------------------------------------
# PyTorch/Caffe2 Operator Micro-benchmarks
# ----------------------------------------
# Tag : short

# Benchmarking PyTorch: Hardswish
# Mode: Eager
# Name: Hardswish_N1_C3_H256_W256_cpu
# Input: N: 1, C: 3, H: 256, W: 256, device: cpu
Run: 0, Forward Execution Time (us) : 93.762
Run: 1, Forward Execution Time (us) : 88.856
Run: 2, Forward Execution Time (us) : 88.891

# Benchmarking PyTorch: Hardswish
# Mode: Eager
# Name: Hardswish_N4_C3_H256_W256_cpu
# Input: N: 4, C: 3, H: 256, W: 256, device: cpu
Run: 0, Forward Execution Time (us) : 1247.414
Run: 1, Forward Execution Time (us) : 1198.864
Run: 2, Forward Execution Time (us) : 1205.172

+ python -m pt.pool_test --num-runs 3 --iterations 100 --warmup-iterations 10
not use dipu
# ----------------------------------------
# PyTorch/Caffe2 Operator Micro-benchmarks
# ----------------------------------------
# Tag : short

# Benchmarking PyTorch: MaxPool1d
# Mode: Eager
# Name: MaxPool1d_kernel3_stride1_N8_C256_L256_cpu
# Input: kernel: 3, stride: 1, N: 8, C: 256, L: 256, device: cpu
Run: 0, Forward Execution Time (us) : 662.096
Run: 1, Forward Execution Time (us) : 686.935
Run: 2, Forward Execution Time (us) : 662.677

# Benchmarking PyTorch: MaxPool1d
# Mode: Eager
# Name: MaxPool1d_kernel3_stride1_N8_C256_L256_cuda
# Input: kernel: 3, stride: 1, N: 8, C: 256, L: 256, device: cuda
Run: 0, Forward Execution Time (us) : 32.207
Run: 1, Forward Execution Time (us) : 38.053
Run: 2, Forward Execution Time (us) : 37.951

# Benchmarking PyTorch: AvgPool1d
# Mode: Eager
# Name: AvgPool1d_kernel3_stride1_N8_C256_L256_cpu
# Input: kernel: 3, stride: 1, N: 8, C: 256, L: 256, device: cpu
Run: 0, Forward Execution Time (us) : 6251.385
Run: 1, Forward Execution Time (us) : 6252.741
Run: 2, Forward Execution Time (us) : 6215.778

# Benchmarking PyTorch: AvgPool1d
# Mode: Eager
# Name: AvgPool1d_kernel3_stride1_N8_C256_L256_cuda
# Input: kernel: 3, stride: 1, N: 8, C: 256, L: 256, device: cuda
Run: 0, Forward Execution Time (us) : 29.411
Run: 1, Forward Execution Time (us) : 29.642
Run: 2, Forward Execution Time (us) : 29.622

# Benchmarking PyTorch: MaxPool2d
# Mode: Eager
# Name: MaxPool2d_kernel[3,1]_stride[2,1]_N1_C16_H32_W32_cpu
# Input: kernel: [3, 1], stride: [2, 1], N: 1, C: 16, H: 32, W: 32, device: cpu
Run: 0, Forward Execution Time (us) : 158.169
Run: 1, Forward Execution Time (us) : 158.326
Run: 2, Forward Execution Time (us) : 158.327

# Benchmarking PyTorch: MaxPool2d
# Mode: Eager
# Name: MaxPool2d_kernel[3,1]_stride[2,1]_N1_C16_H32_W32_cuda
# Input: kernel: [3, 1], stride: [2, 1], N: 1, C: 16, H: 32, W: 32, device: cuda
Run: 0, Forward Execution Time (us) : 28.566
Run: 1, Forward Execution Time (us) : 28.731
Run: 2, Forward Execution Time (us) : 28.693

# Benchmarking PyTorch: AvgPool2d
# Mode: Eager
# Name: AvgPool2d_kernel[3,1]_stride[2,1]_N1_C16_H32_W32_cpu
# Input: kernel: [3, 1], stride: [2, 1], N: 1, C: 16, H: 32, W: 32, device: cpu
Run: 0, Forward Execution Time (us) : 109.052
Run: 1, Forward Execution Time (us) : 110.506
Run: 2, Forward Execution Time (us) : 110.201

# Benchmarking PyTorch: AvgPool2d
# Mode: Eager
# Name: AvgPool2d_kernel[3,1]_stride[2,1]_N1_C16_H32_W32_cuda
# Input: kernel: [3, 1], stride: [2, 1], N: 1, C: 16, H: 32, W: 32, device: cuda
Run: 0, Forward Execution Time (us) : 22.186
Run: 1, Forward Execution Time (us) : 22.327
Run: 2, Forward Execution Time (us) : 22.354

# Benchmarking PyTorch: AdaptiveMaxPool2d
# Mode: Eager
# Name: AdaptiveMaxPool2d_kernel[3,1]_stride[2,1]_N1_C16_H32_W32_cpu
# Input: kernel: [3, 1], stride: [2, 1], N: 1, C: 16, H: 32, W: 32, device: cpu
Run: 0, Forward Execution Time (us) : 40.040
Run: 1, Forward Execution Time (us) : 41.917
Run: 2, Forward Execution Time (us) : 41.207

# Benchmarking PyTorch: AdaptiveMaxPool2d
# Mode: Eager
# Name: AdaptiveMaxPool2d_kernel[3,1]_stride[2,1]_N1_C16_H32_W32_cuda
# Input: kernel: [3, 1], stride: [2, 1], N: 1, C: 16, H: 32, W: 32, device: cuda
Run: 0, Forward Execution Time (us) : 31.666
Run: 1, Forward Execution Time (us) : 31.790
Run: 2, Forward Execution Time (us) : 31.879

# Benchmarking PyTorch: FractionalMaxPool2d
# Mode: Eager
# Name: FractionalMaxPool2d_kernel[3,1]_stride[2,1]_N1_C16_H32_W32_cpu
# Input: kernel: [3, 1], stride: [2, 1], N: 1, C: 16, H: 32, W: 32, device: cpu
Run: 0, Forward Execution Time (us) : 32.915
Run: 1, Forward Execution Time (us) : 39.161
Run: 2, Forward Execution Time (us) : 38.999

# Benchmarking PyTorch: FractionalMaxPool2d
# Mode: Eager
# Name: FractionalMaxPool2d_kernel[3,1]_stride[2,1]_N1_C16_H32_W32_cuda
# Input: kernel: [3, 1], stride: [2, 1], N: 1, C: 16, H: 32, W: 32, device: cuda
Run: 0, Forward Execution Time (us) : 54.967
Run: 1, Forward Execution Time (us) : 55.412
Run: 2, Forward Execution Time (us) : 55.293

# Benchmarking PyTorch: MaxPool3d
# Mode: Eager
# Name: MaxPool3d_kernel[3,1,3]_stride[2,1,2]_N1_C16_D16_H32_W32_cpu
# Input: kernel: [3, 1, 3], stride: [2, 1, 2], N: 1, C: 16, D: 16, H: 32, W: 32, device: cpu
Run: 0, Forward Execution Time (us) : 1869.740
Run: 1, Forward Execution Time (us) : 1872.567
Run: 2, Forward Execution Time (us) : 1914.191

# Benchmarking PyTorch: MaxPool3d
# Mode: Eager
# Name: MaxPool3d_kernel[3,1,3]_stride[2,1,2]_N1_C16_D16_H32_W32_cuda
# Input: kernel: [3, 1, 3], stride: [2, 1, 2], N: 1, C: 16, D: 16, H: 32, W: 32, device: cuda
Run: 0, Forward Execution Time (us) : 32.784
Run: 1, Forward Execution Time (us) : 32.887
Run: 2, Forward Execution Time (us) : 32.665

# Benchmarking PyTorch: AvgPool3d
# Mode: Eager
# Name: AvgPool3d_kernel[3,1,3]_stride[2,1,2]_N1_C16_D16_H32_W32_cpu
# Input: kernel: [3, 1, 3], stride: [2, 1, 2], N: 1, C: 16, D: 16, H: 32, W: 32, device: cpu
Run: 0, Forward Execution Time (us) : 1252.906
Run: 1, Forward Execution Time (us) : 1225.346
Run: 2, Forward Execution Time (us) : 1222.075

# Benchmarking PyTorch: AvgPool3d
# Mode: Eager
# Name: AvgPool3d_kernel[3,1,3]_stride[2,1,2]_N1_C16_D16_H32_W32_cuda
# Input: kernel: [3, 1, 3], stride: [2, 1, 2], N: 1, C: 16, D: 16, H: 32, W: 32, device: cuda
Run: 0, Forward Execution Time (us) : 30.286
Run: 1, Forward Execution Time (us) : 30.379
Run: 2, Forward Execution Time (us) : 30.381

# Benchmarking PyTorch: AdaptiveMaxPool3d
# Mode: Eager
# Name: AdaptiveMaxPool3d_kernel[3,1,3]_stride[2,1,2]_N1_C16_D16_H32_W32_cpu
# Input: kernel: [3, 1, 3], stride: [2, 1, 2], N: 1, C: 16, D: 16, H: 32, W: 32, device: cpu
Run: 0, Forward Execution Time (us) : 326.603
Run: 1, Forward Execution Time (us) : 322.427
Run: 2, Forward Execution Time (us) : 325.401

# Benchmarking PyTorch: AdaptiveMaxPool3d
# Mode: Eager
# Name: AdaptiveMaxPool3d_kernel[3,1,3]_stride[2,1,2]_N1_C16_D16_H32_W32_cuda
# Input: kernel: [3, 1, 3], stride: [2, 1, 2], N: 1, C: 16, D: 16, H: 32, W: 32, device: cuda
Run: 0, Forward Execution Time (us) : 209.103
Run: 1, Forward Execution Time (us) : 209.177
Run: 2, Forward Execution Time (us) : 209.094

# Benchmarking PyTorch: FractionalMaxPool3d
# Mode: Eager
# Name: FractionalMaxPool3d_kernel[3,1,3]_stride[2,1,2]_N1_C16_D16_H32_W32_cpu
# Input: kernel: [3, 1, 3], stride: [2, 1, 2], N: 1, C: 16, D: 16, H: 32, W: 32, device: cpu
Run: 0, Forward Execution Time (us) : 38.222
Run: 1, Forward Execution Time (us) : 45.550
Run: 2, Forward Execution Time (us) : 45.241

# Benchmarking PyTorch: FractionalMaxPool3d
# Mode: Eager
# Name: FractionalMaxPool3d_kernel[3,1,3]_stride[2,1,2]_N1_C16_D16_H32_W32_cuda
# Input: kernel: [3, 1, 3], stride: [2, 1, 2], N: 1, C: 16, D: 16, H: 32, W: 32, device: cuda
Run: 0, Forward Execution Time (us) : 56.070
Run: 1, Forward Execution Time (us) : 56.261
Run: 2, Forward Execution Time (us) : 56.128

+ python -m pt.qinstancenorm_test --num-runs 3 --iterations 100 --warmup-iterations 10
not use dipu
# ----------------------------------------
# PyTorch/Caffe2 Operator Micro-benchmarks
# ----------------------------------------
# Tag : short

# Benchmarking PyTorch: QInstanceNormBenchmark
# Mode: Eager
# Name: QInstanceNormBenchmark_dims(32,8,16)_dtypetorch.qint8
# Input: dims: (32, 8, 16), dtype: torch.qint8
Run: 0, Forward Execution Time (us) : 41.058
Run: 1, Forward Execution Time (us) : 48.864
Run: 2, Forward Execution Time (us) : 48.690

# Benchmarking PyTorch: QInstanceNormBenchmark
# Mode: Eager
# Name: QInstanceNormBenchmark_dims(32,8,56,56)_dtypetorch.qint8
# Input: dims: (32, 8, 56, 56), dtype: torch.qint8
Run: 0, Forward Execution Time (us) : 452.077
Run: 1, Forward Execution Time (us) : 452.321
Run: 2, Forward Execution Time (us) : 452.269

+ python -m pt.split_test --num-runs 3 --iterations 100 --warmup-iterations 10
not use dipu
# ----------------------------------------
# PyTorch/Caffe2 Operator Micro-benchmarks
# ----------------------------------------
# Tag : short

# Benchmarking PyTorch: split
# Mode: Eager
# Name: split_M8_N8_parts2_cpu
# Input: M: 8, N: 8, parts: 2, device: cpu
Run: 0, Forward Execution Time (us) : 11.397
Run: 1, Forward Execution Time (us) : 12.852
Run: 2, Forward Execution Time (us) : 13.305

# Benchmarking PyTorch: split
# Mode: Eager
# Name: split_M8_N8_parts2_cuda
# Input: M: 8, N: 8, parts: 2, device: cuda
Run: 0, Forward Execution Time (us) : 11.596
Run: 1, Forward Execution Time (us) : 13.090
Run: 2, Forward Execution Time (us) : 13.343

# Benchmarking PyTorch: split
# Mode: Eager
# Name: split_M256_N512_parts2_cpu
# Input: M: 256, N: 512, parts: 2, device: cpu
Run: 0, Forward Execution Time (us) : 11.371
Run: 1, Forward Execution Time (us) : 13.376
Run: 2, Forward Execution Time (us) : 12.568

# Benchmarking PyTorch: split
# Mode: Eager
# Name: split_M256_N512_parts2_cuda
# Input: M: 256, N: 512, parts: 2, device: cuda
Run: 0, Forward Execution Time (us) : 12.606
Run: 1, Forward Execution Time (us) : 12.961
Run: 2, Forward Execution Time (us) : 13.580

# Benchmarking PyTorch: split
# Mode: Eager
# Name: split_M512_N512_parts2_cpu
# Input: M: 512, N: 512, parts: 2, device: cpu
Run: 0, Forward Execution Time (us) : 11.249
Run: 1, Forward Execution Time (us) : 12.784
Run: 2, Forward Execution Time (us) : 13.432

# Benchmarking PyTorch: split
# Mode: Eager
# Name: split_M512_N512_parts2_cuda
# Input: M: 512, N: 512, parts: 2, device: cuda
Run: 0, Forward Execution Time (us) : 13.181
Run: 1, Forward Execution Time (us) : 12.618
Run: 2, Forward Execution Time (us) : 13.070

+ python -m pt.chunk_test --num-runs 3 --iterations 100 --warmup-iterations 10
not use dipu
# ----------------------------------------
# PyTorch/Caffe2 Operator Micro-benchmarks
# ----------------------------------------
# Tag : short

# Benchmarking PyTorch: chunk
# Mode: Eager
# Name: chunk_M8_N8_chunks2_cpu
# Input: M: 8, N: 8, chunks: 2, device: cpu
Run: 0, Forward Execution Time (us) : 10.206
Run: 1, Forward Execution Time (us) : 12.200
Run: 2, Forward Execution Time (us) : 11.402

# Benchmarking PyTorch: chunk
# Mode: Eager
# Name: chunk_M8_N8_chunks2_cuda
# Input: M: 8, N: 8, chunks: 2, device: cuda
Run: 0, Forward Execution Time (us) : 10.469
Run: 1, Forward Execution Time (us) : 11.812
Run: 2, Forward Execution Time (us) : 11.906

# Benchmarking PyTorch: chunk
# Mode: Eager
# Name: chunk_M256_N512_chunks2_cpu
# Input: M: 256, N: 512, chunks: 2, device: cpu
Run: 0, Forward Execution Time (us) : 10.298
Run: 1, Forward Execution Time (us) : 12.242
Run: 2, Forward Execution Time (us) : 11.612

# Benchmarking PyTorch: chunk
# Mode: Eager
# Name: chunk_M256_N512_chunks2_cuda
# Input: M: 256, N: 512, chunks: 2, device: cuda
Run: 0, Forward Execution Time (us) : 11.585
Run: 1, Forward Execution Time (us) : 12.481
Run: 2, Forward Execution Time (us) : 11.680

# Benchmarking PyTorch: chunk
# Mode: Eager
# Name: chunk_M512_N512_chunks2_cpu
# Input: M: 512, N: 512, chunks: 2, device: cpu
Run: 0, Forward Execution Time (us) : 11.376
Run: 1, Forward Execution Time (us) : 12.136
Run: 2, Forward Execution Time (us) : 11.533

# Benchmarking PyTorch: chunk
# Mode: Eager
# Name: chunk_M512_N512_chunks2_cuda
# Input: M: 512, N: 512, chunks: 2, device: cuda
Run: 0, Forward Execution Time (us) : 11.700
Run: 1, Forward Execution Time (us) : 12.413
Run: 2, Forward Execution Time (us) : 11.779

+ python -m pt.index_select_test --num-runs 3 --iterations 100 --warmup-iterations 10
not use dipu
# ----------------------------------------
# PyTorch/Caffe2 Operator Micro-benchmarks
# ----------------------------------------
# Tag : short

# Benchmarking PyTorch: index_select
# Mode: Eager
# Name: index_select_M8_N8_K1_dim1_cpu
# Input: M: 8, N: 8, K: 1, dim: 1, device: cpu
Run: 0, Forward Execution Time (us) : 6.424
Run: 1, Forward Execution Time (us) : 7.495
Run: 2, Forward Execution Time (us) : 7.447

# Benchmarking PyTorch: index_select
# Mode: Eager
# Name: index_select_M8_N8_K1_dim1_cuda
# Input: M: 8, N: 8, K: 1, dim: 1, device: cuda
Run: 0, Forward Execution Time (us) : 16.096
Run: 1, Forward Execution Time (us) : 19.333
Run: 2, Forward Execution Time (us) : 18.951

# Benchmarking PyTorch: index_select
# Mode: Eager
# Name: index_select_M256_N512_K1_dim1_cpu
# Input: M: 256, N: 512, K: 1, dim: 1, device: cpu
Run: 0, Forward Execution Time (us) : 107.713
Run: 1, Forward Execution Time (us) : 109.194
Run: 2, Forward Execution Time (us) : 108.829

# Benchmarking PyTorch: index_select
# Mode: Eager
# Name: index_select_M256_N512_K1_dim1_cuda
# Input: M: 256, N: 512, K: 1, dim: 1, device: cuda
Run: 0, Forward Execution Time (us) : 18.813
Run: 1, Forward Execution Time (us) : 19.157
Run: 2, Forward Execution Time (us) : 18.966

# Benchmarking PyTorch: index_select
# Mode: Eager
# Name: index_select_M512_N512_K1_dim1_cpu
# Input: M: 512, N: 512, K: 1, dim: 1, device: cpu
Run: 0, Forward Execution Time (us) : 214.211
Run: 1, Forward Execution Time (us) : 216.856
Run: 2, Forward Execution Time (us) : 217.198

# Benchmarking PyTorch: index_select
# Mode: Eager
# Name: index_select_M512_N512_K1_dim1_cuda
# Input: M: 512, N: 512, K: 1, dim: 1, device: cuda
Run: 0, Forward Execution Time (us) : 19.250
Run: 1, Forward Execution Time (us) : 19.436
Run: 2, Forward Execution Time (us) : 19.413

# Benchmarking PyTorch: index_select
# Mode: Eager
# Name: index_select_M8_N8_K2_dim1_cpu
# Input: M: 8, N: 8, K: 2, dim: 1, device: cpu
Run: 0, Forward Execution Time (us) : 7.889
Run: 1, Forward Execution Time (us) : 7.944
Run: 2, Forward Execution Time (us) : 7.906

# Benchmarking PyTorch: index_select
# Mode: Eager
# Name: index_select_M8_N8_K2_dim1_cuda
# Input: M: 8, N: 8, K: 2, dim: 1, device: cuda
Run: 0, Forward Execution Time (us) : 18.788
Run: 1, Forward Execution Time (us) : 19.032
Run: 2, Forward Execution Time (us) : 18.952

# Benchmarking PyTorch: index_select
# Mode: Eager
# Name: index_select_M256_N512_K2_dim1_cpu
# Input: M: 256, N: 512, K: 2, dim: 1, device: cpu
Run: 0, Forward Execution Time (us) : 2212.900
Run: 1, Forward Execution Time (us) : 2252.773
Run: 2, Forward Execution Time (us) : 2213.808

# Benchmarking PyTorch: index_select
# Mode: Eager
# Name: index_select_M256_N512_K2_dim1_cuda
# Input: M: 256, N: 512, K: 2, dim: 1, device: cuda
Run: 0, Forward Execution Time (us) : 19.137
Run: 1, Forward Execution Time (us) : 19.394
Run: 2, Forward Execution Time (us) : 19.244

# Benchmarking PyTorch: index_select
# Mode: Eager
# Name: index_select_M512_N512_K2_dim1_cpu
# Input: M: 512, N: 512, K: 2, dim: 1, device: cpu
Run: 0, Forward Execution Time (us) : 4410.508
Run: 1, Forward Execution Time (us) : 4450.217
Run: 2, Forward Execution Time (us) : 4410.828

# Benchmarking PyTorch: index_select
# Mode: Eager
# Name: index_select_M512_N512_K2_dim1_cuda
# Input: M: 512, N: 512, K: 2, dim: 1, device: cuda
Run: 0, Forward Execution Time (us) : 18.840
Run: 1, Forward Execution Time (us) : 19.156
Run: 2, Forward Execution Time (us) : 18.975

+ python -m pt.qactivation_test --num-runs 3 --iterations 100 --warmup-iterations 10
not use dipu
# ----------------------------------------
# PyTorch/Caffe2 Operator Micro-benchmarks
# ----------------------------------------
# Tag : short

# Benchmarking PyTorch: relu
# Mode: Eager
# Name: relu_dims(3,4,5)_contigFalse_inplaceFalse_dtypetorch.quint8
# Input: dims: (3, 4, 5), contig: False, inplace: False, dtype: torch.quint8
Run: 0, Forward Execution Time (us) : 13.903
Run: 1, Forward Execution Time (us) : 16.306
Run: 2, Forward Execution Time (us) : 16.057

# Benchmarking PyTorch: relu
# Mode: Eager
# Name: relu_dims(3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint8
# Input: dims: (3, 4, 5), contig: False, inplace: False, dtype: torch.qint8
Run: 0, Forward Execution Time (us) : 16.195
Run: 1, Forward Execution Time (us) : 16.253
Run: 2, Forward Execution Time (us) : 16.267

# Benchmarking PyTorch: relu
# Mode: Eager
# Name: relu_dims(3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint32
# Input: dims: (3, 4, 5), contig: False, inplace: False, dtype: torch.qint32
Run: 0, Forward Execution Time (us) : 16.159
Run: 1, Forward Execution Time (us) : 16.115
Run: 2, Forward Execution Time (us) : 16.148

# Benchmarking PyTorch: relu
# Mode: Eager
# Name: relu_dims(2,3,4,5)_contigFalse_inplaceFalse_dtypetorch.quint8
# Input: dims: (2, 3, 4, 5), contig: False, inplace: False, dtype: torch.quint8
Run: 0, Forward Execution Time (us) : 17.257
Run: 1, Forward Execution Time (us) : 17.292
Run: 2, Forward Execution Time (us) : 17.239

# Benchmarking PyTorch: relu
# Mode: Eager
# Name: relu_dims(2,3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint8
# Input: dims: (2, 3, 4, 5), contig: False, inplace: False, dtype: torch.qint8
Run: 0, Forward Execution Time (us) : 17.260
Run: 1, Forward Execution Time (us) : 17.324
Run: 2, Forward Execution Time (us) : 17.394

# Benchmarking PyTorch: relu
# Mode: Eager
# Name: relu_dims(2,3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint32
# Input: dims: (2, 3, 4, 5), contig: False, inplace: False, dtype: torch.qint32
Run: 0, Forward Execution Time (us) : 17.321
Run: 1, Forward Execution Time (us) : 17.295
Run: 2, Forward Execution Time (us) : 17.267

# Benchmarking PyTorch: relu
# Mode: Eager
# Name: relu_dims(512,512)_contigFalse_inplaceFalse_dtypetorch.quint8
# Input: dims: (512, 512), contig: False, inplace: False, dtype: torch.quint8
Run: 0, Forward Execution Time (us) : 234.390
Run: 1, Forward Execution Time (us) : 277.807
Run: 2, Forward Execution Time (us) : 276.280

# Benchmarking PyTorch: relu
# Mode: Eager
# Name: relu_dims(512,512)_contigFalse_inplaceFalse_dtypetorch.qint8
# Input: dims: (512, 512), contig: False, inplace: False, dtype: torch.qint8
Run: 0, Forward Execution Time (us) : 236.251
Run: 1, Forward Execution Time (us) : 280.072
Run: 2, Forward Execution Time (us) : 279.150

# Benchmarking PyTorch: relu
# Mode: Eager
# Name: relu_dims(512,512)_contigFalse_inplaceFalse_dtypetorch.qint32
# Input: dims: (512, 512), contig: False, inplace: False, dtype: torch.qint32
Run: 0, Forward Execution Time (us) : 650.299
Run: 1, Forward Execution Time (us) : 651.475
Run: 2, Forward Execution Time (us) : 652.012

# Benchmarking PyTorch: relu
# Mode: Eager
# Name: relu_dims(256,1024)_contigFalse_inplaceFalse_dtypetorch.quint8
# Input: dims: (256, 1024), contig: False, inplace: False, dtype: torch.quint8
Run: 0, Forward Execution Time (us) : 247.358
Run: 1, Forward Execution Time (us) : 292.555
Run: 2, Forward Execution Time (us) : 292.478

# Benchmarking PyTorch: relu
# Mode: Eager
# Name: relu_dims(256,1024)_contigFalse_inplaceFalse_dtypetorch.qint8
# Input: dims: (256, 1024), contig: False, inplace: False, dtype: torch.qint8
Run: 0, Forward Execution Time (us) : 247.981
Run: 1, Forward Execution Time (us) : 293.271
Run: 2, Forward Execution Time (us) : 293.454

# Benchmarking PyTorch: relu
# Mode: Eager
# Name: relu_dims(256,1024)_contigFalse_inplaceFalse_dtypetorch.qint32
# Input: dims: (256, 1024), contig: False, inplace: False, dtype: torch.qint32
Run: 0, Forward Execution Time (us) : 701.962
Run: 1, Forward Execution Time (us) : 702.278
Run: 2, Forward Execution Time (us) : 702.812

# Benchmarking PyTorch: relu6
# Mode: Eager
# Name: relu6_dims(3,4,5)_contigFalse_inplaceFalse_dtypetorch.quint8
# Input: dims: (3, 4, 5), contig: False, inplace: False, dtype: torch.quint8
Run: 0, Forward Execution Time (us) : 12.926
Run: 1, Forward Execution Time (us) : 14.303
Run: 2, Forward Execution Time (us) : 14.139

# Benchmarking PyTorch: relu6
# Mode: Eager
# Name: relu6_dims(3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint8
# Input: dims: (3, 4, 5), contig: False, inplace: False, dtype: torch.qint8
Run: 0, Forward Execution Time (us) : 14.195
Run: 1, Forward Execution Time (us) : 14.190
Run: 2, Forward Execution Time (us) : 14.131

# Benchmarking PyTorch: relu6
# Mode: Eager
# Name: relu6_dims(3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint32
# Input: dims: (3, 4, 5), contig: False, inplace: False, dtype: torch.qint32
Run: 0, Forward Execution Time (us) : 14.238
Run: 1, Forward Execution Time (us) : 14.101
Run: 2, Forward Execution Time (us) : 14.145

# Benchmarking PyTorch: relu6
# Mode: Eager
# Name: relu6_dims(2,3,4,5)_contigFalse_inplaceFalse_dtypetorch.quint8
# Input: dims: (2, 3, 4, 5), contig: False, inplace: False, dtype: torch.quint8
Run: 0, Forward Execution Time (us) : 15.286
Run: 1, Forward Execution Time (us) : 15.339
Run: 2, Forward Execution Time (us) : 15.412

# Benchmarking PyTorch: relu6
# Mode: Eager
# Name: relu6_dims(2,3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint8
# Input: dims: (2, 3, 4, 5), contig: False, inplace: False, dtype: torch.qint8
Run: 0, Forward Execution Time (us) : 15.282
Run: 1, Forward Execution Time (us) : 15.420
Run: 2, Forward Execution Time (us) : 15.348

# Benchmarking PyTorch: relu6
# Mode: Eager
# Name: relu6_dims(2,3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint32
# Input: dims: (2, 3, 4, 5), contig: False, inplace: False, dtype: torch.qint32
Run: 0, Forward Execution Time (us) : 15.342
Run: 1, Forward Execution Time (us) : 15.404
Run: 2, Forward Execution Time (us) : 15.363

# Benchmarking PyTorch: relu6
# Mode: Eager
# Name: relu6_dims(512,512)_contigFalse_inplaceFalse_dtypetorch.quint8
# Input: dims: (512, 512), contig: False, inplace: False, dtype: torch.quint8
Run: 0, Forward Execution Time (us) : 348.337
Run: 1, Forward Execution Time (us) : 414.093
Run: 2, Forward Execution Time (us) : 413.551

# Benchmarking PyTorch: relu6
# Mode: Eager
# Name: relu6_dims(512,512)_contigFalse_inplaceFalse_dtypetorch.qint8
# Input: dims: (512, 512), contig: False, inplace: False, dtype: torch.qint8
Run: 0, Forward Execution Time (us) : 330.777
Run: 1, Forward Execution Time (us) : 393.401
Run: 2, Forward Execution Time (us) : 393.339

# Benchmarking PyTorch: relu6
# Mode: Eager
# Name: relu6_dims(512,512)_contigFalse_inplaceFalse_dtypetorch.qint32
# Input: dims: (512, 512), contig: False, inplace: False, dtype: torch.qint32
Run: 0, Forward Execution Time (us) : 834.486
Run: 1, Forward Execution Time (us) : 835.448
Run: 2, Forward Execution Time (us) : 837.763

# Benchmarking PyTorch: relu6
# Mode: Eager
# Name: relu6_dims(256,1024)_contigFalse_inplaceFalse_dtypetorch.quint8
# Input: dims: (256, 1024), contig: False, inplace: False, dtype: torch.quint8
Run: 0, Forward Execution Time (us) : 357.357
Run: 1, Forward Execution Time (us) : 422.864
Run: 2, Forward Execution Time (us) : 422.228

# Benchmarking PyTorch: relu6
# Mode: Eager
# Name: relu6_dims(256,1024)_contigFalse_inplaceFalse_dtypetorch.qint8
# Input: dims: (256, 1024), contig: False, inplace: False, dtype: torch.qint8
Run: 0, Forward Execution Time (us) : 335.814
Run: 1, Forward Execution Time (us) : 395.848
Run: 2, Forward Execution Time (us) : 394.536

# Benchmarking PyTorch: relu6
# Mode: Eager
# Name: relu6_dims(256,1024)_contigFalse_inplaceFalse_dtypetorch.qint32
# Input: dims: (256, 1024), contig: False, inplace: False, dtype: torch.qint32
Run: 0, Forward Execution Time (us) : 897.703
Run: 1, Forward Execution Time (us) : 899.068
Run: 2, Forward Execution Time (us) : 898.801

# Benchmarking PyTorch: functional.hardtanh
# Mode: Eager
# Name: functional.hardtanh_dims(3,4,5)_contigFalse_inplaceFalse_dtypetorch.quint8
# Input: dims: (3, 4, 5), contig: False, inplace: False, dtype: torch.quint8
Run: 0, Forward Execution Time (us) : 12.004
Run: 1, Forward Execution Time (us) : 13.692
Run: 2, Forward Execution Time (us) : 13.644

# Benchmarking PyTorch: functional.hardtanh
# Mode: Eager
# Name: functional.hardtanh_dims(3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint8
# Input: dims: (3, 4, 5), contig: False, inplace: False, dtype: torch.qint8
Run: 0, Forward Execution Time (us) : 13.525
Run: 1, Forward Execution Time (us) : 13.668
Run: 2, Forward Execution Time (us) : 13.627

# Benchmarking PyTorch: functional.hardtanh
# Mode: Eager
# Name: functional.hardtanh_dims(3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint32
# Input: dims: (3, 4, 5), contig: False, inplace: False, dtype: torch.qint32
Run: 0, Forward Execution Time (us) : 13.832
Run: 1, Forward Execution Time (us) : 13.640
Run: 2, Forward Execution Time (us) : 13.661

# Benchmarking PyTorch: functional.hardtanh
# Mode: Eager
# Name: functional.hardtanh_dims(2,3,4,5)_contigFalse_inplaceFalse_dtypetorch.quint8
# Input: dims: (2, 3, 4, 5), contig: False, inplace: False, dtype: torch.quint8
Run: 0, Forward Execution Time (us) : 14.996
Run: 1, Forward Execution Time (us) : 14.966
Run: 2, Forward Execution Time (us) : 14.894

# Benchmarking PyTorch: functional.hardtanh
# Mode: Eager
# Name: functional.hardtanh_dims(2,3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint8
# Input: dims: (2, 3, 4, 5), contig: False, inplace: False, dtype: torch.qint8
Run: 0, Forward Execution Time (us) : 14.930
Run: 1, Forward Execution Time (us) : 14.994
Run: 2, Forward Execution Time (us) : 14.965

# Benchmarking PyTorch: functional.hardtanh
# Mode: Eager
# Name: functional.hardtanh_dims(2,3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint32
# Input: dims: (2, 3, 4, 5), contig: False, inplace: False, dtype: torch.qint32
Run: 0, Forward Execution Time (us) : 14.977
Run: 1, Forward Execution Time (us) : 15.029
Run: 2, Forward Execution Time (us) : 14.938

# Benchmarking PyTorch: functional.hardtanh
# Mode: Eager
# Name: functional.hardtanh_dims(512,512)_contigFalse_inplaceFalse_dtypetorch.quint8
# Input: dims: (512, 512), contig: False, inplace: False, dtype: torch.quint8
Run: 0, Forward Execution Time (us) : 350.510
Run: 1, Forward Execution Time (us) : 415.652
Run: 2, Forward Execution Time (us) : 415.542

# Benchmarking PyTorch: functional.hardtanh
# Mode: Eager
# Name: functional.hardtanh_dims(512,512)_contigFalse_inplaceFalse_dtypetorch.qint8
# Input: dims: (512, 512), contig: False, inplace: False, dtype: torch.qint8
Run: 0, Forward Execution Time (us) : 335.452
Run: 1, Forward Execution Time (us) : 396.196
Run: 2, Forward Execution Time (us) : 397.380

# Benchmarking PyTorch: functional.hardtanh
# Mode: Eager
# Name: functional.hardtanh_dims(512,512)_contigFalse_inplaceFalse_dtypetorch.qint32
# Input: dims: (512, 512), contig: False, inplace: False, dtype: torch.qint32
Run: 0, Forward Execution Time (us) : 814.098
Run: 1, Forward Execution Time (us) : 814.418
Run: 2, Forward Execution Time (us) : 814.262

# Benchmarking PyTorch: functional.hardtanh
# Mode: Eager
# Name: functional.hardtanh_dims(256,1024)_contigFalse_inplaceFalse_dtypetorch.quint8
# Input: dims: (256, 1024), contig: False, inplace: False, dtype: torch.quint8
Run: 0, Forward Execution Time (us) : 358.615
Run: 1, Forward Execution Time (us) : 424.052
Run: 2, Forward Execution Time (us) : 422.764

# Benchmarking PyTorch: functional.hardtanh
# Mode: Eager
# Name: functional.hardtanh_dims(256,1024)_contigFalse_inplaceFalse_dtypetorch.qint8
# Input: dims: (256, 1024), contig: False, inplace: False, dtype: torch.qint8
Run: 0, Forward Execution Time (us) : 336.136
Run: 1, Forward Execution Time (us) : 399.885
Run: 2, Forward Execution Time (us) : 397.938

# Benchmarking PyTorch: functional.hardtanh
# Mode: Eager
# Name: functional.hardtanh_dims(256,1024)_contigFalse_inplaceFalse_dtypetorch.qint32
# Input: dims: (256, 1024), contig: False, inplace: False, dtype: torch.qint32
Run: 0, Forward Execution Time (us) : 891.515
Run: 1, Forward Execution Time (us) : 892.597
Run: 2, Forward Execution Time (us) : 892.246

# Benchmarking PyTorch: functional.hardsigmoid
# Mode: Eager
# Name: functional.hardsigmoid_dims(3,4,5)_contigFalse_inplaceFalse_dtypetorch.quint8
# Input: dims: (3, 4, 5), contig: False, inplace: False, dtype: torch.quint8
Run: 0, Forward Execution Time (us) : 11.671
Run: 1, Forward Execution Time (us) : 13.299
Run: 2, Forward Execution Time (us) : 13.262

# Benchmarking PyTorch: functional.hardsigmoid
# Mode: Eager
# Name: functional.hardsigmoid_dims(3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint8
# Input: dims: (3, 4, 5), contig: False, inplace: False, dtype: torch.qint8
Run: 0, Forward Execution Time (us) : 13.088
Run: 1, Forward Execution Time (us) : 13.197
Run: 2, Forward Execution Time (us) : 13.133

# Benchmarking PyTorch: functional.hardsigmoid
# Mode: Eager
# Name: functional.hardsigmoid_dims(3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint32
# Input: dims: (3, 4, 5), contig: False, inplace: False, dtype: torch.qint32
Run: 0, Forward Execution Time (us) : 13.247
Run: 1, Forward Execution Time (us) : 13.175
Run: 2, Forward Execution Time (us) : 13.203

# Benchmarking PyTorch: functional.hardsigmoid
# Mode: Eager
# Name: functional.hardsigmoid_dims(2,3,4,5)_contigFalse_inplaceFalse_dtypetorch.quint8
# Input: dims: (2, 3, 4, 5), contig: False, inplace: False, dtype: torch.quint8
Run: 0, Forward Execution Time (us) : 16.747
Run: 1, Forward Execution Time (us) : 16.784
Run: 2, Forward Execution Time (us) : 16.754

# Benchmarking PyTorch: functional.hardsigmoid
# Mode: Eager
# Name: functional.hardsigmoid_dims(2,3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint8
# Input: dims: (2, 3, 4, 5), contig: False, inplace: False, dtype: torch.qint8
Run: 0, Forward Execution Time (us) : 16.757
Run: 1, Forward Execution Time (us) : 16.815
Run: 2, Forward Execution Time (us) : 16.803

# Benchmarking PyTorch: functional.hardsigmoid
# Mode: Eager
# Name: functional.hardsigmoid_dims(2,3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint32
# Input: dims: (2, 3, 4, 5), contig: False, inplace: False, dtype: torch.qint32
Run: 0, Forward Execution Time (us) : 16.824
Run: 1, Forward Execution Time (us) : 16.800
Run: 2, Forward Execution Time (us) : 16.766

# Benchmarking PyTorch: functional.hardsigmoid
# Mode: Eager
# Name: functional.hardsigmoid_dims(512,512)_contigFalse_inplaceFalse_dtypetorch.quint8
# Input: dims: (512, 512), contig: False, inplace: False, dtype: torch.quint8
Run: 0, Forward Execution Time (us) : 9363.392
Run: 1, Forward Execution Time (us) : 9373.403
Run: 2, Forward Execution Time (us) : 9383.789

# Benchmarking PyTorch: functional.hardsigmoid
# Mode: Eager
# Name: functional.hardsigmoid_dims(512,512)_contigFalse_inplaceFalse_dtypetorch.qint8
# Input: dims: (512, 512), contig: False, inplace: False, dtype: torch.qint8
Run: 0, Forward Execution Time (us) : 9610.038
Run: 1, Forward Execution Time (us) : 9616.089
Run: 2, Forward Execution Time (us) : 9611.699

# Benchmarking PyTorch: functional.hardsigmoid
# Mode: Eager
# Name: functional.hardsigmoid_dims(512,512)_contigFalse_inplaceFalse_dtypetorch.qint32
# Input: dims: (512, 512), contig: False, inplace: False, dtype: torch.qint32
Run: 0, Forward Execution Time (us) : 10362.092
Run: 1, Forward Execution Time (us) : 10363.898
Run: 2, Forward Execution Time (us) : 10369.594

# Benchmarking PyTorch: functional.hardsigmoid
# Mode: Eager
# Name: functional.hardsigmoid_dims(256,1024)_contigFalse_inplaceFalse_dtypetorch.quint8
# Input: dims: (256, 1024), contig: False, inplace: False, dtype: torch.quint8
Run: 0, Forward Execution Time (us) : 9376.256
Run: 1, Forward Execution Time (us) : 9377.272
Run: 2, Forward Execution Time (us) : 9378.969

# Benchmarking PyTorch: functional.hardsigmoid
# Mode: Eager
# Name: functional.hardsigmoid_dims(256,1024)_contigFalse_inplaceFalse_dtypetorch.qint8
# Input: dims: (256, 1024), contig: False, inplace: False, dtype: torch.qint8
Run: 0, Forward Execution Time (us) : 9616.396
Run: 1, Forward Execution Time (us) : 9610.864
Run: 2, Forward Execution Time (us) : 9614.064

# Benchmarking PyTorch: functional.hardsigmoid
# Mode: Eager
# Name: functional.hardsigmoid_dims(256,1024)_contigFalse_inplaceFalse_dtypetorch.qint32
# Input: dims: (256, 1024), contig: False, inplace: False, dtype: torch.qint32
Run: 0, Forward Execution Time (us) : 10468.648
Run: 1, Forward Execution Time (us) : 10483.661
Run: 2, Forward Execution Time (us) : 10486.048

# Benchmarking PyTorch: functional.leaky_relu
# Mode: Eager
# Name: functional.leaky_relu_dims(3,4,5)_contigFalse_inplaceFalse_dtypetorch.quint8
# Input: dims: (3, 4, 5), contig: False, inplace: False, dtype: torch.quint8
Run: 0, Forward Execution Time (us) : 18.330
Run: 1, Forward Execution Time (us) : 20.962
Run: 2, Forward Execution Time (us) : 20.987

# Benchmarking PyTorch: functional.leaky_relu
# Mode: Eager
# Name: functional.leaky_relu_dims(3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint8
# Input: dims: (3, 4, 5), contig: False, inplace: False, dtype: torch.qint8
Run: 0, Forward Execution Time (us) : 20.788
Run: 1, Forward Execution Time (us) : 20.876
Run: 2, Forward Execution Time (us) : 20.887

# Benchmarking PyTorch: functional.leaky_relu
# Mode: Eager
# Name: functional.leaky_relu_dims(3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint32
# Input: dims: (3, 4, 5), contig: False, inplace: False, dtype: torch.qint32
Run: 0, Forward Execution Time (us) : 20.702
Run: 1, Forward Execution Time (us) : 20.678
Run: 2, Forward Execution Time (us) : 20.665

# Benchmarking PyTorch: functional.leaky_relu
# Mode: Eager
# Name: functional.leaky_relu_dims(2,3,4,5)_contigFalse_inplaceFalse_dtypetorch.quint8
# Input: dims: (2, 3, 4, 5), contig: False, inplace: False, dtype: torch.quint8
Run: 0, Forward Execution Time (us) : 22.200
Run: 1, Forward Execution Time (us) : 22.226
Run: 2, Forward Execution Time (us) : 22.268

# Benchmarking PyTorch: functional.leaky_relu
# Mode: Eager
# Name: functional.leaky_relu_dims(2,3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint8
# Input: dims: (2, 3, 4, 5), contig: False, inplace: False, dtype: torch.qint8
Run: 0, Forward Execution Time (us) : 21.848
Run: 1, Forward Execution Time (us) : 22.012
Run: 2, Forward Execution Time (us) : 21.947

# Benchmarking PyTorch: functional.leaky_relu
# Mode: Eager
# Name: functional.leaky_relu_dims(2,3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint32
# Input: dims: (2, 3, 4, 5), contig: False, inplace: False, dtype: torch.qint32
Run: 0, Forward Execution Time (us) : 23.821
Run: 1, Forward Execution Time (us) : 23.758
Run: 2, Forward Execution Time (us) : 23.738

# Benchmarking PyTorch: functional.leaky_relu
# Mode: Eager
# Name: functional.leaky_relu_dims(512,512)_contigFalse_inplaceFalse_dtypetorch.quint8
# Input: dims: (512, 512), contig: False, inplace: False, dtype: torch.quint8
Run: 0, Forward Execution Time (us) : 343.128
Run: 1, Forward Execution Time (us) : 343.356
Run: 2, Forward Execution Time (us) : 343.322

# Benchmarking PyTorch: functional.leaky_relu
# Mode: Eager
# Name: functional.leaky_relu_dims(512,512)_contigFalse_inplaceFalse_dtypetorch.qint8
# Input: dims: (512, 512), contig: False, inplace: False, dtype: torch.qint8
Run: 0, Forward Execution Time (us) : 359.486
Run: 1, Forward Execution Time (us) : 359.674
Run: 2, Forward Execution Time (us) : 360.222

# Benchmarking PyTorch: functional.leaky_relu
# Mode: Eager
# Name: functional.leaky_relu_dims(512,512)_contigFalse_inplaceFalse_dtypetorch.qint32
# Input: dims: (512, 512), contig: False, inplace: False, dtype: torch.qint32
Run: 0, Forward Execution Time (us) : 7245.948
Run: 1, Forward Execution Time (us) : 7257.585
Run: 2, Forward Execution Time (us) : 7251.120

# Benchmarking PyTorch: functional.leaky_relu
# Mode: Eager
# Name: functional.leaky_relu_dims(256,1024)_contigFalse_inplaceFalse_dtypetorch.quint8
# Input: dims: (256, 1024), contig: False, inplace: False, dtype: torch.quint8
Run: 0, Forward Execution Time (us) : 339.218
Run: 1, Forward Execution Time (us) : 339.455
Run: 2, Forward Execution Time (us) : 339.400

# Benchmarking PyTorch: functional.leaky_relu
# Mode: Eager
# Name: functional.leaky_relu_dims(256,1024)_contigFalse_inplaceFalse_dtypetorch.qint8
# Input: dims: (256, 1024), contig: False, inplace: False, dtype: torch.qint8
Run: 0, Forward Execution Time (us) : 359.518
Run: 1, Forward Execution Time (us) : 359.076
Run: 2, Forward Execution Time (us) : 360.421

# Benchmarking PyTorch: functional.leaky_relu
# Mode: Eager
# Name: functional.leaky_relu_dims(256,1024)_contigFalse_inplaceFalse_dtypetorch.qint32
# Input: dims: (256, 1024), contig: False, inplace: False, dtype: torch.qint32
Run: 0, Forward Execution Time (us) : 7267.634
Run: 1, Forward Execution Time (us) : 7270.735
Run: 2, Forward Execution Time (us) : 7269.488

# Benchmarking PyTorch: functional.sigmoid
# Mode: Eager
# Name: functional.sigmoid_dims(3,4,5)_contigFalse_inplaceFalse_dtypetorch.quint8
# Input: dims: (3, 4, 5), contig: False, inplace: False, dtype: torch.quint8
Run: 0, Forward Execution Time (us) : 11.196
Run: 1, Forward Execution Time (us) : 13.367
Run: 2, Forward Execution Time (us) : 13.234

# Benchmarking PyTorch: functional.sigmoid
# Mode: Eager
# Name: functional.sigmoid_dims(3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint8
# Input: dims: (3, 4, 5), contig: False, inplace: False, dtype: torch.qint8
Run: 0, Forward Execution Time (us) : 13.441
Run: 1, Forward Execution Time (us) : 13.403
Run: 2, Forward Execution Time (us) : 13.394

# Benchmarking PyTorch: functional.sigmoid
# Mode: Eager
# Name: functional.sigmoid_dims(3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint32
# Input: dims: (3, 4, 5), contig: False, inplace: False, dtype: torch.qint32
Run: 0, Forward Execution Time (us) : 13.467
Run: 1, Forward Execution Time (us) : 13.565
Run: 2, Forward Execution Time (us) : 13.603

# Benchmarking PyTorch: functional.sigmoid
# Mode: Eager
# Name: functional.sigmoid_dims(2,3,4,5)_contigFalse_inplaceFalse_dtypetorch.quint8
# Input: dims: (2, 3, 4, 5), contig: False, inplace: False, dtype: torch.quint8
Run: 0, Forward Execution Time (us) : 18.006
Run: 1, Forward Execution Time (us) : 18.155
Run: 2, Forward Execution Time (us) : 18.150

# Benchmarking PyTorch: functional.sigmoid
# Mode: Eager
# Name: functional.sigmoid_dims(2,3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint8
# Input: dims: (2, 3, 4, 5), contig: False, inplace: False, dtype: torch.qint8
Run: 0, Forward Execution Time (us) : 18.359
Run: 1, Forward Execution Time (us) : 18.480
Run: 2, Forward Execution Time (us) : 18.469

# Benchmarking PyTorch: functional.sigmoid
# Mode: Eager
# Name: functional.sigmoid_dims(2,3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint32
# Input: dims: (2, 3, 4, 5), contig: False, inplace: False, dtype: torch.qint32
Run: 0, Forward Execution Time (us) : 18.205
Run: 1, Forward Execution Time (us) : 18.359
Run: 2, Forward Execution Time (us) : 18.308

# Benchmarking PyTorch: functional.sigmoid
# Mode: Eager
# Name: functional.sigmoid_dims(512,512)_contigFalse_inplaceFalse_dtypetorch.quint8
# Input: dims: (512, 512), contig: False, inplace: False, dtype: torch.quint8
Run: 0, Forward Execution Time (us) : 13586.148
Run: 1, Forward Execution Time (us) : 13593.486
Run: 2, Forward Execution Time (us) : 13590.552

# Benchmarking PyTorch: functional.sigmoid
# Mode: Eager
# Name: functional.sigmoid_dims(512,512)_contigFalse_inplaceFalse_dtypetorch.qint8
# Input: dims: (512, 512), contig: False, inplace: False, dtype: torch.qint8
Run: 0, Forward Execution Time (us) : 14245.369
Run: 1, Forward Execution Time (us) : 14248.718
Run: 2, Forward Execution Time (us) : 14244.230

# Benchmarking PyTorch: functional.sigmoid
# Mode: Eager
# Name: functional.sigmoid_dims(512,512)_contigFalse_inplaceFalse_dtypetorch.qint32
# Input: dims: (512, 512), contig: False, inplace: False, dtype: torch.qint32
Run: 0, Forward Execution Time (us) : 14624.675
Run: 1, Forward Execution Time (us) : 14620.720
Run: 2, Forward Execution Time (us) : 14622.384

# Benchmarking PyTorch: functional.sigmoid
# Mode: Eager
# Name: functional.sigmoid_dims(256,1024)_contigFalse_inplaceFalse_dtypetorch.quint8
# Input: dims: (256, 1024), contig: False, inplace: False, dtype: torch.quint8
Run: 0, Forward Execution Time (us) : 13578.743
Run: 1, Forward Execution Time (us) : 13609.451
Run: 2, Forward Execution Time (us) : 13574.507

# Benchmarking PyTorch: functional.sigmoid
# Mode: Eager
# Name: functional.sigmoid_dims(256,1024)_contigFalse_inplaceFalse_dtypetorch.qint8
# Input: dims: (256, 1024), contig: False, inplace: False, dtype: torch.qint8
Run: 0, Forward Execution Time (us) : 14246.994
Run: 1, Forward Execution Time (us) : 14244.170
Run: 2, Forward Execution Time (us) : 14240.354

# Benchmarking PyTorch: functional.sigmoid
# Mode: Eager
# Name: functional.sigmoid_dims(256,1024)_contigFalse_inplaceFalse_dtypetorch.qint32
# Input: dims: (256, 1024), contig: False, inplace: False, dtype: torch.qint32
Run: 0, Forward Execution Time (us) : 14710.362
Run: 1, Forward Execution Time (us) : 14719.509
Run: 2, Forward Execution Time (us) : 14726.442

# Benchmarking PyTorch: functional.tanh
# Mode: Eager
# Name: functional.tanh_dims(3,4,5)_contigFalse_inplaceFalse_dtypetorch.quint8
# Input: dims: (3, 4, 5), contig: False, inplace: False, dtype: torch.quint8
Run: 0, Forward Execution Time (us) : 11.307
Run: 1, Forward Execution Time (us) : 12.847
Run: 2, Forward Execution Time (us) : 12.733

# Benchmarking PyTorch: functional.tanh
# Mode: Eager
# Name: functional.tanh_dims(3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint8
# Input: dims: (3, 4, 5), contig: False, inplace: False, dtype: torch.qint8
Run: 0, Forward Execution Time (us) : 12.830
Run: 1, Forward Execution Time (us) : 12.812
Run: 2, Forward Execution Time (us) : 12.840

# Benchmarking PyTorch: functional.tanh
# Mode: Eager
# Name: functional.tanh_dims(3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint32
# Input: dims: (3, 4, 5), contig: False, inplace: False, dtype: torch.qint32
Run: 0, Forward Execution Time (us) : 12.892
Run: 1, Forward Execution Time (us) : 12.873
Run: 2, Forward Execution Time (us) : 12.814

# Benchmarking PyTorch: functional.tanh
# Mode: Eager
# Name: functional.tanh_dims(2,3,4,5)_contigFalse_inplaceFalse_dtypetorch.quint8
# Input: dims: (2, 3, 4, 5), contig: False, inplace: False, dtype: torch.quint8
Run: 0, Forward Execution Time (us) : 17.143
Run: 1, Forward Execution Time (us) : 17.140
Run: 2, Forward Execution Time (us) : 17.107

# Benchmarking PyTorch: functional.tanh
# Mode: Eager
# Name: functional.tanh_dims(2,3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint8
# Input: dims: (2, 3, 4, 5), contig: False, inplace: False, dtype: torch.qint8
Run: 0, Forward Execution Time (us) : 17.040
Run: 1, Forward Execution Time (us) : 17.092
Run: 2, Forward Execution Time (us) : 17.100

# Benchmarking PyTorch: functional.tanh
# Mode: Eager
# Name: functional.tanh_dims(2,3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint32
# Input: dims: (2, 3, 4, 5), contig: False, inplace: False, dtype: torch.qint32
Run: 0, Forward Execution Time (us) : 17.114
Run: 1, Forward Execution Time (us) : 17.161
Run: 2, Forward Execution Time (us) : 17.184

# Benchmarking PyTorch: functional.tanh
# Mode: Eager
# Name: functional.tanh_dims(512,512)_contigFalse_inplaceFalse_dtypetorch.quint8
# Input: dims: (512, 512), contig: False, inplace: False, dtype: torch.quint8
Run: 0, Forward Execution Time (us) : 12000.615
Run: 1, Forward Execution Time (us) : 11973.714
Run: 2, Forward Execution Time (us) : 11973.572

# Benchmarking PyTorch: functional.tanh
# Mode: Eager
# Name: functional.tanh_dims(512,512)_contigFalse_inplaceFalse_dtypetorch.qint8
# Input: dims: (512, 512), contig: False, inplace: False, dtype: torch.qint8
Run: 0, Forward Execution Time (us) : 12459.296
Run: 1, Forward Execution Time (us) : 12456.884
Run: 2, Forward Execution Time (us) : 12456.259

# Benchmarking PyTorch: functional.tanh
# Mode: Eager
# Name: functional.tanh_dims(512,512)_contigFalse_inplaceFalse_dtypetorch.qint32
# Input: dims: (512, 512), contig: False, inplace: False, dtype: torch.qint32
Run: 0, Forward Execution Time (us) : 12976.906
Run: 1, Forward Execution Time (us) : 12985.788
Run: 2, Forward Execution Time (us) : 12979.720

# Benchmarking PyTorch: functional.tanh
# Mode: Eager
# Name: functional.tanh_dims(256,1024)_contigFalse_inplaceFalse_dtypetorch.quint8
# Input: dims: (256, 1024), contig: False, inplace: False, dtype: torch.quint8
Run: 0, Forward Execution Time (us) : 11983.505
Run: 1, Forward Execution Time (us) : 11988.550
Run: 2, Forward Execution Time (us) : 12001.318

# Benchmarking PyTorch: functional.tanh
# Mode: Eager
# Name: functional.tanh_dims(256,1024)_contigFalse_inplaceFalse_dtypetorch.qint8
# Input: dims: (256, 1024), contig: False, inplace: False, dtype: torch.qint8
Run: 0, Forward Execution Time (us) : 12458.004
Run: 1, Forward Execution Time (us) : 12466.532
Run: 2, Forward Execution Time (us) : 12461.710

# Benchmarking PyTorch: functional.tanh
# Mode: Eager
# Name: functional.tanh_dims(256,1024)_contigFalse_inplaceFalse_dtypetorch.qint32
# Input: dims: (256, 1024), contig: False, inplace: False, dtype: torch.qint32
Run: 0, Forward Execution Time (us) : 13103.849
Run: 1, Forward Execution Time (us) : 13095.439
Run: 2, Forward Execution Time (us) : 13096.362

# Benchmarking PyTorch: functional.hardswish
# Mode: Eager
# Name: functional.hardswish_dims(3,4,5)_contigFalse_inplaceFalse_dtypetorch.quint8
# Input: dims: (3, 4, 5), contig: False, inplace: False, dtype: torch.quint8
Run: 0, Forward Execution Time (us) : 15.506
Run: 1, Forward Execution Time (us) : 17.778
Run: 2, Forward Execution Time (us) : 17.686

# Benchmarking PyTorch: functional.hardswish
# Mode: Eager
# Name: functional.hardswish_dims(3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint8
# Input: dims: (3, 4, 5), contig: False, inplace: False, dtype: torch.qint8
Run: 0, Forward Execution Time (us) : 17.653
Run: 1, Forward Execution Time (us) : 17.774
Run: 2, Forward Execution Time (us) : 17.716

# Benchmarking PyTorch: functional.hardswish
# Mode: Eager
# Name: functional.hardswish_dims(3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint32
# Input: dims: (3, 4, 5), contig: False, inplace: False, dtype: torch.qint32
Run: 0, Forward Execution Time (us) : 17.850
Run: 1, Forward Execution Time (us) : 17.766
Run: 2, Forward Execution Time (us) : 17.752

# Benchmarking PyTorch: functional.hardswish
# Mode: Eager
# Name: functional.hardswish_dims(2,3,4,5)_contigFalse_inplaceFalse_dtypetorch.quint8
# Input: dims: (2, 3, 4, 5), contig: False, inplace: False, dtype: torch.quint8
Run: 0, Forward Execution Time (us) : 21.223
Run: 1, Forward Execution Time (us) : 21.427
Run: 2, Forward Execution Time (us) : 21.349

# Benchmarking PyTorch: functional.hardswish
# Mode: Eager
# Name: functional.hardswish_dims(2,3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint8
# Input: dims: (2, 3, 4, 5), contig: False, inplace: False, dtype: torch.qint8
Run: 0, Forward Execution Time (us) : 21.271
Run: 1, Forward Execution Time (us) : 21.450
Run: 2, Forward Execution Time (us) : 21.394

# Benchmarking PyTorch: functional.hardswish
# Mode: Eager
# Name: functional.hardswish_dims(2,3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint32
# Input: dims: (2, 3, 4, 5), contig: False, inplace: False, dtype: torch.qint32
Run: 0, Forward Execution Time (us) : 21.328
Run: 1, Forward Execution Time (us) : 21.440
Run: 2, Forward Execution Time (us) : 21.431

# Benchmarking PyTorch: functional.hardswish
# Mode: Eager
# Name: functional.hardswish_dims(512,512)_contigFalse_inplaceFalse_dtypetorch.quint8
# Input: dims: (512, 512), contig: False, inplace: False, dtype: torch.quint8
Run: 0, Forward Execution Time (us) : 9365.076
Run: 1, Forward Execution Time (us) : 9360.049
Run: 2, Forward Execution Time (us) : 9365.275

# Benchmarking PyTorch: functional.hardswish
# Mode: Eager
# Name: functional.hardswish_dims(512,512)_contigFalse_inplaceFalse_dtypetorch.qint8
# Input: dims: (512, 512), contig: False, inplace: False, dtype: torch.qint8
Run: 0, Forward Execution Time (us) : 9483.197
Run: 1, Forward Execution Time (us) : 9476.037
Run: 2, Forward Execution Time (us) : 9482.740

# Benchmarking PyTorch: functional.hardswish
# Mode: Eager
# Name: functional.hardswish_dims(512,512)_contigFalse_inplaceFalse_dtypetorch.qint32
# Input: dims: (512, 512), contig: False, inplace: False, dtype: torch.qint32
Run: 0, Forward Execution Time (us) : 10183.999
Run: 1, Forward Execution Time (us) : 10185.157
Run: 2, Forward Execution Time (us) : 10186.631

# Benchmarking PyTorch: functional.hardswish
# Mode: Eager
# Name: functional.hardswish_dims(256,1024)_contigFalse_inplaceFalse_dtypetorch.quint8
# Input: dims: (256, 1024), contig: False, inplace: False, dtype: torch.quint8
Run: 0, Forward Execution Time (us) : 9343.066
Run: 1, Forward Execution Time (us) : 9336.676
Run: 2, Forward Execution Time (us) : 9322.306

# Benchmarking PyTorch: functional.hardswish
# Mode: Eager
# Name: functional.hardswish_dims(256,1024)_contigFalse_inplaceFalse_dtypetorch.qint8
# Input: dims: (256, 1024), contig: False, inplace: False, dtype: torch.qint8
Run: 0, Forward Execution Time (us) : 9481.599
Run: 1, Forward Execution Time (us) : 9478.397
Run: 2, Forward Execution Time (us) : 9475.336

# Benchmarking PyTorch: functional.hardswish
# Mode: Eager
# Name: functional.hardswish_dims(256,1024)_contigFalse_inplaceFalse_dtypetorch.qint32
# Input: dims: (256, 1024), contig: False, inplace: False, dtype: torch.qint32
Run: 0, Forward Execution Time (us) : 10325.500
Run: 1, Forward Execution Time (us) : 10336.940
Run: 2, Forward Execution Time (us) : 10326.075

# Benchmarking PyTorch: functional.elu
# Mode: Eager
# Name: functional.elu_dims(3,4,5)_contigFalse_inplaceFalse_dtypetorch.quint8
# Input: dims: (3, 4, 5), contig: False, inplace: False, dtype: torch.quint8
Run: 0, Forward Execution Time (us) : 15.980
Run: 1, Forward Execution Time (us) : 18.913
Run: 2, Forward Execution Time (us) : 18.888

# Benchmarking PyTorch: functional.elu
# Mode: Eager
# Name: functional.elu_dims(3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint8
# Input: dims: (3, 4, 5), contig: False, inplace: False, dtype: torch.qint8
Run: 0, Forward Execution Time (us) : 19.482
Run: 1, Forward Execution Time (us) : 19.612
Run: 2, Forward Execution Time (us) : 19.597

# Benchmarking PyTorch: functional.elu
# Mode: Eager
# Name: functional.elu_dims(3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint32
# Input: dims: (3, 4, 5), contig: False, inplace: False, dtype: torch.qint32
Run: 0, Forward Execution Time (us) : 19.535
Run: 1, Forward Execution Time (us) : 19.642
Run: 2, Forward Execution Time (us) : 19.563

# Benchmarking PyTorch: functional.elu
# Mode: Eager
# Name: functional.elu_dims(2,3,4,5)_contigFalse_inplaceFalse_dtypetorch.quint8
# Input: dims: (2, 3, 4, 5), contig: False, inplace: False, dtype: torch.quint8
Run: 0, Forward Execution Time (us) : 22.242
Run: 1, Forward Execution Time (us) : 22.298
Run: 2, Forward Execution Time (us) : 22.272

# Benchmarking PyTorch: functional.elu
# Mode: Eager
# Name: functional.elu_dims(2,3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint8
# Input: dims: (2, 3, 4, 5), contig: False, inplace: False, dtype: torch.qint8
Run: 0, Forward Execution Time (us) : 23.788
Run: 1, Forward Execution Time (us) : 23.907
Run: 2, Forward Execution Time (us) : 23.900

# Benchmarking PyTorch: functional.elu
# Mode: Eager
# Name: functional.elu_dims(2,3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint32
# Input: dims: (2, 3, 4, 5), contig: False, inplace: False, dtype: torch.qint32
Run: 0, Forward Execution Time (us) : 23.779
Run: 1, Forward Execution Time (us) : 23.897
Run: 2, Forward Execution Time (us) : 23.878

# Benchmarking PyTorch: functional.elu
# Mode: Eager
# Name: functional.elu_dims(512,512)_contigFalse_inplaceFalse_dtypetorch.quint8
# Input: dims: (512, 512), contig: False, inplace: False, dtype: torch.quint8
Run: 0, Forward Execution Time (us) : 8357.954
Run: 1, Forward Execution Time (us) : 8350.689
Run: 2, Forward Execution Time (us) : 8351.976

# Benchmarking PyTorch: functional.elu
# Mode: Eager
# Name: functional.elu_dims(512,512)_contigFalse_inplaceFalse_dtypetorch.qint8
# Input: dims: (512, 512), contig: False, inplace: False, dtype: torch.qint8
Run: 0, Forward Execution Time (us) : 12132.777
Run: 1, Forward Execution Time (us) : 12137.600
Run: 2, Forward Execution Time (us) : 12112.928

# Benchmarking PyTorch: functional.elu
# Mode: Eager
# Name: functional.elu_dims(512,512)_contigFalse_inplaceFalse_dtypetorch.qint32
# Input: dims: (512, 512), contig: False, inplace: False, dtype: torch.qint32
Run: 0, Forward Execution Time (us) : 12878.787
Run: 1, Forward Execution Time (us) : 12874.562
Run: 2, Forward Execution Time (us) : 12879.270

# Benchmarking PyTorch: functional.elu
# Mode: Eager
# Name: functional.elu_dims(256,1024)_contigFalse_inplaceFalse_dtypetorch.quint8
# Input: dims: (256, 1024), contig: False, inplace: False, dtype: torch.quint8
Run: 0, Forward Execution Time (us) : 8355.053
Run: 1, Forward Execution Time (us) : 8361.735
Run: 2, Forward Execution Time (us) : 8381.485

# Benchmarking PyTorch: functional.elu
# Mode: Eager
# Name: functional.elu_dims(256,1024)_contigFalse_inplaceFalse_dtypetorch.qint8
# Input: dims: (256, 1024), contig: False, inplace: False, dtype: torch.qint8
Run: 0, Forward Execution Time (us) : 12131.272
Run: 1, Forward Execution Time (us) : 12130.604
Run: 2, Forward Execution Time (us) : 12130.524

# Benchmarking PyTorch: functional.elu
# Mode: Eager
# Name: functional.elu_dims(256,1024)_contigFalse_inplaceFalse_dtypetorch.qint32
# Input: dims: (256, 1024), contig: False, inplace: False, dtype: torch.qint32
Run: 0, Forward Execution Time (us) : 13017.102
Run: 1, Forward Execution Time (us) : 13007.668
Run: 2, Forward Execution Time (us) : 13171.828

# Benchmarking PyTorch: functional.celu
# Mode: Eager
# Name: functional.celu_dims(3,4,5)_contigFalse_inplaceFalse_dtypetorch.quint8
# Input: dims: (3, 4, 5), contig: False, inplace: False, dtype: torch.quint8
Run: 0, Forward Execution Time (us) : 15.808
Run: 1, Forward Execution Time (us) : 18.227
Run: 2, Forward Execution Time (us) : 18.243

# Benchmarking PyTorch: functional.celu
# Mode: Eager
# Name: functional.celu_dims(3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint8
# Input: dims: (3, 4, 5), contig: False, inplace: False, dtype: torch.qint8
Run: 0, Forward Execution Time (us) : 18.984
Run: 1, Forward Execution Time (us) : 19.075
Run: 2, Forward Execution Time (us) : 19.108

# Benchmarking PyTorch: functional.celu
# Mode: Eager
# Name: functional.celu_dims(3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint32
# Input: dims: (3, 4, 5), contig: False, inplace: False, dtype: torch.qint32
Run: 0, Forward Execution Time (us) : 19.073
Run: 1, Forward Execution Time (us) : 18.993
Run: 2, Forward Execution Time (us) : 19.071

# Benchmarking PyTorch: functional.celu
# Mode: Eager
# Name: functional.celu_dims(2,3,4,5)_contigFalse_inplaceFalse_dtypetorch.quint8
# Input: dims: (2, 3, 4, 5), contig: False, inplace: False, dtype: torch.quint8
Run: 0, Forward Execution Time (us) : 21.592
Run: 1, Forward Execution Time (us) : 21.703
Run: 2, Forward Execution Time (us) : 21.655

# Benchmarking PyTorch: functional.celu
# Mode: Eager
# Name: functional.celu_dims(2,3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint8
# Input: dims: (2, 3, 4, 5), contig: False, inplace: False, dtype: torch.qint8
Run: 0, Forward Execution Time (us) : 23.026
Run: 1, Forward Execution Time (us) : 23.293
Run: 2, Forward Execution Time (us) : 23.390

# Benchmarking PyTorch: functional.celu
# Mode: Eager
# Name: functional.celu_dims(2,3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint32
# Input: dims: (2, 3, 4, 5), contig: False, inplace: False, dtype: torch.qint32
Run: 0, Forward Execution Time (us) : 23.319
Run: 1, Forward Execution Time (us) : 23.451
Run: 2, Forward Execution Time (us) : 23.300

# Benchmarking PyTorch: functional.celu
# Mode: Eager
# Name: functional.celu_dims(512,512)_contigFalse_inplaceFalse_dtypetorch.quint8
# Input: dims: (512, 512), contig: False, inplace: False, dtype: torch.quint8
Run: 0, Forward Execution Time (us) : 8372.069
Run: 1, Forward Execution Time (us) : 8369.154
Run: 2, Forward Execution Time (us) : 8369.167

# Benchmarking PyTorch: functional.celu
# Mode: Eager
# Name: functional.celu_dims(512,512)_contigFalse_inplaceFalse_dtypetorch.qint8
# Input: dims: (512, 512), contig: False, inplace: False, dtype: torch.qint8
Run: 0, Forward Execution Time (us) : 12127.929
Run: 1, Forward Execution Time (us) : 12178.464
Run: 2, Forward Execution Time (us) : 12121.991

# Benchmarking PyTorch: functional.celu
# Mode: Eager
# Name: functional.celu_dims(512,512)_contigFalse_inplaceFalse_dtypetorch.qint32
# Input: dims: (512, 512), contig: False, inplace: False, dtype: torch.qint32
Run: 0, Forward Execution Time (us) : 12771.241
Run: 1, Forward Execution Time (us) : 12757.685
Run: 2, Forward Execution Time (us) : 12757.518

# Benchmarking PyTorch: functional.celu
# Mode: Eager
# Name: functional.celu_dims(256,1024)_contigFalse_inplaceFalse_dtypetorch.quint8
# Input: dims: (256, 1024), contig: False, inplace: False, dtype: torch.quint8
Run: 0, Forward Execution Time (us) : 8382.465
Run: 1, Forward Execution Time (us) : 8375.945
Run: 2, Forward Execution Time (us) : 8380.937

# Benchmarking PyTorch: functional.celu
# Mode: Eager
# Name: functional.celu_dims(256,1024)_contigFalse_inplaceFalse_dtypetorch.qint8
# Input: dims: (256, 1024), contig: False, inplace: False, dtype: torch.qint8
Run: 0, Forward Execution Time (us) : 12142.667
Run: 1, Forward Execution Time (us) : 12139.288
Run: 2, Forward Execution Time (us) : 12140.570

# Benchmarking PyTorch: functional.celu
# Mode: Eager
# Name: functional.celu_dims(256,1024)_contigFalse_inplaceFalse_dtypetorch.qint32
# Input: dims: (256, 1024), contig: False, inplace: False, dtype: torch.qint32
Run: 0, Forward Execution Time (us) : 12882.778
Run: 1, Forward Execution Time (us) : 12882.616
Run: 2, Forward Execution Time (us) : 12906.522

+ python -m pt.qinterpolate_test --num-runs 3 --iterations 100 --warmup-iterations 10
not use dipu
# ----------------------------------------
# PyTorch/Caffe2 Operator Micro-benchmarks
# ----------------------------------------
# Tag : short

# Benchmarking PyTorch: q_interpolate
# Mode: Eager
# Name: q_interpolate_M32_N32_K32_dtypetorch.quint8_modenearest_scale0.5_contigTrue
# Input: M: 32, N: 32, K: 32, dtype: torch.quint8, mode: nearest, scale: 0.5, contig: True
Run: 0, Forward Execution Time (us) : 12.730
Run: 1, Forward Execution Time (us) : 14.689
Run: 2, Forward Execution Time (us) : 13.798

# Benchmarking PyTorch: q_interpolate
# Mode: Eager
# Name: q_interpolate_M32_N32_K32_dtypetorch.quint8_modebilinear_scale0.5_contigTrue
# Input: M: 32, N: 32, K: 32, dtype: torch.quint8, mode: bilinear, scale: 0.5, contig: True
Run: 0, Forward Execution Time (us) : 26.208
Run: 1, Forward Execution Time (us) : 25.395
Run: 2, Forward Execution Time (us) : 25.341

# Benchmarking PyTorch: q_interpolate
# Mode: Eager
# Name: q_interpolate_M32_N32_K32_dtypetorch.quint8_modenearest_scale2.0_contigTrue
# Input: M: 32, N: 32, K: 32, dtype: torch.quint8, mode: nearest, scale: 2.0, contig: True
Run: 0, Forward Execution Time (us) : 13.825
Run: 1, Forward Execution Time (us) : 14.351
Run: 2, Forward Execution Time (us) : 14.742

# Benchmarking PyTorch: q_interpolate
# Mode: Eager
# Name: q_interpolate_M32_N32_K32_dtypetorch.quint8_modebilinear_scale2.0_contigTrue
# Input: M: 32, N: 32, K: 32, dtype: torch.quint8, mode: bilinear, scale: 2.0, contig: True
Run: 0, Forward Execution Time (us) : 25.273
Run: 1, Forward Execution Time (us) : 25.347
Run: 2, Forward Execution Time (us) : 25.444

# Benchmarking PyTorch: q_interpolate
# Mode: Eager
# Name: q_interpolate_M3_N720_K1280_dtypetorch.quint8_modebilinear_scale0.83333_contigTrue
# Input: M: 3, N: 720, K: 1280, dtype: torch.quint8, mode: bilinear, scale: 0.83333, contig: True
Run: 0, Forward Execution Time (us) : 704.073
Run: 1, Forward Execution Time (us) : 704.771
Run: 2, Forward Execution Time (us) : 705.166

+ python -m pt.stack_test --num-runs 3 --iterations 100 --warmup-iterations 10
/nvme/share/share/platform/env/miniconda3.8/envs/pt2.0_diopi/pytorch2.0/benchmarks/operator_benchmark/pt/stack_test.py:88: UserWarning: An output with one or more elements was resized since it had shape [1, 1, 1], which does not match the required output shape [2, 1, 1, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /nvme/share/share/platform/dep/DIOPI_pytorch/pytorch2.0_c263bd/aten/src/ATen/native/Resize.cpp:33.)
  return torch.stack(inputs, dim=dim, out=result)
/nvme/share/share/platform/env/miniconda3.8/envs/pt2.0_diopi/pytorch2.0/benchmarks/operator_benchmark/pt/stack_test.py:88: UserWarning: An output with one or more elements was resized since it had shape [1, 1, 1], which does not match the required output shape [1, 2, 1, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /nvme/share/share/platform/dep/DIOPI_pytorch/pytorch2.0_c263bd/aten/src/ATen/native/Resize.cpp:33.)
  return torch.stack(inputs, dim=dim, out=result)
/nvme/share/share/platform/env/miniconda3.8/envs/pt2.0_diopi/pytorch2.0/benchmarks/operator_benchmark/pt/stack_test.py:88: UserWarning: An output with one or more elements was resized since it had shape [1, 1, 1], which does not match the required output shape [1, 1, 2, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /nvme/share/share/platform/dep/DIOPI_pytorch/pytorch2.0_c263bd/aten/src/ATen/native/Resize.cpp:33.)
  return torch.stack(inputs, dim=dim, out=result)
/nvme/share/share/platform/env/miniconda3.8/envs/pt2.0_diopi/pytorch2.0/benchmarks/operator_benchmark/pt/stack_test.py:88: UserWarning: An output with one or more elements was resized since it had shape [1, 1, 1], which does not match the required output shape [1, 1, 1, 2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /nvme/share/share/platform/dep/DIOPI_pytorch/pytorch2.0_c263bd/aten/src/ATen/native/Resize.cpp:33.)
  return torch.stack(inputs, dim=dim, out=result)
/nvme/share/share/platform/env/miniconda3.8/envs/pt2.0_diopi/pytorch2.0/benchmarks/operator_benchmark/pt/stack_test.py:88: UserWarning: An output with one or more elements was resized since it had shape [512, 512, 2], which does not match the required output shape [2, 512, 512, 2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /nvme/share/share/platform/dep/DIOPI_pytorch/pytorch2.0_c263bd/aten/src/ATen/native/Resize.cpp:33.)
  return torch.stack(inputs, dim=dim, out=result)
/nvme/share/share/platform/env/miniconda3.8/envs/pt2.0_diopi/pytorch2.0/benchmarks/operator_benchmark/pt/stack_test.py:88: UserWarning: An output with one or more elements was resized since it had shape [512, 512, 2], which does not match the required output shape [512, 2, 512, 2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /nvme/share/share/platform/dep/DIOPI_pytorch/pytorch2.0_c263bd/aten/src/ATen/native/Resize.cpp:33.)
  return torch.stack(inputs, dim=dim, out=result)
/nvme/share/share/platform/env/miniconda3.8/envs/pt2.0_diopi/pytorch2.0/benchmarks/operator_benchmark/pt/stack_test.py:88: UserWarning: An output with one or more elements was resized since it had shape [512, 512, 2], which does not match the required output shape [512, 512, 2, 2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /nvme/share/share/platform/dep/DIOPI_pytorch/pytorch2.0_c263bd/aten/src/ATen/native/Resize.cpp:33.)
  return torch.stack(inputs, dim=dim, out=result)
/nvme/share/share/platform/env/miniconda3.8/envs/pt2.0_diopi/pytorch2.0/benchmarks/operator_benchmark/pt/stack_test.py:88: UserWarning: An output with one or more elements was resized since it had shape [128, 1024, 2], which does not match the required output shape [2, 128, 1024, 2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /nvme/share/share/platform/dep/DIOPI_pytorch/pytorch2.0_c263bd/aten/src/ATen/native/Resize.cpp:33.)
  return torch.stack(inputs, dim=dim, out=result)
/nvme/share/share/platform/env/miniconda3.8/envs/pt2.0_diopi/pytorch2.0/benchmarks/operator_benchmark/pt/stack_test.py:88: UserWarning: An output with one or more elements was resized since it had shape [128, 1024, 2], which does not match the required output shape [128, 2, 1024, 2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /nvme/share/share/platform/dep/DIOPI_pytorch/pytorch2.0_c263bd/aten/src/ATen/native/Resize.cpp:33.)
  return torch.stack(inputs, dim=dim, out=result)
/nvme/share/share/platform/env/miniconda3.8/envs/pt2.0_diopi/pytorch2.0/benchmarks/operator_benchmark/pt/stack_test.py:88: UserWarning: An output with one or more elements was resized since it had shape [128, 1024, 2], which does not match the required output shape [128, 1024, 2, 2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /nvme/share/share/platform/dep/DIOPI_pytorch/pytorch2.0_c263bd/aten/src/ATen/native/Resize.cpp:33.)
  return torch.stack(inputs, dim=dim, out=result)
not use dipu
# ----------------------------------------
# PyTorch/Caffe2 Operator Micro-benchmarks
# ----------------------------------------
# Tag : short

# Benchmarking PyTorch: stack
# Mode: Eager
# Name: stack_sizes(1,1,1)_N2_cpu_dim0
# Input: sizes: (1, 1, 1), N: 2, device: cpu, dim: 0
Run: 0, Forward Execution Time (us) : 7.528
Run: 1, Forward Execution Time (us) : 9.159
Run: 2, Forward Execution Time (us) : 9.028

# Benchmarking PyTorch: stack
# Mode: Eager
# Name: stack_sizes(1,1,1)_N2_cpu_dim1
# Input: sizes: (1, 1, 1), N: 2, device: cpu, dim: 1
Run: 0, Forward Execution Time (us) : 8.959
Run: 1, Forward Execution Time (us) : 9.020
Run: 2, Forward Execution Time (us) : 9.018

# Benchmarking PyTorch: stack
# Mode: Eager
# Name: stack_sizes(1,1,1)_N2_cpu_dim2
# Input: sizes: (1, 1, 1), N: 2, device: cpu, dim: 2
Run: 0, Forward Execution Time (us) : 8.948
Run: 1, Forward Execution Time (us) : 8.993
Run: 2, Forward Execution Time (us) : 8.987

# Benchmarking PyTorch: stack
# Mode: Eager
# Name: stack_sizes(1,1,1)_N2_cpu_dim3
# Input: sizes: (1, 1, 1), N: 2, device: cpu, dim: 3
Run: 0, Forward Execution Time (us) : 9.883
Run: 1, Forward Execution Time (us) : 9.870
Run: 2, Forward Execution Time (us) : 9.853

# Benchmarking PyTorch: stack
# Mode: Eager
# Name: stack_sizes(1,1,1)_N2_cuda_dim0
# Input: sizes: (1, 1, 1), N: 2, device: cuda, dim: 0
Run: 0, Forward Execution Time (us) : 18.374
Run: 1, Forward Execution Time (us) : 18.464
Run: 2, Forward Execution Time (us) : 18.056

# Benchmarking PyTorch: stack
# Mode: Eager
# Name: stack_sizes(1,1,1)_N2_cuda_dim1
# Input: sizes: (1, 1, 1), N: 2, device: cuda, dim: 1
Run: 0, Forward Execution Time (us) : 17.890
Run: 1, Forward Execution Time (us) : 18.040
Run: 2, Forward Execution Time (us) : 18.011

# Benchmarking PyTorch: stack
# Mode: Eager
# Name: stack_sizes(1,1,1)_N2_cuda_dim2
# Input: sizes: (1, 1, 1), N: 2, device: cuda, dim: 2
Run: 0, Forward Execution Time (us) : 17.821
Run: 1, Forward Execution Time (us) : 18.006
Run: 2, Forward Execution Time (us) : 17.946

# Benchmarking PyTorch: stack
# Mode: Eager
# Name: stack_sizes(1,1,1)_N2_cuda_dim3
# Input: sizes: (1, 1, 1), N: 2, device: cuda, dim: 3
Run: 0, Forward Execution Time (us) : 19.233
Run: 1, Forward Execution Time (us) : 19.287
Run: 2, Forward Execution Time (us) : 19.285

# Benchmarking PyTorch: stack
# Mode: Eager
# Name: stack_sizes(512,512,2)_N2_cpu_dim0
# Input: sizes: (512, 512, 2), N: 2, device: cpu, dim: 0
Run: 0, Forward Execution Time (us) : 336.783
Run: 1, Forward Execution Time (us) : 336.667
Run: 2, Forward Execution Time (us) : 336.596

# Benchmarking PyTorch: stack
# Mode: Eager
# Name: stack_sizes(512,512,2)_N2_cpu_dim1
# Input: sizes: (512, 512, 2), N: 2, device: cpu, dim: 1
Run: 0, Forward Execution Time (us) : 328.605
Run: 1, Forward Execution Time (us) : 336.649
Run: 2, Forward Execution Time (us) : 336.593

# Benchmarking PyTorch: stack
# Mode: Eager
# Name: stack_sizes(512,512,2)_N2_cpu_dim2
# Input: sizes: (512, 512, 2), N: 2, device: cpu, dim: 2
Run: 0, Forward Execution Time (us) : 2700.301
Run: 1, Forward Execution Time (us) : 2673.106
Run: 2, Forward Execution Time (us) : 2671.260

# Benchmarking PyTorch: stack
# Mode: Eager
# Name: stack_sizes(512,512,2)_N2_cpu_dim3
# Input: sizes: (512, 512, 2), N: 2, device: cpu, dim: 3
Run: 0, Forward Execution Time (us) : 4941.460
Run: 1, Forward Execution Time (us) : 4920.681
Run: 2, Forward Execution Time (us) : 4964.846

# Benchmarking PyTorch: stack
# Mode: Eager
# Name: stack_sizes(512,512,2)_N2_cuda_dim0
# Input: sizes: (512, 512, 2), N: 2, device: cuda, dim: 0
Run: 0, Forward Execution Time (us) : 18.041
Run: 1, Forward Execution Time (us) : 18.238
Run: 2, Forward Execution Time (us) : 18.252

# Benchmarking PyTorch: stack
# Mode: Eager
# Name: stack_sizes(512,512,2)_N2_cuda_dim1
# Input: sizes: (512, 512, 2), N: 2, device: cuda, dim: 1
Run: 0, Forward Execution Time (us) : 18.025
Run: 1, Forward Execution Time (us) : 18.140
Run: 2, Forward Execution Time (us) : 18.113

# Benchmarking PyTorch: stack
# Mode: Eager
# Name: stack_sizes(512,512,2)_N2_cuda_dim2
# Input: sizes: (512, 512, 2), N: 2, device: cuda, dim: 2
Run: 0, Forward Execution Time (us) : 18.163
Run: 1, Forward Execution Time (us) : 18.185
Run: 2, Forward Execution Time (us) : 18.261

# Benchmarking PyTorch: stack
# Mode: Eager
# Name: stack_sizes(512,512,2)_N2_cuda_dim3
# Input: sizes: (512, 512, 2), N: 2, device: cuda, dim: 3
Run: 0, Forward Execution Time (us) : 19.356
Run: 1, Forward Execution Time (us) : 19.529
Run: 2, Forward Execution Time (us) : 19.565

# Benchmarking PyTorch: stack
# Mode: Eager
# Name: stack_sizes(128,1024,2)_N2_cpu_dim0
# Input: sizes: (128, 1024, 2), N: 2, device: cpu, dim: 0
Run: 0, Forward Execution Time (us) : 170.896
Run: 1, Forward Execution Time (us) : 175.673
Run: 2, Forward Execution Time (us) : 175.709

# Benchmarking PyTorch: stack
# Mode: Eager
# Name: stack_sizes(128,1024,2)_N2_cpu_dim1
# Input: sizes: (128, 1024, 2), N: 2, device: cpu, dim: 1
Run: 0, Forward Execution Time (us) : 171.038
Run: 1, Forward Execution Time (us) : 175.695
Run: 2, Forward Execution Time (us) : 175.607

# Benchmarking PyTorch: stack
# Mode: Eager
# Name: stack_sizes(128,1024,2)_N2_cpu_dim2
# Input: sizes: (128, 1024, 2), N: 2, device: cpu, dim: 2
Run: 0, Forward Execution Time (us) : 1349.833
Run: 1, Forward Execution Time (us) : 1339.347
Run: 2, Forward Execution Time (us) : 1338.226

# Benchmarking PyTorch: stack
# Mode: Eager
# Name: stack_sizes(128,1024,2)_N2_cpu_dim3
# Input: sizes: (128, 1024, 2), N: 2, device: cpu, dim: 3
Run: 0, Forward Execution Time (us) : 2456.642
Run: 1, Forward Execution Time (us) : 2506.327
Run: 2, Forward Execution Time (us) : 2466.909

# Benchmarking PyTorch: stack
# Mode: Eager
# Name: stack_sizes(128,1024,2)_N2_cuda_dim0
# Input: sizes: (128, 1024, 2), N: 2, device: cuda, dim: 0
Run: 0, Forward Execution Time (us) : 17.737
Run: 1, Forward Execution Time (us) : 17.977
Run: 2, Forward Execution Time (us) : 18.013

# Benchmarking PyTorch: stack
# Mode: Eager
# Name: stack_sizes(128,1024,2)_N2_cuda_dim1
# Input: sizes: (128, 1024, 2), N: 2, device: cuda, dim: 1
Run: 0, Forward Execution Time (us) : 17.887
Run: 1, Forward Execution Time (us) : 17.972
Run: 2, Forward Execution Time (us) : 17.999

# Benchmarking PyTorch: stack
# Mode: Eager
# Name: stack_sizes(128,1024,2)_N2_cuda_dim2
# Input: sizes: (128, 1024, 2), N: 2, device: cuda, dim: 2
Run: 0, Forward Execution Time (us) : 17.774
Run: 1, Forward Execution Time (us) : 17.937
Run: 2, Forward Execution Time (us) : 17.901

# Benchmarking PyTorch: stack
# Mode: Eager
# Name: stack_sizes(128,1024,2)_N2_cuda_dim3
# Input: sizes: (128, 1024, 2), N: 2, device: cuda, dim: 3
Run: 0, Forward Execution Time (us) : 19.210
Run: 1, Forward Execution Time (us) : 19.264
Run: 2, Forward Execution Time (us) : 19.309

+ python -m pt.qarithmetic_test --num-runs 3 --iterations 100 --warmup-iterations 10
not use dipu
# ----------------------------------------
# PyTorch/Caffe2 Operator Micro-benchmarks
# ----------------------------------------
# Tag : short

# Benchmarking PyTorch: add
# Mode: Eager
# Name: add_N2_dtypetorch.quint8_contigFalse
# Input: N: 2, dtype: torch.quint8, contig: False
Run: 0, Forward Execution Time (us) : 15.013
Run: 1, Forward Execution Time (us) : 17.614
Run: 2, Forward Execution Time (us) : 17.601

# Benchmarking PyTorch: add
# Mode: Eager
# Name: add_N2_dtypetorch.quint8_contigTrue
# Input: N: 2, dtype: torch.quint8, contig: True
Run: 0, Forward Execution Time (us) : 16.730
Run: 1, Forward Execution Time (us) : 16.784
Run: 2, Forward Execution Time (us) : 16.834

# Benchmarking PyTorch: add
# Mode: Eager
# Name: add_N2_dtypetorch.qint8_contigFalse
# Input: N: 2, dtype: torch.qint8, contig: False
Run: 0, Forward Execution Time (us) : 17.403
Run: 1, Forward Execution Time (us) : 17.514
Run: 2, Forward Execution Time (us) : 17.513

# Benchmarking PyTorch: add
# Mode: Eager
# Name: add_N2_dtypetorch.qint8_contigTrue
# Input: N: 2, dtype: torch.qint8, contig: True
Run: 0, Forward Execution Time (us) : 16.573
Run: 1, Forward Execution Time (us) : 16.715
Run: 2, Forward Execution Time (us) : 16.657

# Benchmarking PyTorch: add
# Mode: Eager
# Name: add_N2_dtypetorch.qint32_contigFalse
# Input: N: 2, dtype: torch.qint32, contig: False
Run: 0, Forward Execution Time (us) : 17.368
Run: 1, Forward Execution Time (us) : 17.495
Run: 2, Forward Execution Time (us) : 17.545

# Benchmarking PyTorch: add
# Mode: Eager
# Name: add_N2_dtypetorch.qint32_contigTrue
# Input: N: 2, dtype: torch.qint32, contig: True
Run: 0, Forward Execution Time (us) : 16.602
Run: 1, Forward Execution Time (us) : 16.695
Run: 2, Forward Execution Time (us) : 16.723

# Benchmarking PyTorch: add
# Mode: Eager
# Name: add_N8_dtypetorch.quint8_contigFalse
# Input: N: 8, dtype: torch.quint8, contig: False
Run: 0, Forward Execution Time (us) : 20.653
Run: 1, Forward Execution Time (us) : 20.626
Run: 2, Forward Execution Time (us) : 20.616

# Benchmarking PyTorch: add
# Mode: Eager
# Name: add_N8_dtypetorch.quint8_contigTrue
# Input: N: 8, dtype: torch.quint8, contig: True
Run: 0, Forward Execution Time (us) : 16.600
Run: 1, Forward Execution Time (us) : 16.622
Run: 2, Forward Execution Time (us) : 16.636

# Benchmarking PyTorch: add
# Mode: Eager
# Name: add_N8_dtypetorch.qint8_contigFalse
# Input: N: 8, dtype: torch.qint8, contig: False
Run: 0, Forward Execution Time (us) : 20.478
Run: 1, Forward Execution Time (us) : 20.596
Run: 2, Forward Execution Time (us) : 20.577

# Benchmarking PyTorch: add
# Mode: Eager
# Name: add_N8_dtypetorch.qint8_contigTrue
# Input: N: 8, dtype: torch.qint8, contig: True
Run: 0, Forward Execution Time (us) : 16.569
Run: 1, Forward Execution Time (us) : 16.616
Run: 2, Forward Execution Time (us) : 16.605

# Benchmarking PyTorch: add
# Mode: Eager
# Name: add_N8_dtypetorch.qint32_contigFalse
# Input: N: 8, dtype: torch.qint32, contig: False
Run: 0, Forward Execution Time (us) : 20.564
Run: 1, Forward Execution Time (us) : 20.511
Run: 2, Forward Execution Time (us) : 20.521

# Benchmarking PyTorch: add
# Mode: Eager
# Name: add_N8_dtypetorch.qint32_contigTrue
# Input: N: 8, dtype: torch.qint32, contig: True
Run: 0, Forward Execution Time (us) : 18.380
Run: 1, Forward Execution Time (us) : 18.540
Run: 2, Forward Execution Time (us) : 18.495

# Benchmarking PyTorch: add
# Mode: Eager
# Name: add_N64_dtypetorch.quint8_contigFalse
# Input: N: 64, dtype: torch.quint8, contig: False
Run: 0, Forward Execution Time (us) : 224.769
Run: 1, Forward Execution Time (us) : 225.116
Run: 2, Forward Execution Time (us) : 224.936

# Benchmarking PyTorch: add
# Mode: Eager
# Name: add_N64_dtypetorch.quint8_contigTrue
# Input: N: 64, dtype: torch.quint8, contig: True
Run: 0, Forward Execution Time (us) : 19.177
Run: 1, Forward Execution Time (us) : 19.273
Run: 2, Forward Execution Time (us) : 19.282

# Benchmarking PyTorch: add
# Mode: Eager
# Name: add_N64_dtypetorch.qint8_contigFalse
# Input: N: 64, dtype: torch.qint8, contig: False
Run: 0, Forward Execution Time (us) : 225.732
Run: 1, Forward Execution Time (us) : 225.719
Run: 2, Forward Execution Time (us) : 225.679

# Benchmarking PyTorch: add
# Mode: Eager
# Name: add_N64_dtypetorch.qint8_contigTrue
# Input: N: 64, dtype: torch.qint8, contig: True
Run: 0, Forward Execution Time (us) : 19.013
Run: 1, Forward Execution Time (us) : 19.230
Run: 2, Forward Execution Time (us) : 19.109

# Benchmarking PyTorch: add
# Mode: Eager
# Name: add_N64_dtypetorch.qint32_contigFalse
# Input: N: 64, dtype: torch.qint32, contig: False
Run: 0, Forward Execution Time (us) : 222.915
Run: 1, Forward Execution Time (us) : 222.777
Run: 2, Forward Execution Time (us) : 222.895

# Benchmarking PyTorch: add
# Mode: Eager
# Name: add_N64_dtypetorch.qint32_contigTrue
# Input: N: 64, dtype: torch.qint32, contig: True
Run: 0, Forward Execution Time (us) : 140.903
Run: 1, Forward Execution Time (us) : 141.032
Run: 2, Forward Execution Time (us) : 141.055

# Benchmarking PyTorch: add
# Mode: Eager
# Name: add_N512_dtypetorch.quint8_contigFalse
# Input: N: 512, dtype: torch.quint8, contig: False
Run: 0, Forward Execution Time (us) : 11226.734
Run: 1, Forward Execution Time (us) : 11163.738
Run: 2, Forward Execution Time (us) : 11214.957

# Benchmarking PyTorch: add
# Mode: Eager
# Name: add_N512_dtypetorch.quint8_contigTrue
# Input: N: 512, dtype: torch.quint8, contig: True
Run: 0, Forward Execution Time (us) : 168.841
Run: 1, Forward Execution Time (us) : 168.872
Run: 2, Forward Execution Time (us) : 168.675

# Benchmarking PyTorch: add
# Mode: Eager
# Name: add_N512_dtypetorch.qint8_contigFalse
# Input: N: 512, dtype: torch.qint8, contig: False
Run: 0, Forward Execution Time (us) : 11248.622
Run: 1, Forward Execution Time (us) : 11250.665
Run: 2, Forward Execution Time (us) : 11285.466

# Benchmarking PyTorch: add
# Mode: Eager
# Name: add_N512_dtypetorch.qint8_contigTrue
# Input: N: 512, dtype: torch.qint8, contig: True
Run: 0, Forward Execution Time (us) : 169.023
Run: 1, Forward Execution Time (us) : 169.122
Run: 2, Forward Execution Time (us) : 169.109

# Benchmarking PyTorch: add
# Mode: Eager
# Name: add_N512_dtypetorch.qint32_contigFalse
# Input: N: 512, dtype: torch.qint32, contig: False
Run: 0, Forward Execution Time (us) : 11110.986
Run: 1, Forward Execution Time (us) : 11137.922
Run: 2, Forward Execution Time (us) : 11167.862

# Benchmarking PyTorch: add
# Mode: Eager
# Name: add_N512_dtypetorch.qint32_contigTrue
# Input: N: 512, dtype: torch.qint32, contig: True
Run: 0, Forward Execution Time (us) : 6732.601
Run: 1, Forward Execution Time (us) : 6722.719
Run: 2, Forward Execution Time (us) : 6734.156

# Benchmarking PyTorch: add_relu
# Mode: Eager
# Name: add_relu_N2_dtypetorch.quint8_contigFalse
# Input: N: 2, dtype: torch.quint8, contig: False
Run: 0, Forward Execution Time (us) : 19.052
Run: 1, Forward Execution Time (us) : 17.308
Run: 2, Forward Execution Time (us) : 17.484

# Benchmarking PyTorch: add_relu
# Mode: Eager
# Name: add_relu_N2_dtypetorch.quint8_contigTrue
# Input: N: 2, dtype: torch.quint8, contig: True
Run: 0, Forward Execution Time (us) : 16.596
Run: 1, Forward Execution Time (us) : 16.882
Run: 2, Forward Execution Time (us) : 16.580

# Benchmarking PyTorch: add_relu
# Mode: Eager
# Name: add_relu_N2_dtypetorch.qint8_contigFalse
# Input: N: 2, dtype: torch.qint8, contig: False
Run: 0, Forward Execution Time (us) : 17.388
Run: 1, Forward Execution Time (us) : 17.278
Run: 2, Forward Execution Time (us) : 17.342

# Benchmarking PyTorch: add_relu
# Mode: Eager
# Name: add_relu_N2_dtypetorch.qint8_contigTrue
# Input: N: 2, dtype: torch.qint8, contig: True
Run: 0, Forward Execution Time (us) : 16.594
Run: 1, Forward Execution Time (us) : 16.515
Run: 2, Forward Execution Time (us) : 16.577

# Benchmarking PyTorch: add_relu
# Mode: Eager
# Name: add_relu_N2_dtypetorch.qint32_contigFalse
# Input: N: 2, dtype: torch.qint32, contig: False
Run: 0, Forward Execution Time (us) : 17.434
Run: 1, Forward Execution Time (us) : 17.373
Run: 2, Forward Execution Time (us) : 17.341

# Benchmarking PyTorch: add_relu
# Mode: Eager
# Name: add_relu_N2_dtypetorch.qint32_contigTrue
# Input: N: 2, dtype: torch.qint32, contig: True
Run: 0, Forward Execution Time (us) : 16.715
Run: 1, Forward Execution Time (us) : 16.675
Run: 2, Forward Execution Time (us) : 16.659

# Benchmarking PyTorch: add_relu
# Mode: Eager
# Name: add_relu_N8_dtypetorch.quint8_contigFalse
# Input: N: 8, dtype: torch.quint8, contig: False
Run: 0, Forward Execution Time (us) : 21.513
Run: 1, Forward Execution Time (us) : 20.521
Run: 2, Forward Execution Time (us) : 20.486

# Benchmarking PyTorch: add_relu
# Mode: Eager
# Name: add_relu_N8_dtypetorch.quint8_contigTrue
# Input: N: 8, dtype: torch.quint8, contig: True
Run: 0, Forward Execution Time (us) : 16.511
Run: 1, Forward Execution Time (us) : 16.451
Run: 2, Forward Execution Time (us) : 16.441

# Benchmarking PyTorch: add_relu
# Mode: Eager
# Name: add_relu_N8_dtypetorch.qint8_contigFalse
# Input: N: 8, dtype: torch.qint8, contig: False
Run: 0, Forward Execution Time (us) : 20.456
Run: 1, Forward Execution Time (us) : 20.387
Run: 2, Forward Execution Time (us) : 20.379

# Benchmarking PyTorch: add_relu
# Mode: Eager
# Name: add_relu_N8_dtypetorch.qint8_contigTrue
# Input: N: 8, dtype: torch.qint8, contig: True
Run: 0, Forward Execution Time (us) : 16.348
Run: 1, Forward Execution Time (us) : 16.392
Run: 2, Forward Execution Time (us) : 16.331

# Benchmarking PyTorch: add_relu
# Mode: Eager
# Name: add_relu_N8_dtypetorch.qint32_contigFalse
# Input: N: 8, dtype: torch.qint32, contig: False
Run: 0, Forward Execution Time (us) : 20.504
Run: 1, Forward Execution Time (us) : 20.612
Run: 2, Forward Execution Time (us) : 20.449

# Benchmarking PyTorch: add_relu
# Mode: Eager
# Name: add_relu_N8_dtypetorch.qint32_contigTrue
# Input: N: 8, dtype: torch.qint32, contig: True
Run: 0, Forward Execution Time (us) : 18.398
Run: 1, Forward Execution Time (us) : 18.391
Run: 2, Forward Execution Time (us) : 18.401

# Benchmarking PyTorch: add_relu
# Mode: Eager
# Name: add_relu_N64_dtypetorch.quint8_contigFalse
# Input: N: 64, dtype: torch.quint8, contig: False
Run: 0, Forward Execution Time (us) : 219.987
Run: 1, Forward Execution Time (us) : 222.037
Run: 2, Forward Execution Time (us) : 219.999

# Benchmarking PyTorch: add_relu
# Mode: Eager
# Name: add_relu_N64_dtypetorch.quint8_contigTrue
# Input: N: 64, dtype: torch.quint8, contig: True
Run: 0, Forward Execution Time (us) : 19.498
Run: 1, Forward Execution Time (us) : 19.314
Run: 2, Forward Execution Time (us) : 19.310

# Benchmarking PyTorch: add_relu
# Mode: Eager
# Name: add_relu_N64_dtypetorch.qint8_contigFalse
# Input: N: 64, dtype: torch.qint8, contig: False
Run: 0, Forward Execution Time (us) : 239.940
Run: 1, Forward Execution Time (us) : 240.285
Run: 2, Forward Execution Time (us) : 240.064

# Benchmarking PyTorch: add_relu
# Mode: Eager
# Name: add_relu_N64_dtypetorch.qint8_contigTrue
# Input: N: 64, dtype: torch.qint8, contig: True
Run: 0, Forward Execution Time (us) : 19.278
Run: 1, Forward Execution Time (us) : 19.285
Run: 2, Forward Execution Time (us) : 19.243

# Benchmarking PyTorch: add_relu
# Mode: Eager
# Name: add_relu_N64_dtypetorch.qint32_contigFalse
# Input: N: 64, dtype: torch.qint32, contig: False
Run: 0, Forward Execution Time (us) : 240.532
Run: 1, Forward Execution Time (us) : 240.940
Run: 2, Forward Execution Time (us) : 240.860

# Benchmarking PyTorch: add_relu
# Mode: Eager
# Name: add_relu_N64_dtypetorch.qint32_contigTrue
# Input: N: 64, dtype: torch.qint32, contig: True
Run: 0, Forward Execution Time (us) : 141.221
Run: 1, Forward Execution Time (us) : 141.365
Run: 2, Forward Execution Time (us) : 141.089

# Benchmarking PyTorch: add_relu
# Mode: Eager
# Name: add_relu_N512_dtypetorch.quint8_contigFalse
# Input: N: 512, dtype: torch.quint8, contig: False
Run: 0, Forward Execution Time (us) : 11034.202
Run: 1, Forward Execution Time (us) : 10977.678
Run: 2, Forward Execution Time (us) : 11026.326

# Benchmarking PyTorch: add_relu
# Mode: Eager
# Name: add_relu_N512_dtypetorch.quint8_contigTrue
# Input: N: 512, dtype: torch.quint8, contig: True
Run: 0, Forward Execution Time (us) : 186.051
Run: 1, Forward Execution Time (us) : 186.230
Run: 2, Forward Execution Time (us) : 186.109

# Benchmarking PyTorch: add_relu
# Mode: Eager
# Name: add_relu_N512_dtypetorch.qint8_contigFalse
# Input: N: 512, dtype: torch.qint8, contig: False
Run: 0, Forward Execution Time (us) : 12112.666
Run: 1, Forward Execution Time (us) : 12112.881
Run: 2, Forward Execution Time (us) : 12152.068

# Benchmarking PyTorch: add_relu
# Mode: Eager
# Name: add_relu_N512_dtypetorch.qint8_contigTrue
# Input: N: 512, dtype: torch.qint8, contig: True
Run: 0, Forward Execution Time (us) : 186.122
Run: 1, Forward Execution Time (us) : 186.526
Run: 2, Forward Execution Time (us) : 186.322

# Benchmarking PyTorch: add_relu
# Mode: Eager
# Name: add_relu_N512_dtypetorch.qint32_contigFalse
# Input: N: 512, dtype: torch.qint32, contig: False
Run: 0, Forward Execution Time (us) : 12396.997
Run: 1, Forward Execution Time (us) : 12390.297
Run: 2, Forward Execution Time (us) : 12400.113

# Benchmarking PyTorch: add_relu
# Mode: Eager
# Name: add_relu_N512_dtypetorch.qint32_contigTrue
# Input: N: 512, dtype: torch.qint32, contig: True
Run: 0, Forward Execution Time (us) : 6764.810
Run: 1, Forward Execution Time (us) : 6766.797
Run: 2, Forward Execution Time (us) : 6767.378

# Benchmarking PyTorch: mul
# Mode: Eager
# Name: mul_N2_dtypetorch.quint8_contigFalse
# Input: N: 2, dtype: torch.quint8, contig: False
Run: 0, Forward Execution Time (us) : 17.837
Run: 1, Forward Execution Time (us) : 17.243
Run: 2, Forward Execution Time (us) : 17.314

# Benchmarking PyTorch: mul
# Mode: Eager
# Name: mul_N2_dtypetorch.quint8_contigTrue
# Input: N: 2, dtype: torch.quint8, contig: True
Run: 0, Forward Execution Time (us) : 16.628
Run: 1, Forward Execution Time (us) : 16.578
Run: 2, Forward Execution Time (us) : 16.570

# Benchmarking PyTorch: mul
# Mode: Eager
# Name: mul_N2_dtypetorch.qint8_contigFalse
# Input: N: 2, dtype: torch.qint8, contig: False
Run: 0, Forward Execution Time (us) : 13.530
Run: 1, Forward Execution Time (us) : 16.400
Run: 2, Forward Execution Time (us) : 16.332

# Benchmarking PyTorch: mul
# Mode: Eager
# Name: mul_N2_dtypetorch.qint8_contigTrue
# Input: N: 2, dtype: torch.qint8, contig: True
Run: 0, Forward Execution Time (us) : 8.173
Run: 1, Forward Execution Time (us) : 8.471
Run: 2, Forward Execution Time (us) : 8.349

# Benchmarking PyTorch: mul
# Mode: Eager
# Name: mul_N2_dtypetorch.qint32_contigFalse
# Input: N: 2, dtype: torch.qint32, contig: False
Run: 0, Forward Execution Time (us) : 9.006
Run: 1, Forward Execution Time (us) : 9.292
Run: 2, Forward Execution Time (us) : 9.267

# Benchmarking PyTorch: mul
# Mode: Eager
# Name: mul_N2_dtypetorch.qint32_contigTrue
# Input: N: 2, dtype: torch.qint32, contig: True
Run: 0, Forward Execution Time (us) : 8.714
Run: 1, Forward Execution Time (us) : 8.819
Run: 2, Forward Execution Time (us) : 8.812

# Benchmarking PyTorch: mul
# Mode: Eager
# Name: mul_N8_dtypetorch.quint8_contigFalse
# Input: N: 8, dtype: torch.quint8, contig: False
Run: 0, Forward Execution Time (us) : 9.698
Run: 1, Forward Execution Time (us) : 9.589
Run: 2, Forward Execution Time (us) : 9.523

# Benchmarking PyTorch: mul
# Mode: Eager
# Name: mul_N8_dtypetorch.quint8_contigTrue
# Input: N: 8, dtype: torch.quint8, contig: True
Run: 0, Forward Execution Time (us) : 8.753
Run: 1, Forward Execution Time (us) : 8.839
Run: 2, Forward Execution Time (us) : 8.774

# Benchmarking PyTorch: mul
# Mode: Eager
# Name: mul_N8_dtypetorch.qint8_contigFalse
# Input: N: 8, dtype: torch.qint8, contig: False
Run: 0, Forward Execution Time (us) : 16.009
Run: 1, Forward Execution Time (us) : 16.246
Run: 2, Forward Execution Time (us) : 16.317

# Benchmarking PyTorch: mul
# Mode: Eager
# Name: mul_N8_dtypetorch.qint8_contigTrue
# Input: N: 8, dtype: torch.qint8, contig: True
Run: 0, Forward Execution Time (us) : 15.409
Run: 1, Forward Execution Time (us) : 15.479
Run: 2, Forward Execution Time (us) : 15.567

# Benchmarking PyTorch: mul
# Mode: Eager
# Name: mul_N8_dtypetorch.qint32_contigFalse
# Input: N: 8, dtype: torch.qint32, contig: False
Run: 0, Forward Execution Time (us) : 18.173
Run: 1, Forward Execution Time (us) : 18.180
Run: 2, Forward Execution Time (us) : 18.259

# Benchmarking PyTorch: mul
# Mode: Eager
# Name: mul_N8_dtypetorch.qint32_contigTrue
# Input: N: 8, dtype: torch.qint32, contig: True
Run: 0, Forward Execution Time (us) : 16.387
Run: 1, Forward Execution Time (us) : 16.475
Run: 2, Forward Execution Time (us) : 16.443

# Benchmarking PyTorch: mul
# Mode: Eager
# Name: mul_N64_dtypetorch.quint8_contigFalse
# Input: N: 64, dtype: torch.quint8, contig: False
Run: 0, Forward Execution Time (us) : 79.590
Run: 1, Forward Execution Time (us) : 79.600
Run: 2, Forward Execution Time (us) : 79.641

# Benchmarking PyTorch: mul
# Mode: Eager
# Name: mul_N64_dtypetorch.quint8_contigTrue
# Input: N: 64, dtype: torch.quint8, contig: True
Run: 0, Forward Execution Time (us) : 18.530
Run: 1, Forward Execution Time (us) : 18.598
Run: 2, Forward Execution Time (us) : 18.684

# Benchmarking PyTorch: mul
# Mode: Eager
# Name: mul_N64_dtypetorch.qint8_contigFalse
# Input: N: 64, dtype: torch.qint8, contig: False
Run: 0, Forward Execution Time (us) : 38.520
Run: 1, Forward Execution Time (us) : 38.528
Run: 2, Forward Execution Time (us) : 38.492

# Benchmarking PyTorch: mul
# Mode: Eager
# Name: mul_N64_dtypetorch.qint8_contigTrue
# Input: N: 64, dtype: torch.qint8, contig: True
Run: 0, Forward Execution Time (us) : 17.302
Run: 1, Forward Execution Time (us) : 17.271
Run: 2, Forward Execution Time (us) : 17.322

# Benchmarking PyTorch: mul
# Mode: Eager
# Name: mul_N64_dtypetorch.qint32_contigFalse
# Input: N: 64, dtype: torch.qint32, contig: False
Run: 0, Forward Execution Time (us) : 81.572
Run: 1, Forward Execution Time (us) : 82.303
Run: 2, Forward Execution Time (us) : 81.643

# Benchmarking PyTorch: mul
# Mode: Eager
# Name: mul_N64_dtypetorch.qint32_contigTrue
# Input: N: 64, dtype: torch.qint32, contig: True
Run: 0, Forward Execution Time (us) : 17.924
Run: 1, Forward Execution Time (us) : 17.988
Run: 2, Forward Execution Time (us) : 17.927

# Benchmarking PyTorch: mul
# Mode: Eager
# Name: mul_N512_dtypetorch.quint8_contigFalse
# Input: N: 512, dtype: torch.quint8, contig: False
Run: 0, Forward Execution Time (us) : 3335.914
Run: 1, Forward Execution Time (us) : 3334.451
Run: 2, Forward Execution Time (us) : 3336.299

# Benchmarking PyTorch: mul
# Mode: Eager
# Name: mul_N512_dtypetorch.quint8_contigTrue
# Input: N: 512, dtype: torch.quint8, contig: True
Run: 0, Forward Execution Time (us) : 119.641
Run: 1, Forward Execution Time (us) : 141.143
Run: 2, Forward Execution Time (us) : 141.187

# Benchmarking PyTorch: mul
# Mode: Eager
# Name: mul_N512_dtypetorch.qint8_contigFalse
# Input: N: 512, dtype: torch.qint8, contig: False
Run: 0, Forward Execution Time (us) : 523.980
Run: 1, Forward Execution Time (us) : 615.672
Run: 2, Forward Execution Time (us) : 618.143

# Benchmarking PyTorch: mul
# Mode: Eager
# Name: mul_N512_dtypetorch.qint8_contigTrue
# Input: N: 512, dtype: torch.qint8, contig: True
Run: 0, Forward Execution Time (us) : 102.789
Run: 1, Forward Execution Time (us) : 121.480
Run: 2, Forward Execution Time (us) : 121.401

# Benchmarking PyTorch: mul
# Mode: Eager
# Name: mul_N512_dtypetorch.qint32_contigFalse
# Input: N: 512, dtype: torch.qint32, contig: False
Run: 0, Forward Execution Time (us) : 3825.951
Run: 1, Forward Execution Time (us) : 3829.437
Run: 2, Forward Execution Time (us) : 3827.781

# Benchmarking PyTorch: mul
# Mode: Eager
# Name: mul_N512_dtypetorch.qint32_contigTrue
# Input: N: 512, dtype: torch.qint32, contig: True
Run: 0, Forward Execution Time (us) : 125.760
Run: 1, Forward Execution Time (us) : 125.445
Run: 2, Forward Execution Time (us) : 123.515

# Benchmarking PyTorch: add_scalar
# Mode: Eager
# Name: add_scalar_N2_dtypetorch.quint8_contigFalse
# Input: N: 2, dtype: torch.quint8, contig: False
Run: 0, Forward Execution Time (us) : 16.228
Run: 1, Forward Execution Time (us) : 16.180
Run: 2, Forward Execution Time (us) : 16.184

# Benchmarking PyTorch: add_scalar
# Mode: Eager
# Name: add_scalar_N2_dtypetorch.quint8_contigTrue
# Input: N: 2, dtype: torch.quint8, contig: True
Run: 0, Forward Execution Time (us) : 15.391
Run: 1, Forward Execution Time (us) : 15.485
Run: 2, Forward Execution Time (us) : 15.493

# Benchmarking PyTorch: add_scalar
# Mode: Eager
# Name: add_scalar_N2_dtypetorch.qint8_contigFalse
# Input: N: 2, dtype: torch.qint8, contig: False
Run: 0, Forward Execution Time (us) : 16.540
Run: 1, Forward Execution Time (us) : 16.554
Run: 2, Forward Execution Time (us) : 16.616

# Benchmarking PyTorch: add_scalar
# Mode: Eager
# Name: add_scalar_N2_dtypetorch.qint8_contigTrue
# Input: N: 2, dtype: torch.qint8, contig: True
Run: 0, Forward Execution Time (us) : 15.845
Run: 1, Forward Execution Time (us) : 15.884
Run: 2, Forward Execution Time (us) : 15.883

# Benchmarking PyTorch: add_scalar
# Mode: Eager
# Name: add_scalar_N2_dtypetorch.qint32_contigFalse
# Input: N: 2, dtype: torch.qint32, contig: False
Run: 0, Forward Execution Time (us) : 16.587
Run: 1, Forward Execution Time (us) : 16.636
Run: 2, Forward Execution Time (us) : 16.664

# Benchmarking PyTorch: add_scalar
# Mode: Eager
# Name: add_scalar_N2_dtypetorch.qint32_contigTrue
# Input: N: 2, dtype: torch.qint32, contig: True
Run: 0, Forward Execution Time (us) : 15.864
Run: 1, Forward Execution Time (us) : 15.910
Run: 2, Forward Execution Time (us) : 15.885

# Benchmarking PyTorch: add_scalar
# Mode: Eager
# Name: add_scalar_N8_dtypetorch.quint8_contigFalse
# Input: N: 8, dtype: torch.quint8, contig: False
Run: 0, Forward Execution Time (us) : 16.925
Run: 1, Forward Execution Time (us) : 16.932
Run: 2, Forward Execution Time (us) : 16.971

# Benchmarking PyTorch: add_scalar
# Mode: Eager
# Name: add_scalar_N8_dtypetorch.quint8_contigTrue
# Input: N: 8, dtype: torch.quint8, contig: True
Run: 0, Forward Execution Time (us) : 15.399
Run: 1, Forward Execution Time (us) : 15.392
Run: 2, Forward Execution Time (us) : 15.374

# Benchmarking PyTorch: add_scalar
# Mode: Eager
# Name: add_scalar_N8_dtypetorch.qint8_contigFalse
# Input: N: 8, dtype: torch.qint8, contig: False
Run: 0, Forward Execution Time (us) : 16.628
Run: 1, Forward Execution Time (us) : 16.623
Run: 2, Forward Execution Time (us) : 16.646

# Benchmarking PyTorch: add_scalar
# Mode: Eager
# Name: add_scalar_N8_dtypetorch.qint8_contigTrue
# Input: N: 8, dtype: torch.qint8, contig: True
Run: 0, Forward Execution Time (us) : 15.720
Run: 1, Forward Execution Time (us) : 15.740
Run: 2, Forward Execution Time (us) : 15.752

# Benchmarking PyTorch: add_scalar
# Mode: Eager
# Name: add_scalar_N8_dtypetorch.qint32_contigFalse
# Input: N: 8, dtype: torch.qint32, contig: False
Run: 0, Forward Execution Time (us) : 16.651
Run: 1, Forward Execution Time (us) : 16.634
Run: 2, Forward Execution Time (us) : 16.679

# Benchmarking PyTorch: add_scalar
# Mode: Eager
# Name: add_scalar_N8_dtypetorch.qint32_contigTrue
# Input: N: 8, dtype: torch.qint32, contig: True
Run: 0, Forward Execution Time (us) : 15.896
Run: 1, Forward Execution Time (us) : 15.944
Run: 2, Forward Execution Time (us) : 15.964

# Benchmarking PyTorch: add_scalar
# Mode: Eager
# Name: add_scalar_N64_dtypetorch.quint8_contigFalse
# Input: N: 64, dtype: torch.quint8, contig: False
Run: 0, Forward Execution Time (us) : 75.652
Run: 1, Forward Execution Time (us) : 75.677
Run: 2, Forward Execution Time (us) : 75.743

# Benchmarking PyTorch: add_scalar
# Mode: Eager
# Name: add_scalar_N64_dtypetorch.quint8_contigTrue
# Input: N: 64, dtype: torch.quint8, contig: True
Run: 0, Forward Execution Time (us) : 16.776
Run: 1, Forward Execution Time (us) : 16.766
Run: 2, Forward Execution Time (us) : 16.813

# Benchmarking PyTorch: add_scalar
# Mode: Eager
# Name: add_scalar_N64_dtypetorch.qint8_contigFalse
# Input: N: 64, dtype: torch.qint8, contig: False
Run: 0, Forward Execution Time (us) : 19.530
Run: 1, Forward Execution Time (us) : 19.832
Run: 2, Forward Execution Time (us) : 19.632

# Benchmarking PyTorch: add_scalar
# Mode: Eager
# Name: add_scalar_N64_dtypetorch.qint8_contigTrue
# Input: N: 64, dtype: torch.qint8, contig: True
Run: 0, Forward Execution Time (us) : 16.103
Run: 1, Forward Execution Time (us) : 16.228
Run: 2, Forward Execution Time (us) : 16.255

# Benchmarking PyTorch: add_scalar
# Mode: Eager
# Name: add_scalar_N64_dtypetorch.qint32_contigFalse
# Input: N: 64, dtype: torch.qint32, contig: False
Run: 0, Forward Execution Time (us) : 20.079
Run: 1, Forward Execution Time (us) : 20.057
Run: 2, Forward Execution Time (us) : 20.033

# Benchmarking PyTorch: add_scalar
# Mode: Eager
# Name: add_scalar_N64_dtypetorch.qint32_contigTrue
# Input: N: 64, dtype: torch.qint32, contig: True
Run: 0, Forward Execution Time (us) : 16.762
Run: 1, Forward Execution Time (us) : 16.795
Run: 2, Forward Execution Time (us) : 16.814

# Benchmarking PyTorch: add_scalar
# Mode: Eager
# Name: add_scalar_N512_dtypetorch.quint8_contigFalse
# Input: N: 512, dtype: torch.quint8, contig: False
Run: 0, Forward Execution Time (us) : 3212.668
Run: 1, Forward Execution Time (us) : 3213.747
Run: 2, Forward Execution Time (us) : 3212.390

# Benchmarking PyTorch: add_scalar
# Mode: Eager
# Name: add_scalar_N512_dtypetorch.quint8_contigTrue
# Input: N: 512, dtype: torch.quint8, contig: True
Run: 0, Forward Execution Time (us) : 70.954
Run: 1, Forward Execution Time (us) : 83.915
Run: 2, Forward Execution Time (us) : 83.924

# Benchmarking PyTorch: add_scalar
# Mode: Eager
# Name: add_scalar_N512_dtypetorch.qint8_contigFalse
# Input: N: 512, dtype: torch.qint8, contig: False
Run: 0, Forward Execution Time (us) : 203.117
Run: 1, Forward Execution Time (us) : 242.102
Run: 2, Forward Execution Time (us) : 248.766

# Benchmarking PyTorch: add_scalar
# Mode: Eager
# Name: add_scalar_N512_dtypetorch.qint8_contigTrue
# Input: N: 512, dtype: torch.qint8, contig: True
Run: 0, Forward Execution Time (us) : 21.571
Run: 1, Forward Execution Time (us) : 25.213
Run: 2, Forward Execution Time (us) : 25.153

# Benchmarking PyTorch: add_scalar
# Mode: Eager
# Name: add_scalar_N512_dtypetorch.qint32_contigFalse
# Input: N: 512, dtype: torch.qint32, contig: False
Run: 0, Forward Execution Time (us) : 517.367
Run: 1, Forward Execution Time (us) : 554.579
Run: 2, Forward Execution Time (us) : 572.279

# Benchmarking PyTorch: add_scalar
# Mode: Eager
# Name: add_scalar_N512_dtypetorch.qint32_contigTrue
# Input: N: 512, dtype: torch.qint32, contig: True
Run: 0, Forward Execution Time (us) : 101.974
Run: 1, Forward Execution Time (us) : 105.903
Run: 2, Forward Execution Time (us) : 105.834

# Benchmarking PyTorch: mul_scalar
# Mode: Eager
# Name: mul_scalar_N2_dtypetorch.quint8_contigFalse
# Input: N: 2, dtype: torch.quint8, contig: False
Run: 0, Forward Execution Time (us) : 16.713
Run: 1, Forward Execution Time (us) : 16.228
Run: 2, Forward Execution Time (us) : 16.237

# Benchmarking PyTorch: mul_scalar
# Mode: Eager
# Name: mul_scalar_N2_dtypetorch.quint8_contigTrue
# Input: N: 2, dtype: torch.quint8, contig: True
Run: 0, Forward Execution Time (us) : 15.455
Run: 1, Forward Execution Time (us) : 15.539
Run: 2, Forward Execution Time (us) : 15.499

# Benchmarking PyTorch: mul_scalar
# Mode: Eager
# Name: mul_scalar_N2_dtypetorch.qint8_contigFalse
# Input: N: 2, dtype: torch.qint8, contig: False
Run: 0, Forward Execution Time (us) : 16.260
Run: 1, Forward Execution Time (us) : 16.245
Run: 2, Forward Execution Time (us) : 16.239

# Benchmarking PyTorch: mul_scalar
# Mode: Eager
# Name: mul_scalar_N2_dtypetorch.qint8_contigTrue
# Input: N: 2, dtype: torch.qint8, contig: True
Run: 0, Forward Execution Time (us) : 15.517
Run: 1, Forward Execution Time (us) : 15.631
Run: 2, Forward Execution Time (us) : 15.602

# Benchmarking PyTorch: mul_scalar
# Mode: Eager
# Name: mul_scalar_N2_dtypetorch.qint32_contigFalse
# Input: N: 2, dtype: torch.qint32, contig: False
Run: 0, Forward Execution Time (us) : 16.164
Run: 1, Forward Execution Time (us) : 16.235
Run: 2, Forward Execution Time (us) : 16.137

# Benchmarking PyTorch: mul_scalar
# Mode: Eager
# Name: mul_scalar_N2_dtypetorch.qint32_contigTrue
# Input: N: 2, dtype: torch.qint32, contig: True
Run: 0, Forward Execution Time (us) : 15.473
Run: 1, Forward Execution Time (us) : 15.540
Run: 2, Forward Execution Time (us) : 15.566

# Benchmarking PyTorch: mul_scalar
# Mode: Eager
# Name: mul_scalar_N8_dtypetorch.quint8_contigFalse
# Input: N: 8, dtype: torch.quint8, contig: False
Run: 0, Forward Execution Time (us) : 16.461
Run: 1, Forward Execution Time (us) : 16.318
Run: 2, Forward Execution Time (us) : 16.350

# Benchmarking PyTorch: mul_scalar
# Mode: Eager
# Name: mul_scalar_N8_dtypetorch.quint8_contigTrue
# Input: N: 8, dtype: torch.quint8, contig: True
Run: 0, Forward Execution Time (us) : 15.490
Run: 1, Forward Execution Time (us) : 15.495
Run: 2, Forward Execution Time (us) : 15.511

# Benchmarking PyTorch: mul_scalar
# Mode: Eager
# Name: mul_scalar_N8_dtypetorch.qint8_contigFalse
# Input: N: 8, dtype: torch.qint8, contig: False
Run: 0, Forward Execution Time (us) : 16.256
Run: 1, Forward Execution Time (us) : 16.367
Run: 2, Forward Execution Time (us) : 16.252

# Benchmarking PyTorch: mul_scalar
# Mode: Eager
# Name: mul_scalar_N8_dtypetorch.qint8_contigTrue
# Input: N: 8, dtype: torch.qint8, contig: True
Run: 0, Forward Execution Time (us) : 15.495
Run: 1, Forward Execution Time (us) : 15.600
Run: 2, Forward Execution Time (us) : 15.565

# Benchmarking PyTorch: mul_scalar
# Mode: Eager
# Name: mul_scalar_N8_dtypetorch.qint32_contigFalse
# Input: N: 8, dtype: torch.qint32, contig: False
Run: 0, Forward Execution Time (us) : 16.297
Run: 1, Forward Execution Time (us) : 16.281
Run: 2, Forward Execution Time (us) : 16.321

# Benchmarking PyTorch: mul_scalar
# Mode: Eager
# Name: mul_scalar_N8_dtypetorch.qint32_contigTrue
# Input: N: 8, dtype: torch.qint32, contig: True
Run: 0, Forward Execution Time (us) : 15.597
Run: 1, Forward Execution Time (us) : 15.580
Run: 2, Forward Execution Time (us) : 15.584

# Benchmarking PyTorch: mul_scalar
# Mode: Eager
# Name: mul_scalar_N64_dtypetorch.quint8_contigFalse
# Input: N: 64, dtype: torch.quint8, contig: False
Run: 0, Forward Execution Time (us) : 18.956
Run: 1, Forward Execution Time (us) : 19.027
Run: 2, Forward Execution Time (us) : 18.985

# Benchmarking PyTorch: mul_scalar
# Mode: Eager
# Name: mul_scalar_N64_dtypetorch.quint8_contigTrue
# Input: N: 64, dtype: torch.quint8, contig: True
Run: 0, Forward Execution Time (us) : 15.859
Run: 1, Forward Execution Time (us) : 15.976
Run: 2, Forward Execution Time (us) : 15.912

# Benchmarking PyTorch: mul_scalar
# Mode: Eager
# Name: mul_scalar_N64_dtypetorch.qint8_contigFalse
# Input: N: 64, dtype: torch.qint8, contig: False
Run: 0, Forward Execution Time (us) : 19.134
Run: 1, Forward Execution Time (us) : 19.349
Run: 2, Forward Execution Time (us) : 19.264

# Benchmarking PyTorch: mul_scalar
# Mode: Eager
# Name: mul_scalar_N64_dtypetorch.qint8_contigTrue
# Input: N: 64, dtype: torch.qint8, contig: True
Run: 0, Forward Execution Time (us) : 15.766
Run: 1, Forward Execution Time (us) : 15.841
Run: 2, Forward Execution Time (us) : 15.832

# Benchmarking PyTorch: mul_scalar
# Mode: Eager
# Name: mul_scalar_N64_dtypetorch.qint32_contigFalse
# Input: N: 64, dtype: torch.qint32, contig: False
Run: 0, Forward Execution Time (us) : 19.579
Run: 1, Forward Execution Time (us) : 19.533
Run: 2, Forward Execution Time (us) : 19.605

# Benchmarking PyTorch: mul_scalar
# Mode: Eager
# Name: mul_scalar_N64_dtypetorch.qint32_contigTrue
# Input: N: 64, dtype: torch.qint32, contig: True
Run: 0, Forward Execution Time (us) : 16.150
Run: 1, Forward Execution Time (us) : 16.209
Run: 2, Forward Execution Time (us) : 16.186

# Benchmarking PyTorch: mul_scalar
# Mode: Eager
# Name: mul_scalar_N512_dtypetorch.quint8_contigFalse
# Input: N: 512, dtype: torch.quint8, contig: False
Run: 0, Forward Execution Time (us) : 188.120
Run: 1, Forward Execution Time (us) : 222.897
Run: 2, Forward Execution Time (us) : 223.013

# Benchmarking PyTorch: mul_scalar
# Mode: Eager
# Name: mul_scalar_N512_dtypetorch.quint8_contigTrue
# Input: N: 512, dtype: torch.quint8, contig: True
Run: 0, Forward Execution Time (us) : 20.686
Run: 1, Forward Execution Time (us) : 24.328
Run: 2, Forward Execution Time (us) : 24.334

# Benchmarking PyTorch: mul_scalar
# Mode: Eager
# Name: mul_scalar_N512_dtypetorch.qint8_contigFalse
# Input: N: 512, dtype: torch.qint8, contig: False
Run: 0, Forward Execution Time (us) : 206.811
Run: 1, Forward Execution Time (us) : 240.971
Run: 2, Forward Execution Time (us) : 239.055

# Benchmarking PyTorch: mul_scalar
# Mode: Eager
# Name: mul_scalar_N512_dtypetorch.qint8_contigTrue
# Input: N: 512, dtype: torch.qint8, contig: True
Run: 0, Forward Execution Time (us) : 21.165
Run: 1, Forward Execution Time (us) : 24.843
Run: 2, Forward Execution Time (us) : 24.936

# Benchmarking PyTorch: mul_scalar
# Mode: Eager
# Name: mul_scalar_N512_dtypetorch.qint32_contigFalse
# Input: N: 512, dtype: torch.qint32, contig: False
Run: 0, Forward Execution Time (us) : 513.297
Run: 1, Forward Execution Time (us) : 552.051
Run: 2, Forward Execution Time (us) : 551.929

# Benchmarking PyTorch: mul_scalar
# Mode: Eager
# Name: mul_scalar_N512_dtypetorch.qint32_contigTrue
# Input: N: 512, dtype: torch.qint32, contig: True
Run: 0, Forward Execution Time (us) : 101.600
Run: 1, Forward Execution Time (us) : 105.486
Run: 2, Forward Execution Time (us) : 105.427

+ python -m pt.qlayernorm_test --num-runs 3 --iterations 100 --warmup-iterations 10
not use dipu
# ----------------------------------------
# PyTorch/Caffe2 Operator Micro-benchmarks
# ----------------------------------------
# Tag : short

# Benchmarking PyTorch: QLayerNormBenchmark
# Mode: Eager
# Name: QLayerNormBenchmark_dims(1,8,16)_dtypetorch.qint8
# Input: dims: (1, 8, 16), dtype: torch.qint8
Run: 0, Forward Execution Time (us) : 22.528
Run: 1, Forward Execution Time (us) : 22.499
Run: 2, Forward Execution Time (us) : 22.221

# Benchmarking PyTorch: QLayerNormBenchmark
# Mode: Eager
# Name: QLayerNormBenchmark_dims(8,8,16)_dtypetorch.qint8
# Input: dims: (8, 8, 16), dtype: torch.qint8
Run: 0, Forward Execution Time (us) : 24.253
Run: 1, Forward Execution Time (us) : 24.311
Run: 2, Forward Execution Time (us) : 24.257

# Benchmarking PyTorch: QLayerNormBenchmark
# Mode: Eager
# Name: QLayerNormBenchmark_dims(32,8,16)_dtypetorch.qint8
# Input: dims: (32, 8, 16), dtype: torch.qint8
Run: 0, Forward Execution Time (us) : 29.518
Run: 1, Forward Execution Time (us) : 29.642
Run: 2, Forward Execution Time (us) : 29.700

# Benchmarking PyTorch: QLayerNormBenchmark
# Mode: Eager
# Name: QLayerNormBenchmark_dims(64,128,56,56)_dtypetorch.qint8
# Input: dims: (64, 128, 56, 56), dtype: torch.qint8
Run: 0, Forward Execution Time (us) : 40859.038
Run: 1, Forward Execution Time (us) : 40863.333
Run: 2, Forward Execution Time (us) : 40869.594

+ python -m pt.sum_test --num-runs 3 --iterations 100 --warmup-iterations 10
not use dipu
# ----------------------------------------
# PyTorch/Caffe2 Operator Micro-benchmarks
# ----------------------------------------
# Tag : short

# Benchmarking PyTorch: sum
# Mode: Eager
# Name: sum_R64_V32_dim0_contiguousTrue_cpu
# Input: R: 64, V: 32, dim: 0, contiguous: True, device: cpu
Run: 0, Forward Execution Time (us) : 11.532
Run: 1, Forward Execution Time (us) : 11.429
Run: 2, Forward Execution Time (us) : 11.422

# Benchmarking PyTorch: sum
# Mode: Eager
# Name: sum_R64_V32_dim0_contiguousTrue_cuda
# Input: R: 64, V: 32, dim: 0, contiguous: True, device: cuda
Run: 0, Forward Execution Time (us) : 16.712
Run: 1, Forward Execution Time (us) : 19.829
Run: 2, Forward Execution Time (us) : 19.717

# Benchmarking PyTorch: sum
# Mode: Eager
# Name: sum_R64_V32_dim0_contiguousFalse_cpu
# Input: R: 64, V: 32, dim: 0, contiguous: False, device: cpu
Run: 0, Forward Execution Time (us) : 12.285
Run: 1, Forward Execution Time (us) : 12.452
Run: 2, Forward Execution Time (us) : 12.469

# Benchmarking PyTorch: sum
# Mode: Eager
# Name: sum_R64_V32_dim0_contiguousFalse_cuda
# Input: R: 64, V: 32, dim: 0, contiguous: False, device: cuda
Run: 0, Forward Execution Time (us) : 19.788
Run: 1, Forward Execution Time (us) : 20.114
Run: 2, Forward Execution Time (us) : 19.940

# Benchmarking PyTorch: sum
# Mode: Eager
# Name: sum_R64_V32_dim1_contiguousTrue_cpu
# Input: R: 64, V: 32, dim: 1, contiguous: True, device: cpu
Run: 0, Forward Execution Time (us) : 12.687
Run: 1, Forward Execution Time (us) : 12.857
Run: 2, Forward Execution Time (us) : 12.803

# Benchmarking PyTorch: sum
# Mode: Eager
# Name: sum_R64_V32_dim1_contiguousTrue_cuda
# Input: R: 64, V: 32, dim: 1, contiguous: True, device: cuda
Run: 0, Forward Execution Time (us) : 19.759
Run: 1, Forward Execution Time (us) : 19.890
Run: 2, Forward Execution Time (us) : 19.816

# Benchmarking PyTorch: sum
# Mode: Eager
# Name: sum_R64_V32_dim1_contiguousFalse_cpu
# Input: R: 64, V: 32, dim: 1, contiguous: False, device: cpu
Run: 0, Forward Execution Time (us) : 12.914
Run: 1, Forward Execution Time (us) : 13.002
Run: 2, Forward Execution Time (us) : 13.024

# Benchmarking PyTorch: sum
# Mode: Eager
# Name: sum_R64_V32_dim1_contiguousFalse_cuda
# Input: R: 64, V: 32, dim: 1, contiguous: False, device: cuda
Run: 0, Forward Execution Time (us) : 19.896
Run: 1, Forward Execution Time (us) : 20.068
Run: 2, Forward Execution Time (us) : 20.010

# Benchmarking PyTorch: sum
# Mode: Eager
# Name: sum_R64_V512_dim0_contiguousTrue_cpu
# Input: R: 64, V: 512, dim: 0, contiguous: True, device: cpu
Run: 0, Forward Execution Time (us) : 14.300
Run: 1, Forward Execution Time (us) : 14.432
Run: 2, Forward Execution Time (us) : 15.822

# Benchmarking PyTorch: sum
# Mode: Eager
# Name: sum_R64_V512_dim0_contiguousTrue_cuda
# Input: R: 64, V: 512, dim: 0, contiguous: True, device: cuda
Run: 0, Forward Execution Time (us) : 19.579
Run: 1, Forward Execution Time (us) : 19.721
Run: 2, Forward Execution Time (us) : 19.695

# Benchmarking PyTorch: sum
# Mode: Eager
# Name: sum_R64_V512_dim0_contiguousFalse_cpu
# Input: R: 64, V: 512, dim: 0, contiguous: False, device: cpu
Run: 0, Forward Execution Time (us) : 33.783
Run: 1, Forward Execution Time (us) : 33.911
Run: 2, Forward Execution Time (us) : 33.851

# Benchmarking PyTorch: sum
# Mode: Eager
# Name: sum_R64_V512_dim0_contiguousFalse_cuda
# Input: R: 64, V: 512, dim: 0, contiguous: False, device: cuda
Run: 0, Forward Execution Time (us) : 19.717
Run: 1, Forward Execution Time (us) : 19.911
Run: 2, Forward Execution Time (us) : 19.881

# Benchmarking PyTorch: sum
# Mode: Eager
# Name: sum_R64_V512_dim1_contiguousTrue_cpu
# Input: R: 64, V: 512, dim: 1, contiguous: True, device: cpu
Run: 0, Forward Execution Time (us) : 38.089
Run: 1, Forward Execution Time (us) : 38.181
Run: 2, Forward Execution Time (us) : 38.201

# Benchmarking PyTorch: sum
# Mode: Eager
# Name: sum_R64_V512_dim1_contiguousTrue_cuda
# Input: R: 64, V: 512, dim: 1, contiguous: True, device: cuda
Run: 0, Forward Execution Time (us) : 19.614
Run: 1, Forward Execution Time (us) : 19.810
Run: 2, Forward Execution Time (us) : 19.664

# Benchmarking PyTorch: sum
# Mode: Eager
# Name: sum_R64_V512_dim1_contiguousFalse_cpu
# Input: R: 64, V: 512, dim: 1, contiguous: False, device: cpu
Run: 0, Forward Execution Time (us) : 38.241
Run: 1, Forward Execution Time (us) : 38.294
Run: 2, Forward Execution Time (us) : 38.303

# Benchmarking PyTorch: sum
# Mode: Eager
# Name: sum_R64_V512_dim1_contiguousFalse_cuda
# Input: R: 64, V: 512, dim: 1, contiguous: False, device: cuda
Run: 0, Forward Execution Time (us) : 19.885
Run: 1, Forward Execution Time (us) : 20.058
Run: 2, Forward Execution Time (us) : 20.001

# Benchmarking PyTorch: sum
# Mode: Eager
# Name: sum_R256_V32_dim0_contiguousTrue_cpu
# Input: R: 256, V: 32, dim: 0, contiguous: True, device: cpu
Run: 0, Forward Execution Time (us) : 11.708
Run: 1, Forward Execution Time (us) : 11.822
Run: 2, Forward Execution Time (us) : 11.777

# Benchmarking PyTorch: sum
# Mode: Eager
# Name: sum_R256_V32_dim0_contiguousTrue_cuda
# Input: R: 256, V: 32, dim: 0, contiguous: True, device: cuda
Run: 0, Forward Execution Time (us) : 19.552
Run: 1, Forward Execution Time (us) : 19.612
Run: 2, Forward Execution Time (us) : 19.567

# Benchmarking PyTorch: sum
# Mode: Eager
# Name: sum_R256_V32_dim0_contiguousFalse_cpu
# Input: R: 256, V: 32, dim: 0, contiguous: False, device: cpu
Run: 0, Forward Execution Time (us) : 14.816
Run: 1, Forward Execution Time (us) : 14.859
Run: 2, Forward Execution Time (us) : 14.818

# Benchmarking PyTorch: sum
# Mode: Eager
# Name: sum_R256_V32_dim0_contiguousFalse_cuda
# Input: R: 256, V: 32, dim: 0, contiguous: False, device: cuda
Run: 0, Forward Execution Time (us) : 19.752
Run: 1, Forward Execution Time (us) : 19.859
Run: 2, Forward Execution Time (us) : 19.878

# Benchmarking PyTorch: sum
# Mode: Eager
# Name: sum_R256_V32_dim1_contiguousTrue_cpu
# Input: R: 256, V: 32, dim: 1, contiguous: True, device: cpu
Run: 0, Forward Execution Time (us) : 16.093
Run: 1, Forward Execution Time (us) : 16.230
Run: 2, Forward Execution Time (us) : 16.071

# Benchmarking PyTorch: sum
# Mode: Eager
# Name: sum_R256_V32_dim1_contiguousTrue_cuda
# Input: R: 256, V: 32, dim: 1, contiguous: True, device: cuda
Run: 0, Forward Execution Time (us) : 16.806
Run: 1, Forward Execution Time (us) : 19.824
Run: 2, Forward Execution Time (us) : 19.801

# Benchmarking PyTorch: sum
# Mode: Eager
# Name: sum_R256_V32_dim1_contiguousFalse_cpu
# Input: R: 256, V: 32, dim: 1, contiguous: False, device: cpu
Run: 0, Forward Execution Time (us) : 15.005
Run: 1, Forward Execution Time (us) : 15.074
Run: 2, Forward Execution Time (us) : 15.208

# Benchmarking PyTorch: sum
# Mode: Eager
# Name: sum_R256_V32_dim1_contiguousFalse_cuda
# Input: R: 256, V: 32, dim: 1, contiguous: False, device: cuda
Run: 0, Forward Execution Time (us) : 19.909
Run: 1, Forward Execution Time (us) : 19.980
Run: 2, Forward Execution Time (us) : 20.016

# Benchmarking PyTorch: sum
# Mode: Eager
# Name: sum_R256_V512_dim0_contiguousTrue_cpu
# Input: R: 256, V: 512, dim: 0, contiguous: True, device: cpu
Run: 0, Forward Execution Time (us) : 23.285
Run: 1, Forward Execution Time (us) : 23.365
Run: 2, Forward Execution Time (us) : 23.573

# Benchmarking PyTorch: sum
# Mode: Eager
# Name: sum_R256_V512_dim0_contiguousTrue_cuda
# Input: R: 256, V: 512, dim: 0, contiguous: True, device: cuda
Run: 0, Forward Execution Time (us) : 19.538
Run: 1, Forward Execution Time (us) : 19.592
Run: 2, Forward Execution Time (us) : 19.630

# Benchmarking PyTorch: sum
# Mode: Eager
# Name: sum_R256_V512_dim0_contiguousFalse_cpu
# Input: R: 256, V: 512, dim: 0, contiguous: False, device: cpu
Run: 0, Forward Execution Time (us) : 137.808
Run: 1, Forward Execution Time (us) : 150.008
Run: 2, Forward Execution Time (us) : 149.953

# Benchmarking PyTorch: sum
# Mode: Eager
# Name: sum_R256_V512_dim0_contiguousFalse_cuda
# Input: R: 256, V: 512, dim: 0, contiguous: False, device: cuda
Run: 0, Forward Execution Time (us) : 19.497
Run: 1, Forward Execution Time (us) : 19.720
Run: 2, Forward Execution Time (us) : 19.673

# Benchmarking PyTorch: sum
# Mode: Eager
# Name: sum_R256_V512_dim1_contiguousTrue_cpu
# Input: R: 256, V: 512, dim: 1, contiguous: True, device: cpu
Run: 0, Forward Execution Time (us) : 44.031
Run: 1, Forward Execution Time (us) : 53.989
Run: 2, Forward Execution Time (us) : 53.781

# Benchmarking PyTorch: sum
# Mode: Eager
# Name: sum_R256_V512_dim1_contiguousTrue_cuda
# Input: R: 256, V: 512, dim: 1, contiguous: True, device: cuda
Run: 0, Forward Execution Time (us) : 17.134
Run: 1, Forward Execution Time (us) : 20.238
Run: 2, Forward Execution Time (us) : 20.247

# Benchmarking PyTorch: sum
# Mode: Eager
# Name: sum_R256_V512_dim1_contiguousFalse_cpu
# Input: R: 256, V: 512, dim: 1, contiguous: False, device: cpu
Run: 0, Forward Execution Time (us) : 81.187
Run: 1, Forward Execution Time (us) : 90.530
Run: 2, Forward Execution Time (us) : 93.510

# Benchmarking PyTorch: sum
# Mode: Eager
# Name: sum_R256_V512_dim1_contiguousFalse_cuda
# Input: R: 256, V: 512, dim: 1, contiguous: False, device: cuda
Run: 0, Forward Execution Time (us) : 20.242
Run: 1, Forward Execution Time (us) : 20.368
Run: 2, Forward Execution Time (us) : 20.279

+ python -m pt.instancenorm_test --num-runs 3 --iterations 100 --warmup-iterations 10
not use dipu
# ----------------------------------------
# PyTorch/Caffe2 Operator Micro-benchmarks
# ----------------------------------------
# Tag : short

# Benchmarking PyTorch: InstanceNormBenchmark
# Mode: Eager
# Name: InstanceNormBenchmark_dims(32,8,16)
# Input: dims: (32, 8, 16)
Run: 0, Forward Execution Time (us) : 64.214
Run: 1, Forward Execution Time (us) : 76.449
Run: 2, Forward Execution Time (us) : 75.988

# Benchmarking PyTorch: InstanceNormBenchmark
# Mode: Eager
# Name: InstanceNormBenchmark_dims(32,8,56,56)
# Input: dims: (32, 8, 56, 56)
Run: 0, Forward Execution Time (us) : 2917.748
Run: 1, Forward Execution Time (us) : 2914.559
Run: 2, Forward Execution Time (us) : 2908.464

+ python -m pt.qatembedding_ops_test --num-runs 3 --iterations 100 --warmup-iterations 10
not use dipu
# ----------------------------------------
# PyTorch/Caffe2 Operator Micro-benchmarks
# ----------------------------------------
# Tag : short

# Benchmarking PyTorch: qatEmbeddingBag
# Mode: Eager
# Name: qatEmbeddingBag_embeddingbags10_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetTrue_cpu
# Input: embeddingbags: 10, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: True, device: cpu
Run: 0, Forward Execution Time (us) : 406.771
Run: 1, Forward Execution Time (us) : 482.518
Run: 2, Forward Execution Time (us) : 481.883

# Benchmarking PyTorch: qatEmbeddingBag
# Mode: Eager
# Name: qatEmbeddingBag_embeddingbags10_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetFalse_cpu
# Input: embeddingbags: 10, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: False, device: cpu
Run: 0, Forward Execution Time (us) : 489.847
Run: 1, Forward Execution Time (us) : 489.891
Run: 2, Forward Execution Time (us) : 490.472

# Benchmarking PyTorch: qatEmbeddingBag
# Mode: Eager
# Name: qatEmbeddingBag_embeddingbags10_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetTrue_cpu
# Input: embeddingbags: 10, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: True, device: cpu
Run: 0, Forward Execution Time (us) : 481.434
Run: 1, Forward Execution Time (us) : 481.302
Run: 2, Forward Execution Time (us) : 481.613

# Benchmarking PyTorch: qatEmbeddingBag
# Mode: Eager
# Name: qatEmbeddingBag_embeddingbags10_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetFalse_cpu
# Input: embeddingbags: 10, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: False, device: cpu
Run: 0, Forward Execution Time (us) : 492.230
Run: 1, Forward Execution Time (us) : 491.357
Run: 2, Forward Execution Time (us) : 491.374

# Benchmarking PyTorch: qatEmbeddingBag
# Mode: Eager
# Name: qatEmbeddingBag_embeddingbags10_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetTrue_cpu
# Input: embeddingbags: 10, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: True, device: cpu
Run: 0, Forward Execution Time (us) : 477.484
Run: 1, Forward Execution Time (us) : 479.468
Run: 2, Forward Execution Time (us) : 479.125

# Benchmarking PyTorch: qatEmbeddingBag
# Mode: Eager
# Name: qatEmbeddingBag_embeddingbags10_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetFalse_cpu
# Input: embeddingbags: 10, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: False, device: cpu
Run: 0, Forward Execution Time (us) : 491.956
Run: 1, Forward Execution Time (us) : 492.030
Run: 2, Forward Execution Time (us) : 492.508

# Benchmarking PyTorch: qatEmbeddingBag
# Mode: Eager
# Name: qatEmbeddingBag_embeddingbags120_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetTrue_cpu
# Input: embeddingbags: 120, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: True, device: cpu
Run: 0, Forward Execution Time (us) : 718.397
Run: 1, Forward Execution Time (us) : 717.889
Run: 2, Forward Execution Time (us) : 717.827

# Benchmarking PyTorch: qatEmbeddingBag
# Mode: Eager
# Name: qatEmbeddingBag_embeddingbags120_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetFalse_cpu
# Input: embeddingbags: 120, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: False, device: cpu
Run: 0, Forward Execution Time (us) : 729.925
Run: 1, Forward Execution Time (us) : 730.605
Run: 2, Forward Execution Time (us) : 731.039

# Benchmarking PyTorch: qatEmbeddingBag
# Mode: Eager
# Name: qatEmbeddingBag_embeddingbags120_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetTrue_cpu
# Input: embeddingbags: 120, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: True, device: cpu
Run: 0, Forward Execution Time (us) : 717.033
Run: 1, Forward Execution Time (us) : 723.938
Run: 2, Forward Execution Time (us) : 718.404

# Benchmarking PyTorch: qatEmbeddingBag
# Mode: Eager
# Name: qatEmbeddingBag_embeddingbags120_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetFalse_cpu
# Input: embeddingbags: 120, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: False, device: cpu
Run: 0, Forward Execution Time (us) : 728.551
Run: 1, Forward Execution Time (us) : 729.370
Run: 2, Forward Execution Time (us) : 729.776

# Benchmarking PyTorch: qatEmbeddingBag
# Mode: Eager
# Name: qatEmbeddingBag_embeddingbags120_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetTrue_cpu
# Input: embeddingbags: 120, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: True, device: cpu
Run: 0, Forward Execution Time (us) : 716.776
Run: 1, Forward Execution Time (us) : 718.291
Run: 2, Forward Execution Time (us) : 718.496

# Benchmarking PyTorch: qatEmbeddingBag
# Mode: Eager
# Name: qatEmbeddingBag_embeddingbags120_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetFalse_cpu
# Input: embeddingbags: 120, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: False, device: cpu
Run: 0, Forward Execution Time (us) : 730.026
Run: 1, Forward Execution Time (us) : 729.816
Run: 2, Forward Execution Time (us) : 730.248

# Benchmarking PyTorch: qatEmbeddingBag
# Mode: Eager
# Name: qatEmbeddingBag_embeddingbags1000_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetTrue_cpu
# Input: embeddingbags: 1000, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: True, device: cpu
Run: 0, Forward Execution Time (us) : 2348.060
Run: 1, Forward Execution Time (us) : 2348.397
Run: 2, Forward Execution Time (us) : 2344.640

# Benchmarking PyTorch: qatEmbeddingBag
# Mode: Eager
# Name: qatEmbeddingBag_embeddingbags1000_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetFalse_cpu
# Input: embeddingbags: 1000, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: False, device: cpu
Run: 0, Forward Execution Time (us) : 2357.841
Run: 1, Forward Execution Time (us) : 2357.007
Run: 2, Forward Execution Time (us) : 2390.157

# Benchmarking PyTorch: qatEmbeddingBag
# Mode: Eager
# Name: qatEmbeddingBag_embeddingbags1000_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetTrue_cpu
# Input: embeddingbags: 1000, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: True, device: cpu
Run: 0, Forward Execution Time (us) : 2349.047
Run: 1, Forward Execution Time (us) : 2350.927
Run: 2, Forward Execution Time (us) : 2351.937

# Benchmarking PyTorch: qatEmbeddingBag
# Mode: Eager
# Name: qatEmbeddingBag_embeddingbags1000_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetFalse_cpu
# Input: embeddingbags: 1000, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: False, device: cpu
Run: 0, Forward Execution Time (us) : 2360.716
Run: 1, Forward Execution Time (us) : 2363.519
Run: 2, Forward Execution Time (us) : 2356.623

# Benchmarking PyTorch: qatEmbeddingBag
# Mode: Eager
# Name: qatEmbeddingBag_embeddingbags1000_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetTrue_cpu
# Input: embeddingbags: 1000, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: True, device: cpu
Run: 0, Forward Execution Time (us) : 2349.189
Run: 1, Forward Execution Time (us) : 2349.472
Run: 2, Forward Execution Time (us) : 2349.196

# Benchmarking PyTorch: qatEmbeddingBag
# Mode: Eager
# Name: qatEmbeddingBag_embeddingbags1000_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetFalse_cpu
# Input: embeddingbags: 1000, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: False, device: cpu
Run: 0, Forward Execution Time (us) : 2359.399
Run: 1, Forward Execution Time (us) : 2386.569
Run: 2, Forward Execution Time (us) : 2361.792

# Benchmarking PyTorch: qatEmbeddingBag
# Mode: Eager
# Name: qatEmbeddingBag_embeddingbags2300_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetTrue_cpu
# Input: embeddingbags: 2300, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: True, device: cpu
Run: 0, Forward Execution Time (us) : 4646.342
Run: 1, Forward Execution Time (us) : 4647.396
Run: 2, Forward Execution Time (us) : 4647.539

# Benchmarking PyTorch: qatEmbeddingBag
# Mode: Eager
# Name: qatEmbeddingBag_embeddingbags2300_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetFalse_cpu
# Input: embeddingbags: 2300, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: False, device: cpu
Run: 0, Forward Execution Time (us) : 4652.094
Run: 1, Forward Execution Time (us) : 4652.576
Run: 2, Forward Execution Time (us) : 4654.962

# Benchmarking PyTorch: qatEmbeddingBag
# Mode: Eager
# Name: qatEmbeddingBag_embeddingbags2300_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetTrue_cpu
# Input: embeddingbags: 2300, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: True, device: cpu
Run: 0, Forward Execution Time (us) : 4643.923
Run: 1, Forward Execution Time (us) : 4645.886
Run: 2, Forward Execution Time (us) : 4676.946

# Benchmarking PyTorch: qatEmbeddingBag
# Mode: Eager
# Name: qatEmbeddingBag_embeddingbags2300_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetFalse_cpu
# Input: embeddingbags: 2300, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: False, device: cpu
Run: 0, Forward Execution Time (us) : 4659.868
Run: 1, Forward Execution Time (us) : 4656.403
Run: 2, Forward Execution Time (us) : 4654.597

# Benchmarking PyTorch: qatEmbeddingBag
# Mode: Eager
# Name: qatEmbeddingBag_embeddingbags2300_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetTrue_cpu
# Input: embeddingbags: 2300, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: True, device: cpu
Run: 0, Forward Execution Time (us) : 4646.316
Run: 1, Forward Execution Time (us) : 4648.074
Run: 2, Forward Execution Time (us) : 4647.224

# Benchmarking PyTorch: qatEmbeddingBag
# Mode: Eager
# Name: qatEmbeddingBag_embeddingbags2300_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetFalse_cpu
# Input: embeddingbags: 2300, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: False, device: cpu
Run: 0, Forward Execution Time (us) : 4654.089
Run: 1, Forward Execution Time (us) : 4653.015
Run: 2, Forward Execution Time (us) : 4648.719

# Benchmarking PyTorch: qatEmbeddingBag
# Mode: Eager
# Name: qatEmbeddingBag_embeddingbags10_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetTrue_cpu
# Input: embeddingbags: 10, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: True, device: cpu
Run: 0, Backward Execution Time (us) : 158.498
Run: 1, Backward Execution Time (us) : 158.316
Run: 2, Backward Execution Time (us) : 157.675

# Benchmarking PyTorch: qatEmbeddingBag
# Mode: Eager
# Name: qatEmbeddingBag_embeddingbags10_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetFalse_cpu
# Input: embeddingbags: 10, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: False, device: cpu
Run: 0, Backward Execution Time (us) : 158.929
Run: 1, Backward Execution Time (us) : 157.435
Run: 2, Backward Execution Time (us) : 157.430

# Benchmarking PyTorch: qatEmbeddingBag
# Mode: Eager
# Name: qatEmbeddingBag_embeddingbags10_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetTrue_cpu
# Input: embeddingbags: 10, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: True, device: cpu
Run: 0, Backward Execution Time (us) : 158.983
Run: 1, Backward Execution Time (us) : 159.085
Run: 2, Backward Execution Time (us) : 158.705

# Benchmarking PyTorch: qatEmbeddingBag
# Mode: Eager
# Name: qatEmbeddingBag_embeddingbags10_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetFalse_cpu
# Input: embeddingbags: 10, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: False, device: cpu
Run: 0, Backward Execution Time (us) : 159.041
Run: 1, Backward Execution Time (us) : 158.960
Run: 2, Backward Execution Time (us) : 158.843

# Benchmarking PyTorch: qatEmbeddingBag
# Mode: Eager
# Name: qatEmbeddingBag_embeddingbags10_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetTrue_cpu
# Input: embeddingbags: 10, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: True, device: cpu
Run: 0, Backward Execution Time (us) : 167.792
Run: 1, Backward Execution Time (us) : 168.575
Run: 2, Backward Execution Time (us) : 168.203

# Benchmarking PyTorch: qatEmbeddingBag
# Mode: Eager
# Name: qatEmbeddingBag_embeddingbags10_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetFalse_cpu
# Input: embeddingbags: 10, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: False, device: cpu
Run: 0, Backward Execution Time (us) : 167.652
Run: 1, Backward Execution Time (us) : 168.446
Run: 2, Backward Execution Time (us) : 168.122

# Benchmarking PyTorch: qatEmbeddingBag
# Mode: Eager
# Name: qatEmbeddingBag_embeddingbags120_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetTrue_cpu
# Input: embeddingbags: 120, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: True, device: cpu
Run: 0, Backward Execution Time (us) : 171.049
Run: 1, Backward Execution Time (us) : 171.422
Run: 2, Backward Execution Time (us) : 171.571

# Benchmarking PyTorch: qatEmbeddingBag
# Mode: Eager
# Name: qatEmbeddingBag_embeddingbags120_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetFalse_cpu
# Input: embeddingbags: 120, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: False, device: cpu
Run: 0, Backward Execution Time (us) : 171.460
Run: 1, Backward Execution Time (us) : 170.707
Run: 2, Backward Execution Time (us) : 171.108

# Benchmarking PyTorch: qatEmbeddingBag
# Mode: Eager
# Name: qatEmbeddingBag_embeddingbags120_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetTrue_cpu
# Input: embeddingbags: 120, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: True, device: cpu
Run: 0, Backward Execution Time (us) : 171.879
Run: 1, Backward Execution Time (us) : 172.914
Run: 2, Backward Execution Time (us) : 172.587

# Benchmarking PyTorch: qatEmbeddingBag
# Mode: Eager
# Name: qatEmbeddingBag_embeddingbags120_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetFalse_cpu
# Input: embeddingbags: 120, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: False, device: cpu
Run: 0, Backward Execution Time (us) : 172.452
Run: 1, Backward Execution Time (us) : 173.150
Run: 2, Backward Execution Time (us) : 172.678

# Benchmarking PyTorch: qatEmbeddingBag
# Mode: Eager
# Name: qatEmbeddingBag_embeddingbags120_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetTrue_cpu
# Input: embeddingbags: 120, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: True, device: cpu
Run: 0, Backward Execution Time (us) : 181.756
Run: 1, Backward Execution Time (us) : 182.265
Run: 2, Backward Execution Time (us) : 183.897

# Benchmarking PyTorch: qatEmbeddingBag
# Mode: Eager
# Name: qatEmbeddingBag_embeddingbags120_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetFalse_cpu
# Input: embeddingbags: 120, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: False, device: cpu
Run: 0, Backward Execution Time (us) : 181.870
Run: 1, Backward Execution Time (us) : 182.149
Run: 2, Backward Execution Time (us) : 182.270

# Benchmarking PyTorch: qatEmbeddingBag
# Mode: Eager
# Name: qatEmbeddingBag_embeddingbags1000_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetTrue_cpu
# Input: embeddingbags: 1000, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: True, device: cpu
Run: 0, Backward Execution Time (us) : 305.641
Run: 1, Backward Execution Time (us) : 305.715
Run: 2, Backward Execution Time (us) : 305.124

# Benchmarking PyTorch: qatEmbeddingBag
# Mode: Eager
# Name: qatEmbeddingBag_embeddingbags1000_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetFalse_cpu
# Input: embeddingbags: 1000, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: False, device: cpu
Run: 0, Backward Execution Time (us) : 305.557
Run: 1, Backward Execution Time (us) : 305.833
Run: 2, Backward Execution Time (us) : 305.825

# Benchmarking PyTorch: qatEmbeddingBag
# Mode: Eager
# Name: qatEmbeddingBag_embeddingbags1000_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetTrue_cpu
# Input: embeddingbags: 1000, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: True, device: cpu
Run: 0, Backward Execution Time (us) : 308.849
Run: 1, Backward Execution Time (us) : 308.358
Run: 2, Backward Execution Time (us) : 309.132

# Benchmarking PyTorch: qatEmbeddingBag
# Mode: Eager
# Name: qatEmbeddingBag_embeddingbags1000_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetFalse_cpu
# Input: embeddingbags: 1000, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: False, device: cpu
Run: 0, Backward Execution Time (us) : 307.991
Run: 1, Backward Execution Time (us) : 307.678
Run: 2, Backward Execution Time (us) : 307.818

# Benchmarking PyTorch: qatEmbeddingBag
# Mode: Eager
# Name: qatEmbeddingBag_embeddingbags1000_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetTrue_cpu
# Input: embeddingbags: 1000, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: True, device: cpu
Run: 0, Backward Execution Time (us) : 317.853
Run: 1, Backward Execution Time (us) : 316.876
Run: 2, Backward Execution Time (us) : 317.413

# Benchmarking PyTorch: qatEmbeddingBag
# Mode: Eager
# Name: qatEmbeddingBag_embeddingbags1000_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetFalse_cpu
# Input: embeddingbags: 1000, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: False, device: cpu
Run: 0, Backward Execution Time (us) : 317.845
Run: 1, Backward Execution Time (us) : 317.241
Run: 2, Backward Execution Time (us) : 317.191

# Benchmarking PyTorch: qatEmbeddingBag
# Mode: Eager
# Name: qatEmbeddingBag_embeddingbags2300_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetTrue_cpu
# Input: embeddingbags: 2300, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: True, device: cpu
Run: 0, Backward Execution Time (us) : 508.003
Run: 1, Backward Execution Time (us) : 508.021
Run: 2, Backward Execution Time (us) : 510.411

# Benchmarking PyTorch: qatEmbeddingBag
# Mode: Eager
# Name: qatEmbeddingBag_embeddingbags2300_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetFalse_cpu
# Input: embeddingbags: 2300, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: False, device: cpu
Run: 0, Backward Execution Time (us) : 507.961
Run: 1, Backward Execution Time (us) : 507.454
Run: 2, Backward Execution Time (us) : 507.340

# Benchmarking PyTorch: qatEmbeddingBag
# Mode: Eager
# Name: qatEmbeddingBag_embeddingbags2300_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetTrue_cpu
# Input: embeddingbags: 2300, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: True, device: cpu
Run: 0, Backward Execution Time (us) : 508.182
Run: 1, Backward Execution Time (us) : 509.359
Run: 2, Backward Execution Time (us) : 507.677

# Benchmarking PyTorch: qatEmbeddingBag
# Mode: Eager
# Name: qatEmbeddingBag_embeddingbags2300_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetFalse_cpu
# Input: embeddingbags: 2300, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: False, device: cpu
Run: 0, Backward Execution Time (us) : 509.484
Run: 1, Backward Execution Time (us) : 509.542
Run: 2, Backward Execution Time (us) : 509.513

# Benchmarking PyTorch: qatEmbeddingBag
# Mode: Eager
# Name: qatEmbeddingBag_embeddingbags2300_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetTrue_cpu
# Input: embeddingbags: 2300, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: True, device: cpu
Run: 0, Backward Execution Time (us) : 519.618
Run: 1, Backward Execution Time (us) : 522.208
Run: 2, Backward Execution Time (us) : 519.520

# Benchmarking PyTorch: qatEmbeddingBag
# Mode: Eager
# Name: qatEmbeddingBag_embeddingbags2300_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetFalse_cpu
# Input: embeddingbags: 2300, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: False, device: cpu
Run: 0, Backward Execution Time (us) : 518.608
Run: 1, Backward Execution Time (us) : 519.187
Run: 2, Backward Execution Time (us) : 519.394

# Benchmarking PyTorch: qatEmbedding
# Mode: Eager
# Name: qatEmbedding_num_embeddings10_embedding_dim64_input_size8_cpu
# Input: num_embeddings: 10, embedding_dim: 64, input_size: 8, device: cpu
Run: 0, Forward Execution Time (us) : 453.152
Run: 1, Forward Execution Time (us) : 454.459
Run: 2, Forward Execution Time (us) : 454.862

# Benchmarking PyTorch: qatEmbedding
# Mode: Eager
# Name: qatEmbedding_num_embeddings10_embedding_dim64_input_size16_cpu
# Input: num_embeddings: 10, embedding_dim: 64, input_size: 16, device: cpu
Run: 0, Forward Execution Time (us) : 456.444
Run: 1, Forward Execution Time (us) : 456.974
Run: 2, Forward Execution Time (us) : 458.187

# Benchmarking PyTorch: qatEmbedding
# Mode: Eager
# Name: qatEmbedding_num_embeddings10_embedding_dim64_input_size64_cpu
# Input: num_embeddings: 10, embedding_dim: 64, input_size: 64, device: cpu
Run: 0, Forward Execution Time (us) : 424.142
Run: 1, Forward Execution Time (us) : 457.601
Run: 2, Forward Execution Time (us) : 457.476

# Benchmarking PyTorch: qatEmbedding
# Mode: Eager
# Name: qatEmbedding_num_embeddings120_embedding_dim64_input_size8_cpu
# Input: num_embeddings: 120, embedding_dim: 64, input_size: 8, device: cpu
Run: 0, Forward Execution Time (us) : 690.127
Run: 1, Forward Execution Time (us) : 690.942
Run: 2, Forward Execution Time (us) : 690.135

# Benchmarking PyTorch: qatEmbedding
# Mode: Eager
# Name: qatEmbedding_num_embeddings120_embedding_dim64_input_size16_cpu
# Input: num_embeddings: 120, embedding_dim: 64, input_size: 16, device: cpu
Run: 0, Forward Execution Time (us) : 689.526
Run: 1, Forward Execution Time (us) : 689.909
Run: 2, Forward Execution Time (us) : 691.225

# Benchmarking PyTorch: qatEmbedding
# Mode: Eager
# Name: qatEmbedding_num_embeddings120_embedding_dim64_input_size64_cpu
# Input: num_embeddings: 120, embedding_dim: 64, input_size: 64, device: cpu
Run: 0, Forward Execution Time (us) : 692.617
Run: 1, Forward Execution Time (us) : 694.238
Run: 2, Forward Execution Time (us) : 694.218

# Benchmarking PyTorch: qatEmbedding
# Mode: Eager
# Name: qatEmbedding_num_embeddings1000_embedding_dim64_input_size8_cpu
# Input: num_embeddings: 1000, embedding_dim: 64, input_size: 8, device: cpu
Run: 0, Forward Execution Time (us) : 2318.761
Run: 1, Forward Execution Time (us) : 2321.692
Run: 2, Forward Execution Time (us) : 2320.812

# Benchmarking PyTorch: qatEmbedding
# Mode: Eager
# Name: qatEmbedding_num_embeddings1000_embedding_dim64_input_size16_cpu
# Input: num_embeddings: 1000, embedding_dim: 64, input_size: 16, device: cpu
Run: 0, Forward Execution Time (us) : 2320.863
Run: 1, Forward Execution Time (us) : 2322.650
Run: 2, Forward Execution Time (us) : 2323.182

# Benchmarking PyTorch: qatEmbedding
# Mode: Eager
# Name: qatEmbedding_num_embeddings1000_embedding_dim64_input_size64_cpu
# Input: num_embeddings: 1000, embedding_dim: 64, input_size: 64, device: cpu
Run: 0, Forward Execution Time (us) : 2324.980
Run: 1, Forward Execution Time (us) : 2326.832
Run: 2, Forward Execution Time (us) : 2325.153

# Benchmarking PyTorch: qatEmbedding
# Mode: Eager
# Name: qatEmbedding_num_embeddings2300_embedding_dim64_input_size8_cpu
# Input: num_embeddings: 2300, embedding_dim: 64, input_size: 8, device: cpu
Run: 0, Forward Execution Time (us) : 4616.869
Run: 1, Forward Execution Time (us) : 4618.188
Run: 2, Forward Execution Time (us) : 4618.722

# Benchmarking PyTorch: qatEmbedding
# Mode: Eager
# Name: qatEmbedding_num_embeddings2300_embedding_dim64_input_size16_cpu
# Input: num_embeddings: 2300, embedding_dim: 64, input_size: 16, device: cpu
Run: 0, Forward Execution Time (us) : 4602.239
Run: 1, Forward Execution Time (us) : 4617.963
Run: 2, Forward Execution Time (us) : 4618.428

# Benchmarking PyTorch: qatEmbedding
# Mode: Eager
# Name: qatEmbedding_num_embeddings2300_embedding_dim64_input_size64_cpu
# Input: num_embeddings: 2300, embedding_dim: 64, input_size: 64, device: cpu
Run: 0, Forward Execution Time (us) : 4624.803
Run: 1, Forward Execution Time (us) : 4625.018
Run: 2, Forward Execution Time (us) : 4603.553

# Benchmarking PyTorch: qatEmbedding
# Mode: Eager
# Name: qatEmbedding_num_embeddings10_embedding_dim64_input_size8_cpu
# Input: num_embeddings: 10, embedding_dim: 64, input_size: 8, device: cpu
Run: 0, Backward Execution Time (us) : 112.661
Run: 1, Backward Execution Time (us) : 112.647
Run: 2, Backward Execution Time (us) : 113.633

# Benchmarking PyTorch: qatEmbedding
# Mode: Eager
# Name: qatEmbedding_num_embeddings10_embedding_dim64_input_size16_cpu
# Input: num_embeddings: 10, embedding_dim: 64, input_size: 16, device: cpu
Run: 0, Backward Execution Time (us) : 114.883
Run: 1, Backward Execution Time (us) : 114.925
Run: 2, Backward Execution Time (us) : 114.564

# Benchmarking PyTorch: qatEmbedding
# Mode: Eager
# Name: qatEmbedding_num_embeddings10_embedding_dim64_input_size64_cpu
# Input: num_embeddings: 10, embedding_dim: 64, input_size: 64, device: cpu
Run: 0, Backward Execution Time (us) : 126.863
Run: 1, Backward Execution Time (us) : 126.639
Run: 2, Backward Execution Time (us) : 126.639

# Benchmarking PyTorch: qatEmbedding
# Mode: Eager
# Name: qatEmbedding_num_embeddings120_embedding_dim64_input_size8_cpu
# Input: num_embeddings: 120, embedding_dim: 64, input_size: 8, device: cpu
Run: 0, Backward Execution Time (us) : 123.871
Run: 1, Backward Execution Time (us) : 123.731
Run: 2, Backward Execution Time (us) : 124.072

# Benchmarking PyTorch: qatEmbedding
# Mode: Eager
# Name: qatEmbedding_num_embeddings120_embedding_dim64_input_size16_cpu
# Input: num_embeddings: 120, embedding_dim: 64, input_size: 16, device: cpu
Run: 0, Backward Execution Time (us) : 125.715
Run: 1, Backward Execution Time (us) : 125.867
Run: 2, Backward Execution Time (us) : 125.784

# Benchmarking PyTorch: qatEmbedding
# Mode: Eager
# Name: qatEmbedding_num_embeddings120_embedding_dim64_input_size64_cpu
# Input: num_embeddings: 120, embedding_dim: 64, input_size: 64, device: cpu
Run: 0, Backward Execution Time (us) : 137.755
Run: 1, Backward Execution Time (us) : 137.802
Run: 2, Backward Execution Time (us) : 137.942

# Benchmarking PyTorch: qatEmbedding
# Mode: Eager
# Name: qatEmbedding_num_embeddings1000_embedding_dim64_input_size8_cpu
# Input: num_embeddings: 1000, embedding_dim: 64, input_size: 8, device: cpu
Run: 0, Backward Execution Time (us) : 249.867
Run: 1, Backward Execution Time (us) : 248.917
Run: 2, Backward Execution Time (us) : 249.744

# Benchmarking PyTorch: qatEmbedding
# Mode: Eager
# Name: qatEmbedding_num_embeddings1000_embedding_dim64_input_size16_cpu
# Input: num_embeddings: 1000, embedding_dim: 64, input_size: 16, device: cpu
Run: 0, Backward Execution Time (us) : 250.474
Run: 1, Backward Execution Time (us) : 251.287
Run: 2, Backward Execution Time (us) : 251.315

# Benchmarking PyTorch: qatEmbedding
# Mode: Eager
# Name: qatEmbedding_num_embeddings1000_embedding_dim64_input_size64_cpu
# Input: num_embeddings: 1000, embedding_dim: 64, input_size: 64, device: cpu
Run: 0, Backward Execution Time (us) : 262.849
Run: 1, Backward Execution Time (us) : 263.810
Run: 2, Backward Execution Time (us) : 263.934

# Benchmarking PyTorch: qatEmbedding
# Mode: Eager
# Name: qatEmbedding_num_embeddings2300_embedding_dim64_input_size8_cpu
# Input: num_embeddings: 2300, embedding_dim: 64, input_size: 8, device: cpu
Run: 0, Backward Execution Time (us) : 451.723
Run: 1, Backward Execution Time (us) : 451.637
Run: 2, Backward Execution Time (us) : 450.831

# Benchmarking PyTorch: qatEmbedding
# Mode: Eager
# Name: qatEmbedding_num_embeddings2300_embedding_dim64_input_size16_cpu
# Input: num_embeddings: 2300, embedding_dim: 64, input_size: 16, device: cpu
Run: 0, Backward Execution Time (us) : 454.145
Run: 1, Backward Execution Time (us) : 456.411
Run: 2, Backward Execution Time (us) : 453.812

# Benchmarking PyTorch: qatEmbedding
# Mode: Eager
# Name: qatEmbedding_num_embeddings2300_embedding_dim64_input_size64_cpu
# Input: num_embeddings: 2300, embedding_dim: 64, input_size: 64, device: cpu
Run: 0, Backward Execution Time (us) : 466.449
Run: 1, Backward Execution Time (us) : 465.344
Run: 2, Backward Execution Time (us) : 465.081

+ python -m pt.tensor_to_test --num-runs 3 --iterations 100 --warmup-iterations 10
not use dipu
# ----------------------------------------
# PyTorch/Caffe2 Operator Micro-benchmarks
# ----------------------------------------
# Tag : short

# Benchmarking PyTorch: FloatToHalfTensorConversionBenchmark
# Mode: Eager
# Name: FloatToHalfTensorConversionBenchmark_M8_N16_cpu
# Input: M: 8, N: 16, device: cpu
Run: 0, Forward Execution Time (us) : 8.173
Run: 1, Forward Execution Time (us) : 9.659
Run: 2, Forward Execution Time (us) : 9.538

# Benchmarking PyTorch: FloatToHalfTensorConversionBenchmark
# Mode: Eager
# Name: FloatToHalfTensorConversionBenchmark_M8_N16_cuda
# Input: M: 8, N: 16, device: cuda
Run: 0, Forward Execution Time (us) : 15.583
Run: 1, Forward Execution Time (us) : 18.467
Run: 2, Forward Execution Time (us) : 18.319

# Benchmarking PyTorch: FloatToHalfTensorConversionBenchmark
# Mode: Eager
# Name: FloatToHalfTensorConversionBenchmark_M8_N64_cpu
# Input: M: 8, N: 64, device: cpu
Run: 0, Forward Execution Time (us) : 11.267
Run: 1, Forward Execution Time (us) : 11.348
Run: 2, Forward Execution Time (us) : 11.348

# Benchmarking PyTorch: FloatToHalfTensorConversionBenchmark
# Mode: Eager
# Name: FloatToHalfTensorConversionBenchmark_M8_N64_cuda
# Input: M: 8, N: 64, device: cuda
Run: 0, Forward Execution Time (us) : 18.155
Run: 1, Forward Execution Time (us) : 18.418
Run: 2, Forward Execution Time (us) : 18.362

# Benchmarking PyTorch: FloatToHalfTensorConversionBenchmark
# Mode: Eager
# Name: FloatToHalfTensorConversionBenchmark_M8_N128_cpu
# Input: M: 8, N: 128, device: cpu
Run: 0, Forward Execution Time (us) : 13.406
Run: 1, Forward Execution Time (us) : 13.404
Run: 2, Forward Execution Time (us) : 13.518

# Benchmarking PyTorch: FloatToHalfTensorConversionBenchmark
# Mode: Eager
# Name: FloatToHalfTensorConversionBenchmark_M8_N128_cuda
# Input: M: 8, N: 128, device: cuda
Run: 0, Forward Execution Time (us) : 18.167
Run: 1, Forward Execution Time (us) : 18.267
Run: 2, Forward Execution Time (us) : 18.244

# Benchmarking PyTorch: FloatToHalfTensorConversionBenchmark
# Mode: Eager
# Name: FloatToHalfTensorConversionBenchmark_M16_N16_cpu
# Input: M: 16, N: 16, device: cpu
Run: 0, Forward Execution Time (us) : 10.086
Run: 1, Forward Execution Time (us) : 10.048
Run: 2, Forward Execution Time (us) : 10.040

# Benchmarking PyTorch: FloatToHalfTensorConversionBenchmark
# Mode: Eager
# Name: FloatToHalfTensorConversionBenchmark_M16_N16_cuda
# Input: M: 16, N: 16, device: cuda
Run: 0, Forward Execution Time (us) : 18.096
Run: 1, Forward Execution Time (us) : 18.324
Run: 2, Forward Execution Time (us) : 18.280

# Benchmarking PyTorch: FloatToHalfTensorConversionBenchmark
# Mode: Eager
# Name: FloatToHalfTensorConversionBenchmark_M16_N64_cpu
# Input: M: 16, N: 64, device: cpu
Run: 0, Forward Execution Time (us) : 13.432
Run: 1, Forward Execution Time (us) : 13.467
Run: 2, Forward Execution Time (us) : 13.661

# Benchmarking PyTorch: FloatToHalfTensorConversionBenchmark
# Mode: Eager
# Name: FloatToHalfTensorConversionBenchmark_M16_N64_cuda
# Input: M: 16, N: 64, device: cuda
Run: 0, Forward Execution Time (us) : 18.062
Run: 1, Forward Execution Time (us) : 18.332
Run: 2, Forward Execution Time (us) : 18.230

# Benchmarking PyTorch: FloatToHalfTensorConversionBenchmark
# Mode: Eager
# Name: FloatToHalfTensorConversionBenchmark_M16_N128_cpu
# Input: M: 16, N: 128, device: cpu
Run: 0, Forward Execution Time (us) : 17.611
Run: 1, Forward Execution Time (us) : 17.679
Run: 2, Forward Execution Time (us) : 17.649

# Benchmarking PyTorch: FloatToHalfTensorConversionBenchmark
# Mode: Eager
# Name: FloatToHalfTensorConversionBenchmark_M16_N128_cuda
# Input: M: 16, N: 128, device: cuda
Run: 0, Forward Execution Time (us) : 18.096
Run: 1, Forward Execution Time (us) : 18.354
Run: 2, Forward Execution Time (us) : 18.393

# Benchmarking PyTorch: FloatToHalfTensorConversionBenchmark
# Mode: Eager
# Name: FloatToHalfTensorConversionBenchmark_M32_N16_cpu
# Input: M: 32, N: 16, device: cpu
Run: 0, Forward Execution Time (us) : 11.238
Run: 1, Forward Execution Time (us) : 11.290
Run: 2, Forward Execution Time (us) : 11.331

# Benchmarking PyTorch: FloatToHalfTensorConversionBenchmark
# Mode: Eager
# Name: FloatToHalfTensorConversionBenchmark_M32_N16_cuda
# Input: M: 32, N: 16, device: cuda
Run: 0, Forward Execution Time (us) : 18.017
Run: 1, Forward Execution Time (us) : 18.278
Run: 2, Forward Execution Time (us) : 18.235

# Benchmarking PyTorch: FloatToHalfTensorConversionBenchmark
# Mode: Eager
# Name: FloatToHalfTensorConversionBenchmark_M32_N64_cpu
# Input: M: 32, N: 64, device: cpu
Run: 0, Forward Execution Time (us) : 17.612
Run: 1, Forward Execution Time (us) : 17.680
Run: 2, Forward Execution Time (us) : 17.643

# Benchmarking PyTorch: FloatToHalfTensorConversionBenchmark
# Mode: Eager
# Name: FloatToHalfTensorConversionBenchmark_M32_N64_cuda
# Input: M: 32, N: 64, device: cuda
Run: 0, Forward Execution Time (us) : 18.106
Run: 1, Forward Execution Time (us) : 18.367
Run: 2, Forward Execution Time (us) : 18.330

# Benchmarking PyTorch: FloatToHalfTensorConversionBenchmark
# Mode: Eager
# Name: FloatToHalfTensorConversionBenchmark_M32_N128_cpu
# Input: M: 32, N: 128, device: cpu
Run: 0, Forward Execution Time (us) : 25.962
Run: 1, Forward Execution Time (us) : 26.085
Run: 2, Forward Execution Time (us) : 25.946

# Benchmarking PyTorch: FloatToHalfTensorConversionBenchmark
# Mode: Eager
# Name: FloatToHalfTensorConversionBenchmark_M32_N128_cuda
# Input: M: 32, N: 128, device: cuda
Run: 0, Forward Execution Time (us) : 18.169
Run: 1, Forward Execution Time (us) : 18.341
Run: 2, Forward Execution Time (us) : 18.297

# Benchmarking PyTorch: HalfToFloatTensorConversionBenchmark
# Mode: Eager
# Name: HalfToFloatTensorConversionBenchmark_M8_N16_cpu
# Input: M: 8, N: 16, device: cpu
Run: 0, Forward Execution Time (us) : 7.805
Run: 1, Forward Execution Time (us) : 9.347
Run: 2, Forward Execution Time (us) : 9.317

# Benchmarking PyTorch: HalfToFloatTensorConversionBenchmark
# Mode: Eager
# Name: HalfToFloatTensorConversionBenchmark_M8_N16_cuda
# Input: M: 8, N: 16, device: cuda
Run: 0, Forward Execution Time (us) : 18.195
Run: 1, Forward Execution Time (us) : 18.428
Run: 2, Forward Execution Time (us) : 18.352

# Benchmarking PyTorch: HalfToFloatTensorConversionBenchmark
# Mode: Eager
# Name: HalfToFloatTensorConversionBenchmark_M8_N64_cpu
# Input: M: 8, N: 64, device: cpu
Run: 0, Forward Execution Time (us) : 10.107
Run: 1, Forward Execution Time (us) : 10.228
Run: 2, Forward Execution Time (us) : 10.187

# Benchmarking PyTorch: HalfToFloatTensorConversionBenchmark
# Mode: Eager
# Name: HalfToFloatTensorConversionBenchmark_M8_N64_cuda
# Input: M: 8, N: 64, device: cuda
Run: 0, Forward Execution Time (us) : 18.299
Run: 1, Forward Execution Time (us) : 18.250
Run: 2, Forward Execution Time (us) : 18.277

# Benchmarking PyTorch: HalfToFloatTensorConversionBenchmark
# Mode: Eager
# Name: HalfToFloatTensorConversionBenchmark_M8_N128_cpu
# Input: M: 8, N: 128, device: cpu
Run: 0, Forward Execution Time (us) : 11.198
Run: 1, Forward Execution Time (us) : 11.268
Run: 2, Forward Execution Time (us) : 11.233

# Benchmarking PyTorch: HalfToFloatTensorConversionBenchmark
# Mode: Eager
# Name: HalfToFloatTensorConversionBenchmark_M8_N128_cuda
# Input: M: 8, N: 128, device: cuda
Run: 0, Forward Execution Time (us) : 18.186
Run: 1, Forward Execution Time (us) : 18.353
Run: 2, Forward Execution Time (us) : 18.298

# Benchmarking PyTorch: HalfToFloatTensorConversionBenchmark
# Mode: Eager
# Name: HalfToFloatTensorConversionBenchmark_M16_N16_cpu
# Input: M: 16, N: 16, device: cpu
Run: 0, Forward Execution Time (us) : 9.616
Run: 1, Forward Execution Time (us) : 9.731
Run: 2, Forward Execution Time (us) : 9.665

# Benchmarking PyTorch: HalfToFloatTensorConversionBenchmark
# Mode: Eager
# Name: HalfToFloatTensorConversionBenchmark_M16_N16_cuda
# Input: M: 16, N: 16, device: cuda
Run: 0, Forward Execution Time (us) : 18.088
Run: 1, Forward Execution Time (us) : 18.358
Run: 2, Forward Execution Time (us) : 18.382

# Benchmarking PyTorch: HalfToFloatTensorConversionBenchmark
# Mode: Eager
# Name: HalfToFloatTensorConversionBenchmark_M16_N64_cpu
# Input: M: 16, N: 64, device: cpu
Run: 0, Forward Execution Time (us) : 11.146
Run: 1, Forward Execution Time (us) : 11.164
Run: 2, Forward Execution Time (us) : 11.258

# Benchmarking PyTorch: HalfToFloatTensorConversionBenchmark
# Mode: Eager
# Name: HalfToFloatTensorConversionBenchmark_M16_N64_cuda
# Input: M: 16, N: 64, device: cuda
Run: 0, Forward Execution Time (us) : 18.135
Run: 1, Forward Execution Time (us) : 18.318
Run: 2, Forward Execution Time (us) : 18.234

# Benchmarking PyTorch: HalfToFloatTensorConversionBenchmark
# Mode: Eager
# Name: HalfToFloatTensorConversionBenchmark_M16_N128_cpu
# Input: M: 16, N: 128, device: cpu
Run: 0, Forward Execution Time (us) : 13.271
Run: 1, Forward Execution Time (us) : 13.409
Run: 2, Forward Execution Time (us) : 13.339

# Benchmarking PyTorch: HalfToFloatTensorConversionBenchmark
# Mode: Eager
# Name: HalfToFloatTensorConversionBenchmark_M16_N128_cuda
# Input: M: 16, N: 128, device: cuda
Run: 0, Forward Execution Time (us) : 18.185
Run: 1, Forward Execution Time (us) : 18.328
Run: 2, Forward Execution Time (us) : 18.249

# Benchmarking PyTorch: HalfToFloatTensorConversionBenchmark
# Mode: Eager
# Name: HalfToFloatTensorConversionBenchmark_M32_N16_cpu
# Input: M: 32, N: 16, device: cpu
Run: 0, Forward Execution Time (us) : 10.182
Run: 1, Forward Execution Time (us) : 10.186
Run: 2, Forward Execution Time (us) : 10.165

# Benchmarking PyTorch: HalfToFloatTensorConversionBenchmark
# Mode: Eager
# Name: HalfToFloatTensorConversionBenchmark_M32_N16_cuda
# Input: M: 32, N: 16, device: cuda
Run: 0, Forward Execution Time (us) : 18.056
Run: 1, Forward Execution Time (us) : 18.283
Run: 2, Forward Execution Time (us) : 18.230

# Benchmarking PyTorch: HalfToFloatTensorConversionBenchmark
# Mode: Eager
# Name: HalfToFloatTensorConversionBenchmark_M32_N64_cpu
# Input: M: 32, N: 64, device: cpu
Run: 0, Forward Execution Time (us) : 13.303
Run: 1, Forward Execution Time (us) : 13.377
Run: 2, Forward Execution Time (us) : 13.379

# Benchmarking PyTorch: HalfToFloatTensorConversionBenchmark
# Mode: Eager
# Name: HalfToFloatTensorConversionBenchmark_M32_N64_cuda
# Input: M: 32, N: 64, device: cuda
Run: 0, Forward Execution Time (us) : 18.110
Run: 1, Forward Execution Time (us) : 18.392
Run: 2, Forward Execution Time (us) : 18.396

# Benchmarking PyTorch: HalfToFloatTensorConversionBenchmark
# Mode: Eager
# Name: HalfToFloatTensorConversionBenchmark_M32_N128_cpu
# Input: M: 32, N: 128, device: cpu
Run: 0, Forward Execution Time (us) : 17.487
Run: 1, Forward Execution Time (us) : 17.531
Run: 2, Forward Execution Time (us) : 17.526

# Benchmarking PyTorch: HalfToFloatTensorConversionBenchmark
# Mode: Eager
# Name: HalfToFloatTensorConversionBenchmark_M32_N128_cuda
# Input: M: 32, N: 128, device: cuda
Run: 0, Forward Execution Time (us) : 18.079
Run: 1, Forward Execution Time (us) : 18.391
Run: 2, Forward Execution Time (us) : 18.303

+ python -m pt.conv_test --num-runs 3 --iterations 100 --warmup-iterations 10
not use dipu
# ----------------------------------------
# PyTorch/Caffe2 Operator Micro-benchmarks
# ----------------------------------------
# Tag : short

# Benchmarking PyTorch: Conv1d
# Mode: Eager
# Name: Conv1d_IC128_OC256_kernel3_stride1_N1_L64_cpu
# Input: IC: 128, OC: 256, kernel: 3, stride: 1, N: 1, L: 64, device: cpu
Run: 0, Forward Execution Time (us) : 355.626
Run: 1, Forward Execution Time (us) : 356.111
Run: 2, Forward Execution Time (us) : 359.497

# Benchmarking PyTorch: Conv1d
# Mode: Eager
# Name: Conv1d_IC128_OC256_kernel3_stride1_N1_L64_cuda
# Input: IC: 128, OC: 256, kernel: 3, stride: 1, N: 1, L: 64, device: cuda
Run: 0, Forward Execution Time (us) : 57.194
Run: 1, Forward Execution Time (us) : 66.733
Run: 2, Forward Execution Time (us) : 66.596

# Benchmarking PyTorch: Conv1d
# Mode: Eager
# Name: Conv1d_IC256_OC256_kernel3_stride2_N4_L64_cpu
# Input: IC: 256, OC: 256, kernel: 3, stride: 2, N: 4, L: 64, device: cpu
Run: 0, Forward Execution Time (us) : 2064.117
Run: 1, Forward Execution Time (us) : 2104.394
Run: 2, Forward Execution Time (us) : 2065.805

# Benchmarking PyTorch: Conv1d
# Mode: Eager
# Name: Conv1d_IC256_OC256_kernel3_stride2_N4_L64_cuda
# Input: IC: 256, OC: 256, kernel: 3, stride: 2, N: 4, L: 64, device: cuda
Run: 0, Forward Execution Time (us) : 71.990
Run: 1, Forward Execution Time (us) : 83.593
Run: 2, Forward Execution Time (us) : 83.558

# Benchmarking PyTorch: ConvTranspose1d
# Mode: Eager
# Name: ConvTranspose1d_IC128_OC256_kernel3_stride1_N1_L64_cpu
# Input: IC: 128, OC: 256, kernel: 3, stride: 1, N: 1, L: 64, device: cpu
Run: 0, Forward Execution Time (us) : 461.415
Run: 1, Forward Execution Time (us) : 461.796
Run: 2, Forward Execution Time (us) : 461.794

# Benchmarking PyTorch: ConvTranspose1d
# Mode: Eager
# Name: ConvTranspose1d_IC128_OC256_kernel3_stride1_N1_L64_cuda
# Input: IC: 128, OC: 256, kernel: 3, stride: 1, N: 1, L: 64, device: cuda
Run: 0, Forward Execution Time (us) : 58.274
Run: 1, Forward Execution Time (us) : 68.270
Run: 2, Forward Execution Time (us) : 68.424

# Benchmarking PyTorch: ConvTranspose1d
# Mode: Eager
# Name: ConvTranspose1d_IC256_OC256_kernel3_stride2_N4_L64_cpu
# Input: IC: 256, OC: 256, kernel: 3, stride: 2, N: 4, L: 64, device: cpu
Run: 0, Forward Execution Time (us) : 2732.649
Run: 1, Forward Execution Time (us) : 2785.652
Run: 2, Forward Execution Time (us) : 2731.784

# Benchmarking PyTorch: ConvTranspose1d
# Mode: Eager
# Name: ConvTranspose1d_IC256_OC256_kernel3_stride2_N4_L64_cuda
# Input: IC: 256, OC: 256, kernel: 3, stride: 2, N: 4, L: 64, device: cuda
Run: 0, Forward Execution Time (us) : 58.470
Run: 1, Forward Execution Time (us) : 68.721
Run: 2, Forward Execution Time (us) : 68.715

# Benchmarking PyTorch: Conv2d
# Mode: Eager
# Name: Conv2d_IC256_OC256_kernel3_stride1_N1_H16_W16_G1_pad0_cpu
# Input: IC: 256, OC: 256, kernel: 3, stride: 1, N: 1, H: 16, W: 16, G: 1, pad: 0, device: cpu
Run: 0, Forward Execution Time (us) : 4525.754
Run: 1, Forward Execution Time (us) : 4575.882
Run: 2, Forward Execution Time (us) : 4522.661

# Benchmarking PyTorch: Conv2d
# Mode: Eager
# Name: Conv2d_IC256_OC256_kernel3_stride1_N1_H16_W16_G1_pad0_cuda
# Input: IC: 256, OC: 256, kernel: 3, stride: 1, N: 1, H: 16, W: 16, G: 1, pad: 0, device: cuda
Run: 0, Forward Execution Time (us) : 77.919
Run: 1, Forward Execution Time (us) : 78.382
Run: 2, Forward Execution Time (us) : 78.235

# Benchmarking PyTorch: ConvTranspose2d
# Mode: Eager
# Name: ConvTranspose2d_IC256_OC256_kernel3_stride1_N1_H16_W16_G1_pad0_cpu
# Input: IC: 256, OC: 256, kernel: 3, stride: 1, N: 1, H: 16, W: 16, G: 1, pad: 0, device: cpu
Run: 0, Forward Execution Time (us) : 5650.564
Run: 1, Forward Execution Time (us) : 5715.307
Run: 2, Forward Execution Time (us) : 5724.098

# Benchmarking PyTorch: ConvTranspose2d
# Mode: Eager
# Name: ConvTranspose2d_IC256_OC256_kernel3_stride1_N1_H16_W16_G1_pad0_cuda
# Input: IC: 256, OC: 256, kernel: 3, stride: 1, N: 1, H: 16, W: 16, G: 1, pad: 0, device: cuda
Run: 0, Forward Execution Time (us) : 69.487
Run: 1, Forward Execution Time (us) : 70.574
Run: 2, Forward Execution Time (us) : 69.566

# Benchmarking PyTorch: Conv2dPointwise
# Mode: Eager
# Name: Conv2dPointwise_IC256_OC256_stride1_N1_H16_W16_G1_pad0_cpu
# Input: IC: 256, OC: 256, stride: 1, N: 1, H: 16, W: 16, G: 1, pad: 0, device: cpu
Run: 0, Forward Execution Time (us) : 547.168
Run: 1, Forward Execution Time (us) : 544.200
Run: 2, Forward Execution Time (us) : 544.179

# Benchmarking PyTorch: Conv2dPointwise
# Mode: Eager
# Name: Conv2dPointwise_IC256_OC256_stride1_N1_H16_W16_G1_pad0_cuda
# Input: IC: 256, OC: 256, stride: 1, N: 1, H: 16, W: 16, G: 1, pad: 0, device: cuda
Run: 0, Forward Execution Time (us) : 62.216
Run: 1, Forward Execution Time (us) : 62.503
Run: 2, Forward Execution Time (us) : 62.439

# Benchmarking PyTorch: Conv3d
# Mode: Eager
# Name: Conv3d_IC64_OC64_kernel3_stride1_N8_D4_H16_W16_cpu
# Input: IC: 64, OC: 64, kernel: 3, stride: 1, N: 8, D: 4, H: 16, W: 16, device: cpu
Run: 0, Forward Execution Time (us) : 25208.371
Run: 1, Forward Execution Time (us) : 26869.515
Run: 2, Forward Execution Time (us) : 26924.623

# Benchmarking PyTorch: Conv3d
# Mode: Eager
# Name: Conv3d_IC64_OC64_kernel3_stride1_N8_D4_H16_W16_cuda
# Input: IC: 64, OC: 64, kernel: 3, stride: 1, N: 8, D: 4, H: 16, W: 16, device: cuda
Run: 0, Forward Execution Time (us) : 102.531
Run: 1, Forward Execution Time (us) : 102.718
Run: 2, Forward Execution Time (us) : 102.623

# Benchmarking PyTorch: ConvTranspose3d
# Mode: Eager
# Name: ConvTranspose3d_IC64_OC64_kernel3_stride1_N8_D4_H16_W16_cpu
# Input: IC: 64, OC: 64, kernel: 3, stride: 1, N: 8, D: 4, H: 16, W: 16, device: cpu
Run: 0, Forward Execution Time (us) : 57091.351
Run: 1, Forward Execution Time (us) : 56939.524
Run: 2, Forward Execution Time (us) : 56751.894

# Benchmarking PyTorch: ConvTranspose3d
# Mode: Eager
# Name: ConvTranspose3d_IC64_OC64_kernel3_stride1_N8_D4_H16_W16_cuda
# Input: IC: 64, OC: 64, kernel: 3, stride: 1, N: 8, D: 4, H: 16, W: 16, device: cuda
Run: 0, Forward Execution Time (us) : 366.002
Run: 1, Forward Execution Time (us) : 365.051
Run: 2, Forward Execution Time (us) : 365.895

+ python -m pt.interpolate_test --num-runs 3 --iterations 100 --warmup-iterations 10
not use dipu
# ----------------------------------------
# PyTorch/Caffe2 Operator Micro-benchmarks
# ----------------------------------------
# Tag : short

# Benchmarking PyTorch: interpolate
# Mode: Eager
# Name: interpolate_input_size(1,3,60,40)_output_size(24,24)_channels_lastTrue_modenearest
# Input: input_size: (1, 3, 60, 40), output_size: (24, 24), channels_last: True, mode: nearest
Run: 0, Forward Execution Time (us) : 25.002
Run: 1, Forward Execution Time (us) : 27.111
Run: 2, Forward Execution Time (us) : 26.956

# Benchmarking PyTorch: interpolate
# Mode: Eager
# Name: interpolate_input_size(1,3,60,40)_output_size(24,24)_channels_lastTrue_modelinear
# Input: input_size: (1, 3, 60, 40), output_size: (24, 24), channels_last: True, mode: linear
Run: 0, Forward Execution Time (us) : 30.219
Run: 1, Forward Execution Time (us) : 29.763
Run: 2, Forward Execution Time (us) : 29.620

# Benchmarking PyTorch: interpolate
# Mode: Eager
# Name: interpolate_input_size(1,3,60,40)_output_size(24,24)_channels_lastTrue_modebicubic
# Input: input_size: (1, 3, 60, 40), output_size: (24, 24), channels_last: True, mode: bicubic
Run: 0, Forward Execution Time (us) : 108.150
Run: 1, Forward Execution Time (us) : 109.841
Run: 2, Forward Execution Time (us) : 109.390

# Benchmarking PyTorch: interpolate
# Mode: Eager
# Name: interpolate_input_size(1,3,60,40)_output_size(24,24)_channels_lastFalse_modenearest
# Input: input_size: (1, 3, 60, 40), output_size: (24, 24), channels_last: False, mode: nearest
Run: 0, Forward Execution Time (us) : 53.315
Run: 1, Forward Execution Time (us) : 53.447
Run: 2, Forward Execution Time (us) : 53.435

# Benchmarking PyTorch: interpolate
# Mode: Eager
# Name: interpolate_input_size(1,3,60,40)_output_size(24,24)_channels_lastFalse_modelinear
# Input: input_size: (1, 3, 60, 40), output_size: (24, 24), channels_last: False, mode: linear
Run: 0, Forward Execution Time (us) : 57.670
Run: 1, Forward Execution Time (us) : 57.725
Run: 2, Forward Execution Time (us) : 57.752

# Benchmarking PyTorch: interpolate
# Mode: Eager
# Name: interpolate_input_size(1,3,60,40)_output_size(24,24)_channels_lastFalse_modebicubic
# Input: input_size: (1, 3, 60, 40), output_size: (24, 24), channels_last: False, mode: bicubic
Run: 0, Forward Execution Time (us) : 90.116
Run: 1, Forward Execution Time (us) : 92.245
Run: 2, Forward Execution Time (us) : 91.394

# Benchmarking PyTorch: interpolate
# Mode: Eager
# Name: interpolate_input_size(1,3,600,400)_output_size(240,240)_channels_lastTrue_modenearest
# Input: input_size: (1, 3, 600, 400), output_size: (240, 240), channels_last: True, mode: nearest
Run: 0, Forward Execution Time (us) : 737.431
Run: 1, Forward Execution Time (us) : 742.882
Run: 2, Forward Execution Time (us) : 739.611

# Benchmarking PyTorch: interpolate
# Mode: Eager
# Name: interpolate_input_size(1,3,600,400)_output_size(240,240)_channels_lastTrue_modelinear
# Input: input_size: (1, 3, 600, 400), output_size: (240, 240), channels_last: True, mode: linear
Run: 0, Forward Execution Time (us) : 1299.888
Run: 1, Forward Execution Time (us) : 1301.072
Run: 2, Forward Execution Time (us) : 1300.681

# Benchmarking PyTorch: interpolate
# Mode: Eager
# Name: interpolate_input_size(1,3,600,400)_output_size(240,240)_channels_lastTrue_modebicubic
# Input: input_size: (1, 3, 600, 400), output_size: (240, 240), channels_last: True, mode: bicubic
Run: 0, Forward Execution Time (us) : 4082.961
Run: 1, Forward Execution Time (us) : 4142.383
Run: 2, Forward Execution Time (us) : 4084.269

# Benchmarking PyTorch: interpolate
# Mode: Eager
# Name: interpolate_input_size(1,3,600,400)_output_size(240,240)_channels_lastFalse_modenearest
# Input: input_size: (1, 3, 600, 400), output_size: (240, 240), channels_last: False, mode: nearest
Run: 0, Forward Execution Time (us) : 205.935
Run: 1, Forward Execution Time (us) : 237.244
Run: 2, Forward Execution Time (us) : 237.932

# Benchmarking PyTorch: interpolate
# Mode: Eager
# Name: interpolate_input_size(1,3,600,400)_output_size(240,240)_channels_lastFalse_modelinear
# Input: input_size: (1, 3, 600, 400), output_size: (240, 240), channels_last: False, mode: linear
Run: 0, Forward Execution Time (us) : 2157.244
Run: 1, Forward Execution Time (us) : 2161.003
Run: 2, Forward Execution Time (us) : 2157.336

# Benchmarking PyTorch: interpolate
# Mode: Eager
# Name: interpolate_input_size(1,3,600,400)_output_size(240,240)_channels_lastFalse_modebicubic
# Input: input_size: (1, 3, 600, 400), output_size: (240, 240), channels_last: False, mode: bicubic
Run: 0, Forward Execution Time (us) : 2590.614
Run: 1, Forward Execution Time (us) : 2592.345
Run: 2, Forward Execution Time (us) : 2640.283

# Benchmarking PyTorch: interpolate
# Mode: Eager
# Name: interpolate_input_size(1,3,320,320)_output_size(256,256)_channels_lastTrue_modenearest
# Input: input_size: (1, 3, 320, 320), output_size: (256, 256), channels_last: True, mode: nearest
Run: 0, Forward Execution Time (us) : 819.755
Run: 1, Forward Execution Time (us) : 826.809
Run: 2, Forward Execution Time (us) : 820.075

# Benchmarking PyTorch: interpolate
# Mode: Eager
# Name: interpolate_input_size(1,3,320,320)_output_size(256,256)_channels_lastTrue_modelinear
# Input: input_size: (1, 3, 320, 320), output_size: (256, 256), channels_last: True, mode: linear
Run: 0, Forward Execution Time (us) : 1468.133
Run: 1, Forward Execution Time (us) : 1468.703
Run: 2, Forward Execution Time (us) : 1469.531

# Benchmarking PyTorch: interpolate
# Mode: Eager
# Name: interpolate_input_size(1,3,320,320)_output_size(256,256)_channels_lastTrue_modebicubic
# Input: input_size: (1, 3, 320, 320), output_size: (256, 256), channels_last: True, mode: bicubic
Run: 0, Forward Execution Time (us) : 4597.398
Run: 1, Forward Execution Time (us) : 4595.128
Run: 2, Forward Execution Time (us) : 4598.959

# Benchmarking PyTorch: interpolate
# Mode: Eager
# Name: interpolate_input_size(1,3,320,320)_output_size(256,256)_channels_lastFalse_modenearest
# Input: input_size: (1, 3, 320, 320), output_size: (256, 256), channels_last: False, mode: nearest
Run: 0, Forward Execution Time (us) : 209.337
Run: 1, Forward Execution Time (us) : 245.319
Run: 2, Forward Execution Time (us) : 244.543

# Benchmarking PyTorch: interpolate
# Mode: Eager
# Name: interpolate_input_size(1,3,320,320)_output_size(256,256)_channels_lastFalse_modelinear
# Input: input_size: (1, 3, 320, 320), output_size: (256, 256), channels_last: False, mode: linear
Run: 0, Forward Execution Time (us) : 2016.446
Run: 1, Forward Execution Time (us) : 2019.988
Run: 2, Forward Execution Time (us) : 2068.159

# Benchmarking PyTorch: interpolate
# Mode: Eager
# Name: interpolate_input_size(1,3,320,320)_output_size(256,256)_channels_lastFalse_modebicubic
# Input: input_size: (1, 3, 320, 320), output_size: (256, 256), channels_last: False, mode: bicubic
Run: 0, Forward Execution Time (us) : 2883.576
Run: 1, Forward Execution Time (us) : 2883.966
Run: 2, Forward Execution Time (us) : 2883.891

# Benchmarking PyTorch: interpolate
# Mode: Eager
# Name: interpolate_input_size(1,1,60,40)_output_size(24,24)_channels_lastTrue_modenearest
# Input: input_size: (1, 1, 60, 40), output_size: (24, 24), channels_last: True, mode: nearest
Run: 0, Forward Execution Time (us) : 25.371
Run: 1, Forward Execution Time (us) : 25.648
Run: 2, Forward Execution Time (us) : 25.871

# Benchmarking PyTorch: interpolate
# Mode: Eager
# Name: interpolate_input_size(1,1,60,40)_output_size(24,24)_channels_lastTrue_modelinear
# Input: input_size: (1, 1, 60, 40), output_size: (24, 24), channels_last: True, mode: linear
Run: 0, Forward Execution Time (us) : 28.691
Run: 1, Forward Execution Time (us) : 28.513
Run: 2, Forward Execution Time (us) : 28.301

# Benchmarking PyTorch: interpolate
# Mode: Eager
# Name: interpolate_input_size(1,1,60,40)_output_size(24,24)_channels_lastTrue_modebicubic
# Input: input_size: (1, 1, 60, 40), output_size: (24, 24), channels_last: True, mode: bicubic
Run: 0, Forward Execution Time (us) : 73.605
Run: 1, Forward Execution Time (us) : 74.055
Run: 2, Forward Execution Time (us) : 74.440

# Benchmarking PyTorch: interpolate
# Mode: Eager
# Name: interpolate_input_size(1,1,60,40)_output_size(24,24)_channels_lastFalse_modenearest
# Input: input_size: (1, 1, 60, 40), output_size: (24, 24), channels_last: False, mode: nearest
Run: 0, Forward Execution Time (us) : 25.417
Run: 1, Forward Execution Time (us) : 25.708
Run: 2, Forward Execution Time (us) : 25.936

# Benchmarking PyTorch: interpolate
# Mode: Eager
# Name: interpolate_input_size(1,1,60,40)_output_size(24,24)_channels_lastFalse_modelinear
# Input: input_size: (1, 1, 60, 40), output_size: (24, 24), channels_last: False, mode: linear
Run: 0, Forward Execution Time (us) : 28.674
Run: 1, Forward Execution Time (us) : 28.561
Run: 2, Forward Execution Time (us) : 28.278

# Benchmarking PyTorch: interpolate
# Mode: Eager
# Name: interpolate_input_size(1,1,60,40)_output_size(24,24)_channels_lastFalse_modebicubic
# Input: input_size: (1, 1, 60, 40), output_size: (24, 24), channels_last: False, mode: bicubic
Run: 0, Forward Execution Time (us) : 73.628
Run: 1, Forward Execution Time (us) : 73.933
Run: 2, Forward Execution Time (us) : 74.414

# Benchmarking PyTorch: interpolate
# Mode: Eager
# Name: interpolate_input_size(1,1,600,400)_output_size(240,240)_channels_lastTrue_modenearest
# Input: input_size: (1, 1, 600, 400), output_size: (240, 240), channels_last: True, mode: nearest
Run: 0, Forward Execution Time (us) : 85.326
Run: 1, Forward Execution Time (us) : 99.458
Run: 2, Forward Execution Time (us) : 99.490

# Benchmarking PyTorch: interpolate
# Mode: Eager
# Name: interpolate_input_size(1,1,600,400)_output_size(240,240)_channels_lastTrue_modelinear
# Input: input_size: (1, 1, 600, 400), output_size: (240, 240), channels_last: True, mode: linear
Run: 0, Forward Execution Time (us) : 193.484
Run: 1, Forward Execution Time (us) : 195.359
Run: 2, Forward Execution Time (us) : 195.238

# Benchmarking PyTorch: interpolate
# Mode: Eager
# Name: interpolate_input_size(1,1,600,400)_output_size(240,240)_channels_lastTrue_modebicubic
# Input: input_size: (1, 1, 600, 400), output_size: (240, 240), channels_last: True, mode: bicubic
Run: 0, Forward Execution Time (us) : 984.582
Run: 1, Forward Execution Time (us) : 988.451
Run: 2, Forward Execution Time (us) : 986.733

# Benchmarking PyTorch: interpolate
# Mode: Eager
# Name: interpolate_input_size(1,1,600,400)_output_size(240,240)_channels_lastFalse_modenearest
# Input: input_size: (1, 1, 600, 400), output_size: (240, 240), channels_last: False, mode: nearest
Run: 0, Forward Execution Time (us) : 84.214
Run: 1, Forward Execution Time (us) : 98.796
Run: 2, Forward Execution Time (us) : 98.485

# Benchmarking PyTorch: interpolate
# Mode: Eager
# Name: interpolate_input_size(1,1,600,400)_output_size(240,240)_channels_lastFalse_modelinear
# Input: input_size: (1, 1, 600, 400), output_size: (240, 240), channels_last: False, mode: linear
Run: 0, Forward Execution Time (us) : 192.874
Run: 1, Forward Execution Time (us) : 194.503
Run: 2, Forward Execution Time (us) : 195.123

# Benchmarking PyTorch: interpolate
# Mode: Eager
# Name: interpolate_input_size(1,1,600,400)_output_size(240,240)_channels_lastFalse_modebicubic
# Input: input_size: (1, 1, 600, 400), output_size: (240, 240), channels_last: False, mode: bicubic
Run: 0, Forward Execution Time (us) : 988.287
Run: 1, Forward Execution Time (us) : 989.302
Run: 2, Forward Execution Time (us) : 988.390

# Benchmarking PyTorch: interpolate
# Mode: Eager
# Name: interpolate_input_size(1,1,320,320)_output_size(256,256)_channels_lastTrue_modenearest
# Input: input_size: (1, 1, 320, 320), output_size: (256, 256), channels_last: True, mode: nearest
Run: 0, Forward Execution Time (us) : 86.161
Run: 1, Forward Execution Time (us) : 101.343
Run: 2, Forward Execution Time (us) : 102.037

# Benchmarking PyTorch: interpolate
# Mode: Eager
# Name: interpolate_input_size(1,1,320,320)_output_size(256,256)_channels_lastTrue_modelinear
# Input: input_size: (1, 1, 320, 320), output_size: (256, 256), channels_last: True, mode: linear
Run: 0, Forward Execution Time (us) : 201.280
Run: 1, Forward Execution Time (us) : 202.697
Run: 2, Forward Execution Time (us) : 203.194

# Benchmarking PyTorch: interpolate
# Mode: Eager
# Name: interpolate_input_size(1,1,320,320)_output_size(256,256)_channels_lastTrue_modebicubic
# Input: input_size: (1, 1, 320, 320), output_size: (256, 256), channels_last: True, mode: bicubic
Run: 0, Forward Execution Time (us) : 1091.431
Run: 1, Forward Execution Time (us) : 1091.211
Run: 2, Forward Execution Time (us) : 1093.496

# Benchmarking PyTorch: interpolate
# Mode: Eager
# Name: interpolate_input_size(1,1,320,320)_output_size(256,256)_channels_lastFalse_modenearest
# Input: input_size: (1, 1, 320, 320), output_size: (256, 256), channels_last: False, mode: nearest
Run: 0, Forward Execution Time (us) : 86.806
Run: 1, Forward Execution Time (us) : 102.046
Run: 2, Forward Execution Time (us) : 102.353

# Benchmarking PyTorch: interpolate
# Mode: Eager
# Name: interpolate_input_size(1,1,320,320)_output_size(256,256)_channels_lastFalse_modelinear
# Input: input_size: (1, 1, 320, 320), output_size: (256, 256), channels_last: False, mode: linear
Run: 0, Forward Execution Time (us) : 201.006
Run: 1, Forward Execution Time (us) : 202.676
Run: 2, Forward Execution Time (us) : 202.010

# Benchmarking PyTorch: interpolate
# Mode: Eager
# Name: interpolate_input_size(1,1,320,320)_output_size(256,256)_channels_lastFalse_modebicubic
# Input: input_size: (1, 1, 320, 320), output_size: (256, 256), channels_last: False, mode: bicubic
Run: 0, Forward Execution Time (us) : 1089.221
Run: 1, Forward Execution Time (us) : 1091.244
Run: 2, Forward Execution Time (us) : 1089.472

# Benchmarking PyTorch: interpolate
# Mode: Eager
# Name: interpolate_input_size(1,3,60,40)_output_size(24,24)_channels_lastTrue_modenearest_dtypetorch.uint8
# Input: input_size: (1, 3, 60, 40), output_size: (24, 24), channels_last: True, mode: nearest, dtype: torch.uint8
Run: 0, Forward Execution Time (us) : 26.526
Run: 1, Forward Execution Time (us) : 26.587
Run: 2, Forward Execution Time (us) : 26.748

# Benchmarking PyTorch: interpolate
# Mode: Eager
# Name: interpolate_input_size(1,3,60,40)_output_size(24,24)_channels_lastFalse_modenearest_dtypetorch.uint8
# Input: input_size: (1, 3, 60, 40), output_size: (24, 24), channels_last: False, mode: nearest, dtype: torch.uint8
Run: 0, Forward Execution Time (us) : 55.441
Run: 1, Forward Execution Time (us) : 55.603
Run: 2, Forward Execution Time (us) : 55.628

# Benchmarking PyTorch: interpolate
# Mode: Eager
# Name: interpolate_input_size(1,3,600,400)_output_size(240,240)_channels_lastTrue_modenearest_dtypetorch.uint8
# Input: input_size: (1, 3, 600, 400), output_size: (240, 240), channels_last: True, mode: nearest, dtype: torch.uint8
Run: 0, Forward Execution Time (us) : 812.942
Run: 1, Forward Execution Time (us) : 821.507
Run: 2, Forward Execution Time (us) : 811.741

# Benchmarking PyTorch: interpolate
# Mode: Eager
# Name: interpolate_input_size(1,3,600,400)_output_size(240,240)_channels_lastFalse_modenearest_dtypetorch.uint8
# Input: input_size: (1, 3, 600, 400), output_size: (240, 240), channels_last: False, mode: nearest, dtype: torch.uint8
Run: 0, Forward Execution Time (us) : 368.482
Run: 1, Forward Execution Time (us) : 434.842
Run: 2, Forward Execution Time (us) : 434.837

# Benchmarking PyTorch: interpolate
# Mode: Eager
# Name: interpolate_input_size(1,3,320,320)_output_size(256,256)_channels_lastTrue_modenearest_dtypetorch.uint8
# Input: input_size: (1, 3, 320, 320), output_size: (256, 256), channels_last: True, mode: nearest, dtype: torch.uint8
Run: 0, Forward Execution Time (us) : 918.681
Run: 1, Forward Execution Time (us) : 918.551
Run: 2, Forward Execution Time (us) : 918.913

# Benchmarking PyTorch: interpolate
# Mode: Eager
# Name: interpolate_input_size(1,3,320,320)_output_size(256,256)_channels_lastFalse_modenearest_dtypetorch.uint8
# Input: input_size: (1, 3, 320, 320), output_size: (256, 256), channels_last: False, mode: nearest, dtype: torch.uint8
Run: 0, Forward Execution Time (us) : 408.256
Run: 1, Forward Execution Time (us) : 482.489
Run: 2, Forward Execution Time (us) : 483.029

# Benchmarking PyTorch: interpolate
# Mode: Eager
# Name: interpolate_input_size(1,1,60,40)_output_size(24,24)_channels_lastTrue_modenearest_dtypetorch.uint8
# Input: input_size: (1, 1, 60, 40), output_size: (24, 24), channels_last: True, mode: nearest, dtype: torch.uint8
Run: 0, Forward Execution Time (us) : 25.203
Run: 1, Forward Execution Time (us) : 25.477
Run: 2, Forward Execution Time (us) : 25.800

# Benchmarking PyTorch: interpolate
# Mode: Eager
# Name: interpolate_input_size(1,1,60,40)_output_size(24,24)_channels_lastFalse_modenearest_dtypetorch.uint8
# Input: input_size: (1, 1, 60, 40), output_size: (24, 24), channels_last: False, mode: nearest, dtype: torch.uint8
Run: 0, Forward Execution Time (us) : 25.086
Run: 1, Forward Execution Time (us) : 25.190
Run: 2, Forward Execution Time (us) : 25.457

# Benchmarking PyTorch: interpolate
# Mode: Eager
# Name: interpolate_input_size(1,1,600,400)_output_size(240,240)_channels_lastTrue_modenearest_dtypetorch.uint8
# Input: input_size: (1, 1, 600, 400), output_size: (240, 240), channels_last: True, mode: nearest, dtype: torch.uint8
Run: 0, Forward Execution Time (us) : 140.332
Run: 1, Forward Execution Time (us) : 165.871
Run: 2, Forward Execution Time (us) : 166.046

# Benchmarking PyTorch: interpolate
# Mode: Eager
# Name: interpolate_input_size(1,1,600,400)_output_size(240,240)_channels_lastFalse_modenearest_dtypetorch.uint8
# Input: input_size: (1, 1, 600, 400), output_size: (240, 240), channels_last: False, mode: nearest, dtype: torch.uint8
Run: 0, Forward Execution Time (us) : 140.149
Run: 1, Forward Execution Time (us) : 165.976
Run: 2, Forward Execution Time (us) : 165.784

# Benchmarking PyTorch: interpolate
# Mode: Eager
# Name: interpolate_input_size(1,1,320,320)_output_size(256,256)_channels_lastTrue_modenearest_dtypetorch.uint8
# Input: input_size: (1, 1, 320, 320), output_size: (256, 256), channels_last: True, mode: nearest, dtype: torch.uint8
Run: 0, Forward Execution Time (us) : 154.712
Run: 1, Forward Execution Time (us) : 183.102
Run: 2, Forward Execution Time (us) : 183.070

# Benchmarking PyTorch: interpolate
# Mode: Eager
# Name: interpolate_input_size(1,1,320,320)_output_size(256,256)_channels_lastFalse_modenearest_dtypetorch.uint8
# Input: input_size: (1, 1, 320, 320), output_size: (256, 256), channels_last: False, mode: nearest, dtype: torch.uint8
Run: 0, Forward Execution Time (us) : 154.910
Run: 1, Forward Execution Time (us) : 183.355
Run: 2, Forward Execution Time (us) : 183.070

+ python -m pt.qbatchnorm_test --num-runs 3 --iterations 100 --warmup-iterations 10
not use dipu
# ----------------------------------------
# PyTorch/Caffe2 Operator Micro-benchmarks
# ----------------------------------------
# Tag : short

# Benchmarking PyTorch: QBatchNorm1d
# Mode: Eager
# Name: QBatchNorm1d_M1_N256_K3136_cpu_dtypetorch.qint8
# Input: M: 1, N: 256, K: 3136, device: cpu, dtype: torch.qint8
Run: 0, Forward Execution Time (us) : 1803.710
Run: 1, Forward Execution Time (us) : 1804.904
Run: 2, Forward Execution Time (us) : 1804.907

# Benchmarking PyTorch: QBatchNorm2d
# Mode: Eager
# Name: QBatchNorm2d_M1_N256_K3136_cpu_dtypetorch.qint8
# Input: M: 1, N: 256, K: 3136, device: cpu, dtype: torch.qint8
Run: 0, Forward Execution Time (us) : 977.403
Run: 1, Forward Execution Time (us) : 978.934
Run: 2, Forward Execution Time (us) : 978.591

+ python -m pt.qobserver_test --num-runs 3 --iterations 100 --warmup-iterations 10
not use dipu
# ----------------------------------------
# PyTorch/Caffe2 Operator Micro-benchmarks
# ----------------------------------------
# Tag : short

# Benchmarking PyTorch: MinMaxObserver
# Mode: Eager
# Name: MinMaxObserver_C3_M512_N512_dtypetorch.quint8_cpu_qschemetorch.per_tensor_affine
# Input: C: 3, M: 512, N: 512, dtype: torch.quint8, device: cpu, qscheme: torch.per_tensor_affine
Run: 0, Forward Execution Time (us) : 434.123
Run: 1, Forward Execution Time (us) : 434.158
Run: 2, Forward Execution Time (us) : 434.230

# Benchmarking PyTorch: MinMaxObserver
# Mode: Eager
# Name: MinMaxObserver_C3_M512_N512_dtypetorch.quint8_cpu_qschemetorch.per_tensor_symmetric
# Input: C: 3, M: 512, N: 512, dtype: torch.quint8, device: cpu, qscheme: torch.per_tensor_symmetric
Run: 0, Forward Execution Time (us) : 414.383
Run: 1, Forward Execution Time (us) : 414.491
Run: 2, Forward Execution Time (us) : 414.525

# Benchmarking PyTorch: MinMaxObserver
# Mode: Eager
# Name: MinMaxObserver_C3_M512_N512_dtypetorch.quint8_cuda_qschemetorch.per_tensor_affine
# Input: C: 3, M: 512, N: 512, dtype: torch.quint8, device: cuda, qscheme: torch.per_tensor_affine
Run: 0, Forward Execution Time (us) : 390.210
Run: 1, Forward Execution Time (us) : 451.638
Run: 2, Forward Execution Time (us) : 451.499

# Benchmarking PyTorch: MinMaxObserver
# Mode: Eager
# Name: MinMaxObserver_C3_M512_N512_dtypetorch.quint8_cuda_qschemetorch.per_tensor_symmetric
# Input: C: 3, M: 512, N: 512, dtype: torch.quint8, device: cuda, qscheme: torch.per_tensor_symmetric
Run: 0, Forward Execution Time (us) : 478.363
Run: 1, Forward Execution Time (us) : 446.611
Run: 2, Forward Execution Time (us) : 447.621

# Benchmarking PyTorch: MovingAverageMinMaxObserver
# Mode: Eager
# Name: MovingAverageMinMaxObserver_C3_M512_N512_dtypetorch.quint8_cpu_qschemetorch.per_tensor_affine
# Input: C: 3, M: 512, N: 512, dtype: torch.quint8, device: cpu, qscheme: torch.per_tensor_affine
Run: 0, Forward Execution Time (us) : 486.804
Run: 1, Forward Execution Time (us) : 487.113
Run: 2, Forward Execution Time (us) : 486.902

# Benchmarking PyTorch: MovingAverageMinMaxObserver
# Mode: Eager
# Name: MovingAverageMinMaxObserver_C3_M512_N512_dtypetorch.quint8_cpu_qschemetorch.per_tensor_symmetric
# Input: C: 3, M: 512, N: 512, dtype: torch.quint8, device: cpu, qscheme: torch.per_tensor_symmetric
Run: 0, Forward Execution Time (us) : 468.547
Run: 1, Forward Execution Time (us) : 468.765
Run: 2, Forward Execution Time (us) : 469.215

# Benchmarking PyTorch: MovingAverageMinMaxObserver
# Mode: Eager
# Name: MovingAverageMinMaxObserver_C3_M512_N512_dtypetorch.quint8_cuda_qschemetorch.per_tensor_affine
# Input: C: 3, M: 512, N: 512, dtype: torch.quint8, device: cuda, qscheme: torch.per_tensor_affine
Run: 0, Forward Execution Time (us) : 555.101
Run: 1, Forward Execution Time (us) : 554.243
Run: 2, Forward Execution Time (us) : 553.565

# Benchmarking PyTorch: MovingAverageMinMaxObserver
# Mode: Eager
# Name: MovingAverageMinMaxObserver_C3_M512_N512_dtypetorch.quint8_cuda_qschemetorch.per_tensor_symmetric
# Input: C: 3, M: 512, N: 512, dtype: torch.quint8, device: cuda, qscheme: torch.per_tensor_symmetric
Run: 0, Forward Execution Time (us) : 551.667
Run: 1, Forward Execution Time (us) : 572.925
Run: 2, Forward Execution Time (us) : 552.482

# Benchmarking PyTorch: PerChannelMinMaxObserver
# Mode: Eager
# Name: PerChannelMinMaxObserver_C3_M512_N512_dtypetorch.quint8_cpu_qschemetorch.per_channel_affine
# Input: C: 3, M: 512, N: 512, dtype: torch.quint8, device: cpu, qscheme: torch.per_channel_affine
Run: 0, Forward Execution Time (us) : 1105.549
Run: 1, Forward Execution Time (us) : 1104.694
Run: 2, Forward Execution Time (us) : 1116.596

# Benchmarking PyTorch: PerChannelMinMaxObserver
# Mode: Eager
# Name: PerChannelMinMaxObserver_C3_M512_N512_dtypetorch.quint8_cpu_qschemetorch.per_channel_symmetric
# Input: C: 3, M: 512, N: 512, dtype: torch.quint8, device: cpu, qscheme: torch.per_channel_symmetric
Run: 0, Forward Execution Time (us) : 1059.104
Run: 1, Forward Execution Time (us) : 1053.791
Run: 2, Forward Execution Time (us) : 1058.122

# Benchmarking PyTorch: PerChannelMinMaxObserver
# Mode: Eager
# Name: PerChannelMinMaxObserver_C3_M512_N512_dtypetorch.quint8_cuda_qschemetorch.per_channel_affine
# Input: C: 3, M: 512, N: 512, dtype: torch.quint8, device: cuda, qscheme: torch.per_channel_affine
Run: 0, Forward Execution Time (us) : 484.156
Run: 1, Forward Execution Time (us) : 483.433
Run: 2, Forward Execution Time (us) : 482.245

# Benchmarking PyTorch: PerChannelMinMaxObserver
# Mode: Eager
# Name: PerChannelMinMaxObserver_C3_M512_N512_dtypetorch.quint8_cuda_qschemetorch.per_channel_symmetric
# Input: C: 3, M: 512, N: 512, dtype: torch.quint8, device: cuda, qscheme: torch.per_channel_symmetric
Run: 0, Forward Execution Time (us) : 417.561
Run: 1, Forward Execution Time (us) : 417.447
Run: 2, Forward Execution Time (us) : 416.959

# Benchmarking PyTorch: MovingAveragePerChannelMinMaxObserver
# Mode: Eager
# Name: MovingAveragePerChannelMinMaxObserver_C3_M512_N512_dtypetorch.quint8_cpu_qschemetorch.per_channel_affine
# Input: C: 3, M: 512, N: 512, dtype: torch.quint8, device: cpu, qscheme: torch.per_channel_affine
Run: 0, Forward Execution Time (us) : 1121.371
Run: 1, Forward Execution Time (us) : 1129.155
Run: 2, Forward Execution Time (us) : 1159.227

# Benchmarking PyTorch: MovingAveragePerChannelMinMaxObserver
# Mode: Eager
# Name: MovingAveragePerChannelMinMaxObserver_C3_M512_N512_dtypetorch.quint8_cpu_qschemetorch.per_channel_symmetric
# Input: C: 3, M: 512, N: 512, dtype: torch.quint8, device: cpu, qscheme: torch.per_channel_symmetric
Run: 0, Forward Execution Time (us) : 1120.287
Run: 1, Forward Execution Time (us) : 1125.854
Run: 2, Forward Execution Time (us) : 1129.281

# Benchmarking PyTorch: MovingAveragePerChannelMinMaxObserver
# Mode: Eager
# Name: MovingAveragePerChannelMinMaxObserver_C3_M512_N512_dtypetorch.quint8_cuda_qschemetorch.per_channel_affine
# Input: C: 3, M: 512, N: 512, dtype: torch.quint8, device: cuda, qscheme: torch.per_channel_affine
Run: 0, Forward Execution Time (us) : 552.106
Run: 1, Forward Execution Time (us) : 551.346
Run: 2, Forward Execution Time (us) : 551.141

# Benchmarking PyTorch: MovingAveragePerChannelMinMaxObserver
# Mode: Eager
# Name: MovingAveragePerChannelMinMaxObserver_C3_M512_N512_dtypetorch.quint8_cuda_qschemetorch.per_channel_symmetric
# Input: C: 3, M: 512, N: 512, dtype: torch.quint8, device: cuda, qscheme: torch.per_channel_symmetric
Run: 0, Forward Execution Time (us) : 489.186
Run: 1, Forward Execution Time (us) : 490.551
Run: 2, Forward Execution Time (us) : 489.317

# Benchmarking PyTorch: HistogramObserver
# Mode: Eager
# Name: HistogramObserver_C3_M512_N512_dtypetorch.quint8_cpu_qschemetorch.per_tensor_affine
# Input: C: 3, M: 512, N: 512, dtype: torch.quint8, device: cpu, qscheme: torch.per_tensor_affine
Run: 0, Forward Execution Time (us) : 3368.184
Run: 1, Forward Execution Time (us) : 3571.794
Run: 2, Forward Execution Time (us) : 3569.188

# Benchmarking PyTorch: HistogramObserver
# Mode: Eager
# Name: HistogramObserver_C3_M512_N512_dtypetorch.quint8_cpu_qschemetorch.per_tensor_symmetric
# Input: C: 3, M: 512, N: 512, dtype: torch.quint8, device: cpu, qscheme: torch.per_tensor_symmetric
Run: 0, Forward Execution Time (us) : 3378.076
Run: 1, Forward Execution Time (us) : 3548.998
Run: 2, Forward Execution Time (us) : 3547.797

# Benchmarking PyTorch: HistogramObserverCalculateQparams
# Mode: Eager
# Name: HistogramObserverCalculateQparams_C3_M512_N512_dtypetorch.quint8_cpu_qschemetorch.per_tensor_affine
# Input: C: 3, M: 512, N: 512, dtype: torch.quint8, device: cpu, qscheme: torch.per_tensor_affine
Run: 0, Forward Execution Time (us) : 3475.526
Run: 1, Forward Execution Time (us) : 3701.671
Run: 2, Forward Execution Time (us) : 3701.346

# Benchmarking PyTorch: HistogramObserverCalculateQparams
# Mode: Eager
# Name: HistogramObserverCalculateQparams_C3_M512_N512_dtypetorch.quint8_cpu_qschemetorch.per_tensor_symmetric
# Input: C: 3, M: 512, N: 512, dtype: torch.quint8, device: cpu, qscheme: torch.per_tensor_symmetric
Run: 0, Forward Execution Time (us) : 3531.544
Run: 1, Forward Execution Time (us) : 3763.189
Run: 2, Forward Execution Time (us) : 3705.514

+ python -m pt.unary_test --num-runs 3 --iterations 100 --warmup-iterations 10
not use dipu
# ----------------------------------------
# PyTorch/Caffe2 Operator Micro-benchmarks
# ----------------------------------------
# Tag : short

# Benchmarking PyTorch: abs
# Mode: Eager
# Name: abs_M512_N512_cpu
# Input: M: 512, N: 512, device: cpu
Run: 0, Forward Execution Time (us) : 90.881
Run: 1, Forward Execution Time (us) : 93.842
Run: 2, Forward Execution Time (us) : 93.727

# Benchmarking PyTorch: abs
# Mode: Eager
# Name: abs_M512_N512_cuda
# Input: M: 512, N: 512, device: cuda
Run: 0, Forward Execution Time (us) : 15.356
Run: 1, Forward Execution Time (us) : 18.283
Run: 2, Forward Execution Time (us) : 17.927

# Benchmarking PyTorch: abs_
# Mode: Eager
# Name: abs__M512_N512_cpu
# Input: M: 512, N: 512, device: cpu
Run: 0, Forward Execution Time (us) : 30.233
Run: 1, Forward Execution Time (us) : 33.421
Run: 2, Forward Execution Time (us) : 33.515

# Benchmarking PyTorch: abs_
# Mode: Eager
# Name: abs__M512_N512_cuda
# Input: M: 512, N: 512, device: cuda
Run: 0, Forward Execution Time (us) : 10.875
Run: 1, Forward Execution Time (us) : 11.020
Run: 2, Forward Execution Time (us) : 10.965

# Benchmarking PyTorch: acos
# Mode: Eager
# Name: acos_M512_N512_cpu
# Input: M: 512, N: 512, device: cpu
Run: 0, Forward Execution Time (us) : 933.608
Run: 1, Forward Execution Time (us) : 933.791
Run: 2, Forward Execution Time (us) : 933.192

# Benchmarking PyTorch: acos
# Mode: Eager
# Name: acos_M512_N512_cuda
# Input: M: 512, N: 512, device: cuda
Run: 0, Forward Execution Time (us) : 15.378
Run: 1, Forward Execution Time (us) : 15.504
Run: 2, Forward Execution Time (us) : 15.503

# Benchmarking PyTorch: acos_
# Mode: Eager
# Name: acos__M512_N512_cpu
# Input: M: 512, N: 512, device: cpu
Run: 0, Forward Execution Time (us) : 927.031
Run: 1, Forward Execution Time (us) : 973.214
Run: 2, Forward Execution Time (us) : 928.728

# Benchmarking PyTorch: acos_
# Mode: Eager
# Name: acos__M512_N512_cuda
# Input: M: 512, N: 512, device: cuda
Run: 0, Forward Execution Time (us) : 10.756
Run: 1, Forward Execution Time (us) : 10.922
Run: 2, Forward Execution Time (us) : 10.948

# Benchmarking PyTorch: argsort
# Mode: Eager
# Name: argsort_M512_N512_cpu
# Input: M: 512, N: 512, device: cpu
Run: 0, Forward Execution Time (us) : 11083.335
Run: 1, Forward Execution Time (us) : 11098.482
Run: 2, Forward Execution Time (us) : 11131.586

# Benchmarking PyTorch: argsort
# Mode: Eager
# Name: argsort_M512_N512_cuda
# Input: M: 512, N: 512, device: cuda
Run: 0, Forward Execution Time (us) : 61.942
Run: 1, Forward Execution Time (us) : 62.027
Run: 2, Forward Execution Time (us) : 62.225

# Benchmarking PyTorch: asin
# Mode: Eager
# Name: asin_M512_N512_cpu
# Input: M: 512, N: 512, device: cpu
Run: 0, Forward Execution Time (us) : 773.160
Run: 1, Forward Execution Time (us) : 772.141
Run: 2, Forward Execution Time (us) : 772.871

# Benchmarking PyTorch: asin
# Mode: Eager
# Name: asin_M512_N512_cuda
# Input: M: 512, N: 512, device: cuda
Run: 0, Forward Execution Time (us) : 15.151
Run: 1, Forward Execution Time (us) : 15.397
Run: 2, Forward Execution Time (us) : 15.323

# Benchmarking PyTorch: asin_
# Mode: Eager
# Name: asin__M512_N512_cpu
# Input: M: 512, N: 512, device: cpu
Run: 0, Forward Execution Time (us) : 764.687
Run: 1, Forward Execution Time (us) : 765.616
Run: 2, Forward Execution Time (us) : 766.310

# Benchmarking PyTorch: asin_
# Mode: Eager
# Name: asin__M512_N512_cuda
# Input: M: 512, N: 512, device: cuda
Run: 0, Forward Execution Time (us) : 10.789
Run: 1, Forward Execution Time (us) : 10.853
Run: 2, Forward Execution Time (us) : 10.846

# Benchmarking PyTorch: atan
# Mode: Eager
# Name: atan_M512_N512_cpu
# Input: M: 512, N: 512, device: cpu
Run: 0, Forward Execution Time (us) : 1050.124
Run: 1, Forward Execution Time (us) : 1050.077
Run: 2, Forward Execution Time (us) : 1052.814

# Benchmarking PyTorch: atan
# Mode: Eager
# Name: atan_M512_N512_cuda
# Input: M: 512, N: 512, device: cuda
Run: 0, Forward Execution Time (us) : 15.219
Run: 1, Forward Execution Time (us) : 15.494
Run: 2, Forward Execution Time (us) : 15.450

# Benchmarking PyTorch: atan_
# Mode: Eager
# Name: atan__M512_N512_cpu
# Input: M: 512, N: 512, device: cpu
Run: 0, Forward Execution Time (us) : 1041.892
Run: 1, Forward Execution Time (us) : 1042.088
Run: 2, Forward Execution Time (us) : 1040.892

# Benchmarking PyTorch: atan_
# Mode: Eager
# Name: atan__M512_N512_cuda
# Input: M: 512, N: 512, device: cuda
Run: 0, Forward Execution Time (us) : 10.843
Run: 1, Forward Execution Time (us) : 10.940
Run: 2, Forward Execution Time (us) : 10.959

# Benchmarking PyTorch: ceil
# Mode: Eager
# Name: ceil_M512_N512_cpu
# Input: M: 512, N: 512, device: cpu
Run: 0, Forward Execution Time (us) : 92.235
Run: 1, Forward Execution Time (us) : 92.163
Run: 2, Forward Execution Time (us) : 92.164

# Benchmarking PyTorch: ceil
# Mode: Eager
# Name: ceil_M512_N512_cuda
# Input: M: 512, N: 512, device: cuda
Run: 0, Forward Execution Time (us) : 15.139
Run: 1, Forward Execution Time (us) : 15.384
Run: 2, Forward Execution Time (us) : 15.262

# Benchmarking PyTorch: ceil_
# Mode: Eager
# Name: ceil__M512_N512_cpu
# Input: M: 512, N: 512, device: cpu
Run: 0, Forward Execution Time (us) : 40.448
Run: 1, Forward Execution Time (us) : 40.526
Run: 2, Forward Execution Time (us) : 40.461

# Benchmarking PyTorch: ceil_
# Mode: Eager
# Name: ceil__M512_N512_cuda
# Input: M: 512, N: 512, device: cuda
Run: 0, Forward Execution Time (us) : 10.826
Run: 1, Forward Execution Time (us) : 10.977
Run: 2, Forward Execution Time (us) : 10.981

# Benchmarking PyTorch: clone
# Mode: Eager
# Name: clone_M512_N512_cpu
# Input: M: 512, N: 512, device: cpu
Run: 0, Forward Execution Time (us) : 90.921
Run: 1, Forward Execution Time (us) : 93.651
Run: 2, Forward Execution Time (us) : 93.653

# Benchmarking PyTorch: clone
# Mode: Eager
# Name: clone_M512_N512_cuda
# Input: M: 512, N: 512, device: cuda
Run: 0, Forward Execution Time (us) : 17.869
Run: 1, Forward Execution Time (us) : 18.137
Run: 2, Forward Execution Time (us) : 18.197

# Benchmarking PyTorch: cos
# Mode: Eager
# Name: cos_M512_N512_cpu
# Input: M: 512, N: 512, device: cpu
Run: 0, Forward Execution Time (us) : 643.697
Run: 1, Forward Execution Time (us) : 648.240
Run: 2, Forward Execution Time (us) : 648.035

# Benchmarking PyTorch: cos
# Mode: Eager
# Name: cos_M512_N512_cuda
# Input: M: 512, N: 512, device: cuda
Run: 0, Forward Execution Time (us) : 15.317
Run: 1, Forward Execution Time (us) : 15.442
Run: 2, Forward Execution Time (us) : 15.276

# Benchmarking PyTorch: cos_
# Mode: Eager
# Name: cos__M512_N512_cpu
# Input: M: 512, N: 512, device: cpu
Run: 0, Forward Execution Time (us) : 641.976
Run: 1, Forward Execution Time (us) : 640.664
Run: 2, Forward Execution Time (us) : 645.687

# Benchmarking PyTorch: cos_
# Mode: Eager
# Name: cos__M512_N512_cuda
# Input: M: 512, N: 512, device: cuda
Run: 0, Forward Execution Time (us) : 10.839
Run: 1, Forward Execution Time (us) : 10.998
Run: 2, Forward Execution Time (us) : 11.078

# Benchmarking PyTorch: cosh
# Mode: Eager
# Name: cosh_M512_N512_cpu
# Input: M: 512, N: 512, device: cpu
Run: 0, Forward Execution Time (us) : 1315.789
Run: 1, Forward Execution Time (us) : 1267.804
Run: 2, Forward Execution Time (us) : 1267.272

# Benchmarking PyTorch: cosh
# Mode: Eager
# Name: cosh_M512_N512_cuda
# Input: M: 512, N: 512, device: cuda
Run: 0, Forward Execution Time (us) : 15.229
Run: 1, Forward Execution Time (us) : 15.308
Run: 2, Forward Execution Time (us) : 15.287

# Benchmarking PyTorch: digamma
# Mode: Eager
# Name: digamma_M512_N512_cpu
# Input: M: 512, N: 512, device: cpu
Run: 0, Forward Execution Time (us) : 9721.273
Run: 1, Forward Execution Time (us) : 9691.051
Run: 2, Forward Execution Time (us) : 9722.251

# Benchmarking PyTorch: digamma
# Mode: Eager
# Name: digamma_M512_N512_cuda
# Input: M: 512, N: 512, device: cuda
Run: 0, Forward Execution Time (us) : 14.631
Run: 1, Forward Execution Time (us) : 14.919
Run: 2, Forward Execution Time (us) : 14.675

# Benchmarking PyTorch: erf
# Mode: Eager
# Name: erf_M512_N512_cpu
# Input: M: 512, N: 512, device: cpu
Run: 0, Forward Execution Time (us) : 684.057
Run: 1, Forward Execution Time (us) : 686.187
Run: 2, Forward Execution Time (us) : 686.262

# Benchmarking PyTorch: erf
# Mode: Eager
# Name: erf_M512_N512_cuda
# Input: M: 512, N: 512, device: cuda
Run: 0, Forward Execution Time (us) : 15.452
Run: 1, Forward Execution Time (us) : 15.543
Run: 2, Forward Execution Time (us) : 15.463

# Benchmarking PyTorch: erf_
# Mode: Eager
# Name: erf__M512_N512_cpu
# Input: M: 512, N: 512, device: cpu
Run: 0, Forward Execution Time (us) : 747.600
Run: 1, Forward Execution Time (us) : 748.794
Run: 2, Forward Execution Time (us) : 750.228

# Benchmarking PyTorch: erf_
# Mode: Eager
# Name: erf__M512_N512_cuda
# Input: M: 512, N: 512, device: cuda
Run: 0, Forward Execution Time (us) : 10.779
Run: 1, Forward Execution Time (us) : 10.862
Run: 2, Forward Execution Time (us) : 10.854

# Benchmarking PyTorch: erfc
# Mode: Eager
# Name: erfc_M512_N512_cpu
# Input: M: 512, N: 512, device: cpu
Run: 0, Forward Execution Time (us) : 2540.695
Run: 1, Forward Execution Time (us) : 2498.693
Run: 2, Forward Execution Time (us) : 2500.402

# Benchmarking PyTorch: erfc
# Mode: Eager
# Name: erfc_M512_N512_cuda
# Input: M: 512, N: 512, device: cuda
Run: 0, Forward Execution Time (us) : 14.603
Run: 1, Forward Execution Time (us) : 14.819
Run: 2, Forward Execution Time (us) : 14.859

# Benchmarking PyTorch: erfc_
# Mode: Eager
# Name: erfc__M512_N512_cpu
# Input: M: 512, N: 512, device: cpu
Run: 0, Forward Execution Time (us) : 2481.946
Run: 1, Forward Execution Time (us) : 2481.938
Run: 2, Forward Execution Time (us) : 2521.700

# Benchmarking PyTorch: erfc_
# Mode: Eager
# Name: erfc__M512_N512_cuda
# Input: M: 512, N: 512, device: cuda
Run: 0, Forward Execution Time (us) : 10.085
Run: 1, Forward Execution Time (us) : 10.161
Run: 2, Forward Execution Time (us) : 10.176

# Benchmarking PyTorch: erfinv
# Mode: Eager
# Name: erfinv_M512_N512_cpu
# Input: M: 512, N: 512, device: cpu
Run: 0, Forward Execution Time (us) : 21671.392
Run: 1, Forward Execution Time (us) : 21665.716
Run: 2, Forward Execution Time (us) : 21664.429

# Benchmarking PyTorch: erfinv
# Mode: Eager
# Name: erfinv_M512_N512_cuda
# Input: M: 512, N: 512, device: cuda
Run: 0, Forward Execution Time (us) : 14.718
Run: 1, Forward Execution Time (us) : 14.640
Run: 2, Forward Execution Time (us) : 14.564

# Benchmarking PyTorch: exp
# Mode: Eager
# Name: exp_M512_N512_cpu
# Input: M: 512, N: 512, device: cpu
Run: 0, Forward Execution Time (us) : 606.552
Run: 1, Forward Execution Time (us) : 609.804
Run: 2, Forward Execution Time (us) : 606.605

# Benchmarking PyTorch: exp
# Mode: Eager
# Name: exp_M512_N512_cuda
# Input: M: 512, N: 512, device: cuda
Run: 0, Forward Execution Time (us) : 15.284
Run: 1, Forward Execution Time (us) : 15.315
Run: 2, Forward Execution Time (us) : 15.441

# Benchmarking PyTorch: exp_
# Mode: Eager
# Name: exp__M512_N512_cpu
# Input: M: 512, N: 512, device: cpu
Run: 0, Forward Execution Time (us) : 599.774
Run: 1, Forward Execution Time (us) : 602.044
Run: 2, Forward Execution Time (us) : 603.212

# Benchmarking PyTorch: exp_
# Mode: Eager
# Name: exp__M512_N512_cuda
# Input: M: 512, N: 512, device: cuda
Run: 0, Forward Execution Time (us) : 10.837
Run: 1, Forward Execution Time (us) : 10.895
Run: 2, Forward Execution Time (us) : 10.929

# Benchmarking PyTorch: expm1
# Mode: Eager
# Name: expm1_M512_N512_cpu
# Input: M: 512, N: 512, device: cpu
Run: 0, Forward Execution Time (us) : 1154.532
Run: 1, Forward Execution Time (us) : 1155.169
Run: 2, Forward Execution Time (us) : 1155.591

# Benchmarking PyTorch: expm1
# Mode: Eager
# Name: expm1_M512_N512_cuda
# Input: M: 512, N: 512, device: cuda
Run: 0, Forward Execution Time (us) : 15.114
Run: 1, Forward Execution Time (us) : 15.251
Run: 2, Forward Execution Time (us) : 15.281

# Benchmarking PyTorch: expm1_
# Mode: Eager
# Name: expm1__M512_N512_cpu
# Input: M: 512, N: 512, device: cpu
Run: 0, Forward Execution Time (us) : 1151.389
Run: 1, Forward Execution Time (us) : 1146.108
Run: 2, Forward Execution Time (us) : 1180.727

# Benchmarking PyTorch: expm1_
# Mode: Eager
# Name: expm1__M512_N512_cuda
# Input: M: 512, N: 512, device: cuda
Run: 0, Forward Execution Time (us) : 10.739
Run: 1, Forward Execution Time (us) : 10.798
Run: 2, Forward Execution Time (us) : 10.878

# Benchmarking PyTorch: floor
# Mode: Eager
# Name: floor_M512_N512_cpu
# Input: M: 512, N: 512, device: cpu
Run: 0, Forward Execution Time (us) : 92.231
Run: 1, Forward Execution Time (us) : 92.190
Run: 2, Forward Execution Time (us) : 92.233

# Benchmarking PyTorch: floor
# Mode: Eager
# Name: floor_M512_N512_cuda
# Input: M: 512, N: 512, device: cuda
Run: 0, Forward Execution Time (us) : 15.096
Run: 1, Forward Execution Time (us) : 15.315
Run: 2, Forward Execution Time (us) : 15.292

# Benchmarking PyTorch: floor_
# Mode: Eager
# Name: floor__M512_N512_cpu
# Input: M: 512, N: 512, device: cpu
Run: 0, Forward Execution Time (us) : 40.518
Run: 1, Forward Execution Time (us) : 40.612
Run: 2, Forward Execution Time (us) : 40.009

# Benchmarking PyTorch: floor_
# Mode: Eager
# Name: floor__M512_N512_cuda
# Input: M: 512, N: 512, device: cuda
Run: 0, Forward Execution Time (us) : 10.827
Run: 1, Forward Execution Time (us) : 10.900
Run: 2, Forward Execution Time (us) : 10.887

# Benchmarking PyTorch: frac
# Mode: Eager
# Name: frac_M512_N512_cpu
# Input: M: 512, N: 512, device: cpu
Run: 0, Forward Execution Time (us) : 91.947
Run: 1, Forward Execution Time (us) : 92.285
Run: 2, Forward Execution Time (us) : 92.233

# Benchmarking PyTorch: frac
# Mode: Eager
# Name: frac_M512_N512_cuda
# Input: M: 512, N: 512, device: cuda
Run: 0, Forward Execution Time (us) : 15.258
Run: 1, Forward Execution Time (us) : 15.413
Run: 2, Forward Execution Time (us) : 15.430

# Benchmarking PyTorch: frac_
# Mode: Eager
# Name: frac__M512_N512_cpu
# Input: M: 512, N: 512, device: cpu
Run: 0, Forward Execution Time (us) : 42.118
Run: 1, Forward Execution Time (us) : 42.158
Run: 2, Forward Execution Time (us) : 42.126

# Benchmarking PyTorch: frac_
# Mode: Eager
# Name: frac__M512_N512_cuda
# Input: M: 512, N: 512, device: cuda
Run: 0, Forward Execution Time (us) : 10.786
Run: 1, Forward Execution Time (us) : 10.932
Run: 2, Forward Execution Time (us) : 10.942

# Benchmarking PyTorch: hardshrink
# Mode: Eager
# Name: hardshrink_M512_N512_cpu
# Input: M: 512, N: 512, device: cpu
Run: 0, Forward Execution Time (us) : 93.717
Run: 1, Forward Execution Time (us) : 93.955
Run: 2, Forward Execution Time (us) : 93.922

# Benchmarking PyTorch: hardshrink
# Mode: Eager
# Name: hardshrink_M512_N512_cuda
# Input: M: 512, N: 512, device: cuda
Run: 0, Forward Execution Time (us) : 15.603
Run: 1, Forward Execution Time (us) : 15.714
Run: 2, Forward Execution Time (us) : 15.689

# Benchmarking PyTorch: lgamma
# Mode: Eager
# Name: lgamma_M512_N512_cpu
# Input: M: 512, N: 512, device: cpu
Run: 0, Forward Execution Time (us) : 4712.410
Run: 1, Forward Execution Time (us) : 4713.142
Run: 2, Forward Execution Time (us) : 4712.133

# Benchmarking PyTorch: lgamma
# Mode: Eager
# Name: lgamma_M512_N512_cuda
# Input: M: 512, N: 512, device: cuda
Run: 0, Forward Execution Time (us) : 14.489
Run: 1, Forward Execution Time (us) : 14.613
Run: 2, Forward Execution Time (us) : 14.634

# Benchmarking PyTorch: log
# Mode: Eager
# Name: log_M512_N512_cpu
# Input: M: 512, N: 512, device: cpu
Run: 0, Forward Execution Time (us) : 865.746
Run: 1, Forward Execution Time (us) : 828.084
Run: 2, Forward Execution Time (us) : 827.396

# Benchmarking PyTorch: log
# Mode: Eager
# Name: log_M512_N512_cuda
# Input: M: 512, N: 512, device: cuda
Run: 0, Forward Execution Time (us) : 15.201
Run: 1, Forward Execution Time (us) : 15.414
Run: 2, Forward Execution Time (us) : 15.417

# Benchmarking PyTorch: log10
# Mode: Eager
# Name: log10_M512_N512_cpu
# Input: M: 512, N: 512, device: cpu
Run: 0, Forward Execution Time (us) : 849.584
Run: 1, Forward Execution Time (us) : 850.454
Run: 2, Forward Execution Time (us) : 849.925

# Benchmarking PyTorch: log10
# Mode: Eager
# Name: log10_M512_N512_cuda
# Input: M: 512, N: 512, device: cuda
Run: 0, Forward Execution Time (us) : 15.106
Run: 1, Forward Execution Time (us) : 15.188
Run: 2, Forward Execution Time (us) : 15.180

# Benchmarking PyTorch: log10_
# Mode: Eager
# Name: log10__M512_N512_cpu
# Input: M: 512, N: 512, device: cpu
Run: 0, Forward Execution Time (us) : 843.544
Run: 1, Forward Execution Time (us) : 842.267
Run: 2, Forward Execution Time (us) : 841.094

# Benchmarking PyTorch: log10_
# Mode: Eager
# Name: log10__M512_N512_cuda
# Input: M: 512, N: 512, device: cuda
Run: 0, Forward Execution Time (us) : 10.600
Run: 1, Forward Execution Time (us) : 10.692
Run: 2, Forward Execution Time (us) : 10.738

# Benchmarking PyTorch: log1p
# Mode: Eager
# Name: log1p_M512_N512_cpu
# Input: M: 512, N: 512, device: cpu
Run: 0, Forward Execution Time (us) : 835.543
Run: 1, Forward Execution Time (us) : 835.084
Run: 2, Forward Execution Time (us) : 834.767

# Benchmarking PyTorch: log1p
# Mode: Eager
# Name: log1p_M512_N512_cuda
# Input: M: 512, N: 512, device: cuda
Run: 0, Forward Execution Time (us) : 15.248
Run: 1, Forward Execution Time (us) : 15.381
Run: 2, Forward Execution Time (us) : 15.317

# Benchmarking PyTorch: log1p_
# Mode: Eager
# Name: log1p__M512_N512_cpu
# Input: M: 512, N: 512, device: cpu
Run: 0, Forward Execution Time (us) : 808.372
Run: 1, Forward Execution Time (us) : 806.651
Run: 2, Forward Execution Time (us) : 790.528

# Benchmarking PyTorch: log1p_
# Mode: Eager
# Name: log1p__M512_N512_cuda
# Input: M: 512, N: 512, device: cuda
Run: 0, Forward Execution Time (us) : 10.761
Run: 1, Forward Execution Time (us) : 10.876
Run: 2, Forward Execution Time (us) : 10.946

# Benchmarking PyTorch: log2
# Mode: Eager
# Name: log2_M512_N512_cpu
# Input: M: 512, N: 512, device: cpu
Run: 0, Forward Execution Time (us) : 872.112
Run: 1, Forward Execution Time (us) : 873.187
Run: 2, Forward Execution Time (us) : 874.328

# Benchmarking PyTorch: log2
# Mode: Eager
# Name: log2_M512_N512_cuda
# Input: M: 512, N: 512, device: cuda
Run: 0, Forward Execution Time (us) : 15.171
Run: 1, Forward Execution Time (us) : 15.394
Run: 2, Forward Execution Time (us) : 15.229

# Benchmarking PyTorch: log2_
# Mode: Eager
# Name: log2__M512_N512_cpu
# Input: M: 512, N: 512, device: cpu
Run: 0, Forward Execution Time (us) : 866.221
Run: 1, Forward Execution Time (us) : 866.973
Run: 2, Forward Execution Time (us) : 866.961

# Benchmarking PyTorch: log2_
# Mode: Eager
# Name: log2__M512_N512_cuda
# Input: M: 512, N: 512, device: cuda
Run: 0, Forward Execution Time (us) : 10.802
Run: 1, Forward Execution Time (us) : 10.928
Run: 2, Forward Execution Time (us) : 10.934

# Benchmarking PyTorch: log_
# Mode: Eager
# Name: log__M512_N512_cpu
# Input: M: 512, N: 512, device: cpu
Run: 0, Forward Execution Time (us) : 818.069
Run: 1, Forward Execution Time (us) : 819.626
Run: 2, Forward Execution Time (us) : 819.456

# Benchmarking PyTorch: log_
# Mode: Eager
# Name: log__M512_N512_cuda
# Input: M: 512, N: 512, device: cuda
Run: 0, Forward Execution Time (us) : 10.905
Run: 1, Forward Execution Time (us) : 11.014
Run: 2, Forward Execution Time (us) : 11.013

# Benchmarking PyTorch: logit
# Mode: Eager
# Name: logit_M512_N512_cpu
# Input: M: 512, N: 512, device: cpu
Run: 0, Forward Execution Time (us) : 845.724
Run: 1, Forward Execution Time (us) : 848.000
Run: 2, Forward Execution Time (us) : 846.855

# Benchmarking PyTorch: logit
# Mode: Eager
# Name: logit_M512_N512_cuda
# Input: M: 512, N: 512, device: cuda
Run: 0, Forward Execution Time (us) : 15.892
Run: 1, Forward Execution Time (us) : 16.105
Run: 2, Forward Execution Time (us) : 16.139

# Benchmarking PyTorch: logit_
# Mode: Eager
# Name: logit__M512_N512_cpu
# Input: M: 512, N: 512, device: cpu
Run: 0, Forward Execution Time (us) : 832.405
Run: 1, Forward Execution Time (us) : 833.024
Run: 2, Forward Execution Time (us) : 833.583

# Benchmarking PyTorch: logit_
# Mode: Eager
# Name: logit__M512_N512_cuda
# Input: M: 512, N: 512, device: cuda
Run: 0, Forward Execution Time (us) : 11.355
Run: 1, Forward Execution Time (us) : 11.605
Run: 2, Forward Execution Time (us) : 11.536

# Benchmarking PyTorch: neg
# Mode: Eager
# Name: neg_M512_N512_cpu
# Input: M: 512, N: 512, device: cpu
Run: 0, Forward Execution Time (us) : 89.282
Run: 1, Forward Execution Time (us) : 91.719
Run: 2, Forward Execution Time (us) : 91.817

# Benchmarking PyTorch: neg
# Mode: Eager
# Name: neg_M512_N512_cuda
# Input: M: 512, N: 512, device: cuda
Run: 0, Forward Execution Time (us) : 15.143
Run: 1, Forward Execution Time (us) : 15.235
Run: 2, Forward Execution Time (us) : 15.407

# Benchmarking PyTorch: neg_
# Mode: Eager
# Name: neg__M512_N512_cpu
# Input: M: 512, N: 512, device: cpu
Run: 0, Forward Execution Time (us) : 30.462
Run: 1, Forward Execution Time (us) : 33.957
Run: 2, Forward Execution Time (us) : 33.835

# Benchmarking PyTorch: neg_
# Mode: Eager
# Name: neg__M512_N512_cuda
# Input: M: 512, N: 512, device: cuda
Run: 0, Forward Execution Time (us) : 10.825
Run: 1, Forward Execution Time (us) : 10.817
Run: 2, Forward Execution Time (us) : 10.924

# Benchmarking PyTorch: reciprocal
# Mode: Eager
# Name: reciprocal_M512_N512_cpu
# Input: M: 512, N: 512, device: cpu
Run: 0, Forward Execution Time (us) : 89.392
Run: 1, Forward Execution Time (us) : 91.986
Run: 2, Forward Execution Time (us) : 91.925

# Benchmarking PyTorch: reciprocal
# Mode: Eager
# Name: reciprocal_M512_N512_cuda
# Input: M: 512, N: 512, device: cuda
Run: 0, Forward Execution Time (us) : 15.177
Run: 1, Forward Execution Time (us) : 15.303
Run: 2, Forward Execution Time (us) : 15.345

# Benchmarking PyTorch: reciprocal_
# Mode: Eager
# Name: reciprocal__M512_N512_cpu
# Input: M: 512, N: 512, device: cpu
Run: 0, Forward Execution Time (us) : 48.910
Run: 1, Forward Execution Time (us) : 57.403
Run: 2, Forward Execution Time (us) : 57.366

# Benchmarking PyTorch: reciprocal_
# Mode: Eager
# Name: reciprocal__M512_N512_cuda
# Input: M: 512, N: 512, device: cuda
Run: 0, Forward Execution Time (us) : 10.775
Run: 1, Forward Execution Time (us) : 10.879
Run: 2, Forward Execution Time (us) : 10.852

# Benchmarking PyTorch: relu
# Mode: Eager
# Name: relu_M512_N512_cpu
# Input: M: 512, N: 512, device: cpu
Run: 0, Forward Execution Time (us) : 92.509
Run: 1, Forward Execution Time (us) : 92.538
Run: 2, Forward Execution Time (us) : 92.479

# Benchmarking PyTorch: relu
# Mode: Eager
# Name: relu_M512_N512_cuda
# Input: M: 512, N: 512, device: cuda
Run: 0, Forward Execution Time (us) : 16.444
Run: 1, Forward Execution Time (us) : 16.534
Run: 2, Forward Execution Time (us) : 16.585

# Benchmarking PyTorch: relu_
# Mode: Eager
# Name: relu__M512_N512_cpu
# Input: M: 512, N: 512, device: cpu
Run: 0, Forward Execution Time (us) : 35.299
Run: 1, Forward Execution Time (us) : 35.552
Run: 2, Forward Execution Time (us) : 35.493

# Benchmarking PyTorch: relu_
# Mode: Eager
# Name: relu__M512_N512_cuda
# Input: M: 512, N: 512, device: cuda
Run: 0, Forward Execution Time (us) : 12.238
Run: 1, Forward Execution Time (us) : 12.103
Run: 2, Forward Execution Time (us) : 12.052

# Benchmarking PyTorch: round
# Mode: Eager
# Name: round_M512_N512_cpu
# Input: M: 512, N: 512, device: cpu
Run: 0, Forward Execution Time (us) : 92.061
Run: 1, Forward Execution Time (us) : 92.348
Run: 2, Forward Execution Time (us) : 92.310

# Benchmarking PyTorch: round
# Mode: Eager
# Name: round_M512_N512_cuda
# Input: M: 512, N: 512, device: cuda
Run: 0, Forward Execution Time (us) : 15.301
Run: 1, Forward Execution Time (us) : 15.333
Run: 2, Forward Execution Time (us) : 15.385

# Benchmarking PyTorch: round_
# Mode: Eager
# Name: round__M512_N512_cpu
# Input: M: 512, N: 512, device: cpu
Run: 0, Forward Execution Time (us) : 40.416
Run: 1, Forward Execution Time (us) : 40.126
Run: 2, Forward Execution Time (us) : 40.176

# Benchmarking PyTorch: round_
# Mode: Eager
# Name: round__M512_N512_cuda
# Input: M: 512, N: 512, device: cuda
Run: 0, Forward Execution Time (us) : 10.783
Run: 1, Forward Execution Time (us) : 10.937
Run: 2, Forward Execution Time (us) : 10.963

# Benchmarking PyTorch: rsqrt
# Mode: Eager
# Name: rsqrt_M512_N512_cpu
# Input: M: 512, N: 512, device: cpu
Run: 0, Forward Execution Time (us) : 104.999
Run: 1, Forward Execution Time (us) : 123.111
Run: 2, Forward Execution Time (us) : 123.121

# Benchmarking PyTorch: rsqrt
# Mode: Eager
# Name: rsqrt_M512_N512_cuda
# Input: M: 512, N: 512, device: cuda
Run: 0, Forward Execution Time (us) : 15.108
Run: 1, Forward Execution Time (us) : 15.169
Run: 2, Forward Execution Time (us) : 15.217

# Benchmarking PyTorch: rsqrt_
# Mode: Eager
# Name: rsqrt__M512_N512_cpu
# Input: M: 512, N: 512, device: cpu
Run: 0, Forward Execution Time (us) : 99.805
Run: 1, Forward Execution Time (us) : 117.318
Run: 2, Forward Execution Time (us) : 117.257

# Benchmarking PyTorch: rsqrt_
# Mode: Eager
# Name: rsqrt__M512_N512_cuda
# Input: M: 512, N: 512, device: cuda
Run: 0, Forward Execution Time (us) : 10.762
Run: 1, Forward Execution Time (us) : 10.907
Run: 2, Forward Execution Time (us) : 10.879

# Benchmarking PyTorch: sigmoid
# Mode: Eager
# Name: sigmoid_M512_N512_cpu
# Input: M: 512, N: 512, device: cpu
Run: 0, Forward Execution Time (us) : 602.386
Run: 1, Forward Execution Time (us) : 604.341
Run: 2, Forward Execution Time (us) : 603.612

# Benchmarking PyTorch: sigmoid
# Mode: Eager
# Name: sigmoid_M512_N512_cuda
# Input: M: 512, N: 512, device: cuda
Run: 0, Forward Execution Time (us) : 15.179
Run: 1, Forward Execution Time (us) : 15.185
Run: 2, Forward Execution Time (us) : 15.181

# Benchmarking PyTorch: sigmoid_
# Mode: Eager
# Name: sigmoid__M512_N512_cpu
# Input: M: 512, N: 512, device: cpu
Run: 0, Forward Execution Time (us) : 596.739
Run: 1, Forward Execution Time (us) : 597.269
Run: 2, Forward Execution Time (us) : 596.941

# Benchmarking PyTorch: sigmoid_
# Mode: Eager
# Name: sigmoid__M512_N512_cuda
# Input: M: 512, N: 512, device: cuda
Run: 0, Forward Execution Time (us) : 10.769
Run: 1, Forward Execution Time (us) : 10.906
Run: 2, Forward Execution Time (us) : 10.941

# Benchmarking PyTorch: sign
# Mode: Eager
# Name: sign_M512_N512_cpu
# Input: M: 512, N: 512, device: cpu
Run: 0, Forward Execution Time (us) : 96.750
Run: 1, Forward Execution Time (us) : 96.792
Run: 2, Forward Execution Time (us) : 96.435

# Benchmarking PyTorch: sign
# Mode: Eager
# Name: sign_M512_N512_cuda
# Input: M: 512, N: 512, device: cuda
Run: 0, Forward Execution Time (us) : 15.286
Run: 1, Forward Execution Time (us) : 15.569
Run: 2, Forward Execution Time (us) : 15.295

# Benchmarking PyTorch: sgn
# Mode: Eager
# Name: sgn_M512_N512_cpu
# Input: M: 512, N: 512, device: cpu
Run: 0, Forward Execution Time (us) : 96.536
Run: 1, Forward Execution Time (us) : 96.635
Run: 2, Forward Execution Time (us) : 97.756

# Benchmarking PyTorch: sgn
# Mode: Eager
# Name: sgn_M512_N512_cuda
# Input: M: 512, N: 512, device: cuda
Run: 0, Forward Execution Time (us) : 14.997
Run: 1, Forward Execution Time (us) : 15.185
Run: 2, Forward Execution Time (us) : 15.165

# Benchmarking PyTorch: sin
# Mode: Eager
# Name: sin_M512_N512_cpu
# Input: M: 512, N: 512, device: cpu
Run: 0, Forward Execution Time (us) : 637.744
Run: 1, Forward Execution Time (us) : 641.191
Run: 2, Forward Execution Time (us) : 641.007

# Benchmarking PyTorch: sin
# Mode: Eager
# Name: sin_M512_N512_cuda
# Input: M: 512, N: 512, device: cuda
Run: 0, Forward Execution Time (us) : 15.244
Run: 1, Forward Execution Time (us) : 15.517
Run: 2, Forward Execution Time (us) : 15.379

# Benchmarking PyTorch: sin_
# Mode: Eager
# Name: sin__M512_N512_cpu
# Input: M: 512, N: 512, device: cpu
Run: 0, Forward Execution Time (us) : 633.559
Run: 1, Forward Execution Time (us) : 635.333
Run: 2, Forward Execution Time (us) : 636.770

# Benchmarking PyTorch: sin_
# Mode: Eager
# Name: sin__M512_N512_cuda
# Input: M: 512, N: 512, device: cuda
Run: 0, Forward Execution Time (us) : 10.978
Run: 1, Forward Execution Time (us) : 11.027
Run: 2, Forward Execution Time (us) : 10.968

# Benchmarking PyTorch: sinh
# Mode: Eager
# Name: sinh_M512_N512_cpu
# Input: M: 512, N: 512, device: cpu
Run: 0, Forward Execution Time (us) : 1309.981
Run: 1, Forward Execution Time (us) : 1311.224
Run: 2, Forward Execution Time (us) : 1311.559

# Benchmarking PyTorch: sinh
# Mode: Eager
# Name: sinh_M512_N512_cuda
# Input: M: 512, N: 512, device: cuda
Run: 0, Forward Execution Time (us) : 15.017
Run: 1, Forward Execution Time (us) : 15.269
Run: 2, Forward Execution Time (us) : 15.305

# Benchmarking PyTorch: sqrt
# Mode: Eager
# Name: sqrt_M512_N512_cpu
# Input: M: 512, N: 512, device: cpu
Run: 0, Forward Execution Time (us) : 89.405
Run: 1, Forward Execution Time (us) : 92.053
Run: 2, Forward Execution Time (us) : 92.029

# Benchmarking PyTorch: sqrt
# Mode: Eager
# Name: sqrt_M512_N512_cuda
# Input: M: 512, N: 512, device: cuda
Run: 0, Forward Execution Time (us) : 15.311
Run: 1, Forward Execution Time (us) : 15.387
Run: 2, Forward Execution Time (us) : 15.384

# Benchmarking PyTorch: sqrt_
# Mode: Eager
# Name: sqrt__M512_N512_cpu
# Input: M: 512, N: 512, device: cpu
Run: 0, Forward Execution Time (us) : 57.462
Run: 1, Forward Execution Time (us) : 67.538
Run: 2, Forward Execution Time (us) : 67.470

# Benchmarking PyTorch: sqrt_
# Mode: Eager
# Name: sqrt__M512_N512_cuda
# Input: M: 512, N: 512, device: cuda
Run: 0, Forward Execution Time (us) : 10.820
Run: 1, Forward Execution Time (us) : 10.933
Run: 2, Forward Execution Time (us) : 10.929

# Benchmarking PyTorch: square
# Mode: Eager
# Name: square_M512_N512_cpu
# Input: M: 512, N: 512, device: cpu
Run: 0, Forward Execution Time (us) : 93.050
Run: 1, Forward Execution Time (us) : 93.097
Run: 2, Forward Execution Time (us) : 93.138

# Benchmarking PyTorch: square
# Mode: Eager
# Name: square_M512_N512_cuda
# Input: M: 512, N: 512, device: cuda
Run: 0, Forward Execution Time (us) : 16.510
Run: 1, Forward Execution Time (us) : 16.668
Run: 2, Forward Execution Time (us) : 16.731

# Benchmarking PyTorch: square_
# Mode: Eager
# Name: square__M512_N512_cpu
# Input: M: 512, N: 512, device: cpu
Run: 0, Forward Execution Time (us) : 37.559
Run: 1, Forward Execution Time (us) : 35.455
Run: 2, Forward Execution Time (us) : 35.281

# Benchmarking PyTorch: square_
# Mode: Eager
# Name: square__M512_N512_cuda
# Input: M: 512, N: 512, device: cuda
Run: 0, Forward Execution Time (us) : 12.090
Run: 1, Forward Execution Time (us) : 12.226
Run: 2, Forward Execution Time (us) : 12.141

# Benchmarking PyTorch: tan
# Mode: Eager
# Name: tan_M512_N512_cpu
# Input: M: 512, N: 512, device: cpu
Run: 0, Forward Execution Time (us) : 1070.049
Run: 1, Forward Execution Time (us) : 1068.737
Run: 2, Forward Execution Time (us) : 1069.538

# Benchmarking PyTorch: tan
# Mode: Eager
# Name: tan_M512_N512_cuda
# Input: M: 512, N: 512, device: cuda
Run: 0, Forward Execution Time (us) : 15.146
Run: 1, Forward Execution Time (us) : 15.388
Run: 2, Forward Execution Time (us) : 15.353

# Benchmarking PyTorch: tan_
# Mode: Eager
# Name: tan__M512_N512_cpu
# Input: M: 512, N: 512, device: cpu
Run: 0, Forward Execution Time (us) : 1071.895
Run: 1, Forward Execution Time (us) : 1068.263
Run: 2, Forward Execution Time (us) : 1066.956

# Benchmarking PyTorch: tan_
# Mode: Eager
# Name: tan__M512_N512_cuda
# Input: M: 512, N: 512, device: cuda
Run: 0, Forward Execution Time (us) : 10.802
Run: 1, Forward Execution Time (us) : 11.072
Run: 2, Forward Execution Time (us) : 10.998

# Benchmarking PyTorch: tanh
# Mode: Eager
# Name: tanh_M512_N512_cpu
# Input: M: 512, N: 512, device: cpu
Run: 0, Forward Execution Time (us) : 1395.685
Run: 1, Forward Execution Time (us) : 1396.475
Run: 2, Forward Execution Time (us) : 1396.466

# Benchmarking PyTorch: tanh
# Mode: Eager
# Name: tanh_M512_N512_cuda
# Input: M: 512, N: 512, device: cuda
Run: 0, Forward Execution Time (us) : 15.105
Run: 1, Forward Execution Time (us) : 15.178
Run: 2, Forward Execution Time (us) : 15.280

# Benchmarking PyTorch: tanh_
# Mode: Eager
# Name: tanh__M512_N512_cpu
# Input: M: 512, N: 512, device: cpu
Run: 0, Forward Execution Time (us) : 1420.784
Run: 1, Forward Execution Time (us) : 1386.801
Run: 2, Forward Execution Time (us) : 1386.478

# Benchmarking PyTorch: tanh_
# Mode: Eager
# Name: tanh__M512_N512_cuda
# Input: M: 512, N: 512, device: cuda
Run: 0, Forward Execution Time (us) : 10.950
Run: 1, Forward Execution Time (us) : 11.003
Run: 2, Forward Execution Time (us) : 10.935

# Benchmarking PyTorch: trunc
# Mode: Eager
# Name: trunc_M512_N512_cpu
# Input: M: 512, N: 512, device: cpu
Run: 0, Forward Execution Time (us) : 91.903
Run: 1, Forward Execution Time (us) : 92.104
Run: 2, Forward Execution Time (us) : 92.076

# Benchmarking PyTorch: trunc
# Mode: Eager
# Name: trunc_M512_N512_cuda
# Input: M: 512, N: 512, device: cuda
Run: 0, Forward Execution Time (us) : 15.292
Run: 1, Forward Execution Time (us) : 15.443
Run: 2, Forward Execution Time (us) : 15.414

# Benchmarking PyTorch: trunc_
# Mode: Eager
# Name: trunc__M512_N512_cpu
# Input: M: 512, N: 512, device: cpu
Run: 0, Forward Execution Time (us) : 40.307
Run: 1, Forward Execution Time (us) : 40.514
Run: 2, Forward Execution Time (us) : 40.547

# Benchmarking PyTorch: trunc_
# Mode: Eager
# Name: trunc__M512_N512_cuda
# Input: M: 512, N: 512, device: cuda
Run: 0, Forward Execution Time (us) : 10.773
Run: 1, Forward Execution Time (us) : 10.908
Run: 2, Forward Execution Time (us) : 10.836

# Benchmarking PyTorch: unique
# Mode: Eager
# Name: unique_M512_N512_cpu
# Input: M: 512, N: 512, device: cpu
Run: 0, Forward Execution Time (us) : 62665.040
Run: 1, Forward Execution Time (us) : 62619.046
Run: 2, Forward Execution Time (us) : 62632.970

# Benchmarking PyTorch: unique
# Mode: Eager
# Name: unique_M512_N512_cuda
# Input: M: 512, N: 512, device: cuda
Run: 0, Forward Execution Time (us) : 272.571
Run: 1, Forward Execution Time (us) : 271.007
Run: 2, Forward Execution Time (us) : 272.589

# Benchmarking PyTorch: zero_
# Mode: Eager
# Name: zero__M512_N512_cpu
# Input: M: 512, N: 512, device: cpu
Run: 0, Forward Execution Time (us) : 29.372
Run: 1, Forward Execution Time (us) : 32.742
Run: 2, Forward Execution Time (us) : 32.492

# Benchmarking PyTorch: zero_
# Mode: Eager
# Name: zero__M512_N512_cuda
# Input: M: 512, N: 512, device: cuda
Run: 0, Forward Execution Time (us) : 10.843
Run: 1, Forward Execution Time (us) : 10.938
Run: 2, Forward Execution Time (us) : 11.015

# Benchmarking PyTorch: bernoulli_
# Mode: Eager
# Name: bernoulli__M512_N512_cpu
# Input: M: 512, N: 512, device: cpu
Run: 0, Forward Execution Time (us) : 5685.149
Run: 1, Forward Execution Time (us) : 5686.144
Run: 2, Forward Execution Time (us) : 5689.616

# Benchmarking PyTorch: bernoulli_
# Mode: Eager
# Name: bernoulli__M512_N512_cuda
# Input: M: 512, N: 512, device: cuda
Run: 0, Forward Execution Time (us) : 12.628
Run: 1, Forward Execution Time (us) : 12.743
Run: 2, Forward Execution Time (us) : 12.745

# Benchmarking PyTorch: cauchy_
# Mode: Eager
# Name: cauchy__M512_N512_cpu
# Input: M: 512, N: 512, device: cpu
Run: 0, Forward Execution Time (us) : 15448.696
Run: 1, Forward Execution Time (us) : 15495.675
Run: 2, Forward Execution Time (us) : 15469.194

# Benchmarking PyTorch: cauchy_
# Mode: Eager
# Name: cauchy__M512_N512_cuda
# Input: M: 512, N: 512, device: cuda
Run: 0, Forward Execution Time (us) : 12.536
Run: 1, Forward Execution Time (us) : 12.597
Run: 2, Forward Execution Time (us) : 12.670

# Benchmarking PyTorch: digamma_
# Mode: Eager
# Name: digamma__M512_N512_cpu
# Input: M: 512, N: 512, device: cpu
Run: 0, Forward Execution Time (us) : 16369.473
Run: 1, Forward Execution Time (us) : 16399.018
Run: 2, Forward Execution Time (us) : 16362.769

# Benchmarking PyTorch: digamma_
# Mode: Eager
# Name: digamma__M512_N512_cuda
# Input: M: 512, N: 512, device: cuda
Run: 0, Forward Execution Time (us) : 14.189
Run: 1, Forward Execution Time (us) : 14.347
Run: 2, Forward Execution Time (us) : 14.332

# Benchmarking PyTorch: exponential_
# Mode: Eager
# Name: exponential__M512_N512_cpu
# Input: M: 512, N: 512, device: cpu
Run: 0, Forward Execution Time (us) : 9750.992
Run: 1, Forward Execution Time (us) : 9744.793
Run: 2, Forward Execution Time (us) : 9791.135

# Benchmarking PyTorch: exponential_
# Mode: Eager
# Name: exponential__M512_N512_cuda
# Input: M: 512, N: 512, device: cuda
Run: 0, Forward Execution Time (us) : 12.496
Run: 1, Forward Execution Time (us) : 12.632
Run: 2, Forward Execution Time (us) : 12.610

# Benchmarking PyTorch: normal_
# Mode: Eager
# Name: normal__M512_N512_cpu
# Input: M: 512, N: 512, device: cpu
Run: 0, Forward Execution Time (us) : 3988.298
Run: 1, Forward Execution Time (us) : 3987.814
Run: 2, Forward Execution Time (us) : 3998.126

# Benchmarking PyTorch: normal_
# Mode: Eager
# Name: normal__M512_N512_cuda
# Input: M: 512, N: 512, device: cuda
Run: 0, Forward Execution Time (us) : 12.680
Run: 1, Forward Execution Time (us) : 12.757
Run: 2, Forward Execution Time (us) : 12.826

# Benchmarking PyTorch: random_
# Mode: Eager
# Name: random__M512_N512_cpu
# Input: M: 512, N: 512, device: cpu
Run: 0, Forward Execution Time (us) : 3747.714
Run: 1, Forward Execution Time (us) : 3742.184
Run: 2, Forward Execution Time (us) : 3745.615

# Benchmarking PyTorch: random_
# Mode: Eager
# Name: random__M512_N512_cuda
# Input: M: 512, N: 512, device: cuda
Run: 0, Forward Execution Time (us) : 12.481
Run: 1, Forward Execution Time (us) : 12.625
Run: 2, Forward Execution Time (us) : 12.659

# Benchmarking PyTorch: sign_
# Mode: Eager
# Name: sign__M512_N512_cpu
# Input: M: 512, N: 512, device: cpu
Run: 0, Forward Execution Time (us) : 60.669
Run: 1, Forward Execution Time (us) : 60.153
Run: 2, Forward Execution Time (us) : 61.634

# Benchmarking PyTorch: sign_
# Mode: Eager
# Name: sign__M512_N512_cuda
# Input: M: 512, N: 512, device: cuda
Run: 0, Forward Execution Time (us) : 10.690
Run: 1, Forward Execution Time (us) : 10.797
Run: 2, Forward Execution Time (us) : 10.815

# Benchmarking PyTorch: uniform_
# Mode: Eager
# Name: uniform__M512_N512_cpu
# Input: M: 512, N: 512, device: cpu
Run: 0, Forward Execution Time (us) : 3742.671
Run: 1, Forward Execution Time (us) : 3760.506
Run: 2, Forward Execution Time (us) : 3732.521

# Benchmarking PyTorch: uniform_
# Mode: Eager
# Name: uniform__M512_N512_cuda
# Input: M: 512, N: 512, device: cuda
Run: 0, Forward Execution Time (us) : 12.577
Run: 1, Forward Execution Time (us) : 12.750
Run: 2, Forward Execution Time (us) : 12.698

# Benchmarking PyTorch: half
# Mode: Eager
# Name: half_M512_N512_cpu
# Input: M: 512, N: 512, device: cpu
Run: 0, Forward Execution Time (us) : 902.019
Run: 1, Forward Execution Time (us) : 903.515
Run: 2, Forward Execution Time (us) : 902.598

# Benchmarking PyTorch: half
# Mode: Eager
# Name: half_M512_N512_cuda
# Input: M: 512, N: 512, device: cuda
Run: 0, Forward Execution Time (us) : 18.145
Run: 1, Forward Execution Time (us) : 18.297
Run: 2, Forward Execution Time (us) : 18.351

# Benchmarking PyTorch: long
# Mode: Eager
# Name: long_M512_N512_cpu
# Input: M: 512, N: 512, device: cpu
Run: 0, Forward Execution Time (us) : 224.007
Run: 1, Forward Execution Time (us) : 263.436
Run: 2, Forward Execution Time (us) : 263.839

# Benchmarking PyTorch: long
# Mode: Eager
# Name: long_M512_N512_cuda
# Input: M: 512, N: 512, device: cuda
Run: 0, Forward Execution Time (us) : 18.485
Run: 1, Forward Execution Time (us) : 18.581
Run: 2, Forward Execution Time (us) : 18.451

+ exit 0
