- schema: "exampleop.overloadname(Tensor self, Scalar other, Scalar alpha=1, *, Tensor(a!) out) -> Tensor(a!)"
  register_op: False  # Whether generate registe code for this op, default value is True
  print_func_call_info: False # whether generate code that prints function call information
  dummy_call_diopi: False # Does not generate code that actually calls the diopi function, defalut value is False
  custom_code_at_the_beginning: "/* Here can be a piece of c++ code at the begining*/"
  custom_code_before_call_diopi: >
    std::cout << "self:" << self << std::endl;
    std::cout << "other:" << other << std::endl;
  custom_code_before_return: >
    dipu::getCurrentDIPUStream().synchronize();
    std::cout << "out:" << out << std::endl;
  interface: diopiAddScalar(ctx, out, self, other, alpha)

- schema: "aten::add.Scalar_out(Tensor self, Scalar other, Scalar alpha=1, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiAddScalar(ctx, out, self, other, alpha)

- schema: "aten::add.out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)"
  custom_code_at_the_beginning: >
    if (other.numel() == 1 && other.is_cpu()) {
        return dipu_add_scalar_out(self, other.item(), alpha, out);
    } else if (self.numel() == 1 && self.is_cpu()) {
        return dipu_add_scalar_out(other, self.item(), alpha, out);
    }
  interface: diopiAdd(ctx, out, self, other, alpha)

- schema: "aten::sub.Scalar_out(Tensor self, Scalar other, Scalar alpha=1, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiSubScalar(ctx, out, self, other, alpha)

- schema: "aten::sub.out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)"
  custom_code_at_the_beginning: >
    if (other.numel() == 1 && other.is_cpu()) {
        return dipu_sub_scalar_out(self, other.item(), alpha, out);
    } else if (self.numel() == 1 && self.is_cpu()) {
        return dipu_sub_scalar_out(other, self.item(), alpha, out);
    }
  interface: diopiSub(ctx, out, self, other, alpha)

- schema: "div.Scalar(Tensor self, Scalar other) -> Tensor"
  custom_code_at_the_beginning: >
    auto out = at::empty_like(self);
  interface: diopiDivScalar(ctx, out, self, other, RoundModeNone)

- schema: "div_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)"
  interface: diopiDivInpScalar(ctx, self, other, RoundModeNone)

- schema: "div_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)"
  custom_code_at_the_beginning: >
    if (other.numel() == 1 && other.is_cpu()) {
        return dipu_div__scalar(self, other.item());
    }
  interface: diopiDivInp(ctx, self, other, RoundModeNone)

- schema: "aten::div.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  custom_code_at_the_beginning: >
    if (other.numel() == 1 && other.is_cpu()) {
        out = dipu_div_scalar(self, other.item());
        return out;
    } else if (self.numel() == 1 && self.is_cpu()) {
        out = dipu_div_scalar(other, self.item());
        return out;
    }
  interface: diopiDiv(ctx, out, self, other, RoundModeNone)

- schema: "aten::div.Scalar_mode_out(Tensor self, Scalar other, *, str? rounding_mode, Tensor(a!) out) -> Tensor(a!)"
  custom_code_at_the_beginning: >
    const auto mode = toDiopiRoundMode(rounding_mode.has_value() ? rounding_mode.value().data():"none");
  interface: diopiDivScalar(ctx, out, self, other, mode)

- schema: "aten::div.out_mode(Tensor self, Tensor other, *, str? rounding_mode, Tensor(a!) out) -> Tensor(a!)"
  custom_code_at_the_beginning: >
    if (other.numel() == 1 && other.is_cpu()) {
        return dipu_div_scalar_mode_out(self, other.item(), rounding_mode, out);
    } else if (self.numel() == 1 && self.is_cpu()) {
        return dipu_div_scalar_mode_out(other, self.item(), rounding_mode, out);
    }
    const auto mode = toDiopiRoundMode(rounding_mode.has_value() ? rounding_mode.value().data():"none");
  interface: diopiDiv(ctx, out, self, other, mode)

- schema: "aten::fill_.Scalar(Tensor(a!) self, Scalar value) -> Tensor(a!)"
  interface: diopiFill(ctx, self, value)

- schema: "mul.Scalar(Tensor self, Scalar other) -> Tensor"
  custom_code_at_the_beginning: auto out = at::empty_like(self);
  interface: diopiMulScalar(ctx, out, self, other)

- schema: "mul_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)"
  interface: diopiMulInpScalar(ctx, self, other)

- schema: "mul_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)"
  custom_code_at_the_beginning: >
    if (other.numel() == 1 && other.is_cpu()) {
        return dipu_mul__scalar(self, other.item());
    }
  interface: diopiMulInp(ctx, self, other)

- schema: "mul.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  custom_code_at_the_beginning: >
    if (other.numel() == 1 && other.is_cpu()) {
        out = dipu_mul_scalar(self, other.item());
        return out;
    } else if (self.numel() == 1 && self.is_cpu()) {
        out = dipu_mul_scalar(other, self.item());
        return out;
    }
  interface: diopiMul(ctx, out, self, other)

- schema: "aten::native_batch_norm.out(Tensor input, Tensor? weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float momentum, float eps, *, Tensor(a!) out, Tensor(b!) save_mean, Tensor(c!) save_invstd) -> (Tensor(a!), Tensor(b!), Tensor(c!))"
  interface: diopiBatchNorm(ctx, out, save_mean, save_invstd, input, weight, bias, const_cast<diopiTensorHandle_t>(running_mean), const_cast<diopiTensorHandle_t>(running_var), training, momentum, eps);

- schema: "aten::native_batch_norm(Tensor input, Tensor? weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float momentum, float eps) -> (Tensor, Tensor, Tensor)"
  custom_code_at_the_beginning: >
    const int64_t dim_c = input.size(1);
    auto out0 = at::empty_like(input);
    auto options = input.options().dtype(at::kFloat);
    auto out1 = at::empty({dim_c}, options);
    auto out2 = at::empty({dim_c}, options);
  interface: diopiBatchNorm(ctx, out0, out1, out2, input, weight, bias, const_cast<diopiTensorHandle_t>(running_mean), const_cast<diopiTensorHandle_t>(running_var), training, momentum, eps);

- schema: "native_batch_norm_backward(Tensor grad_out, Tensor input, Tensor? weight, Tensor? running_mean, Tensor? running_var, Tensor? save_mean, Tensor? save_invstd, bool train, float eps, bool[3] output_mask) -> (Tensor, Tensor, Tensor)"
  custom_code_at_the_beginning: >
    int64_t dim_c = input.size(1);
    auto options = input.options().dtype(at::kFloat);
    at::Tensor out0 = at::empty(input.sizes(), input.options());
    at::Tensor out1 = at::empty({dim_c}, options);
    at::Tensor out2 = at::empty({dim_c}, options);
  interface: diopiBatchNormBackward(ctx, out0, out1, out2, grad_out, input, weight, running_mean, running_var, save_mean, save_invstd, train, eps)

- schema: "adaptive_avg_pool2d.out(Tensor self, SymInt[2] output_size, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiAdaptiveAvgPool2d(ctx, out, self, output_size)

- schema: "_adaptive_avg_pool2d(Tensor self, SymInt[2] output_size) -> Tensor"
  custom_code_at_the_beginning: >
    TORCH_CHECK(output_size.size() == 2, __func__, ":", __FILE__, ":", __LINE__,
        " output_size should equal 2, size is ", output_size.size());
    auto out_tensor_size = self.sizes().vec();
    out_tensor_size[self.dim() - 2] = output_size[0].expect_int();
    out_tensor_size[self.dim() - 1] = output_size[1].expect_int();
    at::Tensor out = at::empty(out_tensor_size, self.options());
  interface: diopiAdaptiveAvgPool2d(ctx, out, self, output_size)

- schema: "_adaptive_avg_pool2d_backward(Tensor grad_output, Tensor self) -> Tensor"
  custom_code_at_the_beginning: >
    auto out = at::empty_like(self);
  interface: diopiAdaptiveAvgPool2dBackward(ctx, out, grad_output, self);

- schema: "eq.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiEqScalar(ctx, out, self, other)

- schema: "eq.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  custom_code_at_the_beginning: >
    if (other.numel() == 1 && other.is_cpu()) {
        out = dipu_eq_scalar_out(self, other.item(), out);
        return out;
    } else if (self.numel() == 1 && self.is_cpu()) {
        out = dipu_eq_scalar_out(other, self.item(), out);
        return out;
    }
  interface: diopiEq(ctx, out, self, other)

- schema: "eq_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)"
  interface: diopiEqInpScalar(ctx, self, other)

- schema: "eq_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)"
  custom_code_at_the_beginning: >
    if (other.numel() == 1 && other.is_cpu()) {
        return dipu_eq__scalar(self, other.item());
    }
  interface: diopiEqInp(ctx, self, other)

- schema: "lt.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiLtScalar(ctx, out, self, other)

- schema: "lt.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  custom_code_at_the_beginning: >
    if (other.numel() == 1 && other.is_cpu()) {
        out = dipu_lt_scalar_out(self, other.item(), out);
        return out;
    } else if (self.numel() == 1 && self.is_cpu()) {
        out = dipu_lt_scalar_out(other, self.item(), out);
        return out;
    }
  interface: diopiLt(ctx, out, self, other)

- schema: "lt_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)"
  interface: diopiLtInpScalar(ctx, self, other)

- schema: "lt_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)"
  custom_code_at_the_beginning: >
    if (other.numel() == 1 && other.is_cpu()) {
        return dipu_lt__scalar(self, other.item());
    }
  interface: diopiLtInp(ctx, self, other)

- schema: "ne.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiNeScalar(ctx, out, self, other)

- schema: "ne.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  custom_code_at_the_beginning: >
    if (other.numel() == 1 && other.is_cpu()) {
        out = dipu_ne_scalar_out(self, other.item(), out);
        return out;
    } else if (self.numel() == 1 && self.is_cpu()) {
        out = dipu_ne_scalar_out(other, self.item(), out);
        return out;
    }
  interface: diopiNe(ctx, out, self, other)

- schema: "ne_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)"
  interface: diopiNeInpScalar(ctx, self, other)

- schema: "ne_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)"
  custom_code_at_the_beginning: >
    if (other.numel() == 1 && other.is_cpu()) {
        return dipu_ne__scalar(self, other.item());
    }
  interface: diopiNeInp(ctx, self, other)

- schema: "ge.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiGeScalar(ctx, out, self, other)

- schema: "ge.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  custom_code_at_the_beginning: >
    if (other.numel() == 1 && other.is_cpu()) {
        out = dipu_ge_scalar_out(self, other.item(), out);
        return out;
    } else if (self.numel() == 1 && self.is_cpu()) {
        out = dipu_ge_scalar_out(other, self.item(), out);
        return out;
    }
  interface: diopiGe(ctx, out, self, other)

- schema: "ge_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)"
  interface: diopiGeInpScalar(ctx, self, other)

- schema: "ge_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)"
  custom_code_at_the_beginning: >
    if (other.numel() == 1 && other.is_cpu()) {
        return dipu_ge__scalar(self, other.item());
    }
  interface: diopiGeInp(ctx, self, other)

- schema: "gt.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiGtScalar(ctx, out, self, other)

- schema: "gt.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  custom_code_at_the_beginning: >
    if (other.numel() == 1 && other.is_cpu()) {
        out = dipu_gt_scalar_out(self, other.item(), out);
        return out;
    } else if (self.numel() == 1 && self.is_cpu()) {
        out = dipu_gt_scalar_out(other, self.item(), out);
        return out;
    }
  interface: diopiGt(ctx, out, self, other)

- schema: "gt_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)"
  interface: diopiGtInpScalar(ctx, self, other)

- schema: "gt_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)"
  custom_code_at_the_beginning: >
    if (other.numel() == 1 && other.is_cpu()) {
        return dipu_gt__scalar(self, other.item());
    }
  interface: diopiGtInp(ctx, self, other)

- schema: "le.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiLeScalar(ctx, out, self, other)

- schema: "le.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  custom_code_at_the_beginning: >
    if (other.numel() == 1 && other.is_cpu()) {
        out = dipu_le_scalar_out(self, other.item(), out);
        return out;
    } else if (self.numel() == 1 && self.is_cpu()) {
        out = dipu_le_scalar_out(other, self.item(), out);
        return out;
    }
  interface: diopiLe(ctx, out, self, other)

- schema: "le_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)"
  interface: diopiLeInpScalar(ctx, self, other)

- schema: "le_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)"
  custom_code_at_the_beginning: >
    if (other.numel() == 1 && other.is_cpu()) {
        return dipu_le__scalar(self, other.item());
    }
  interface: diopiLeInp(ctx, self, other)

- schema: "relu_(Tensor(a!) self) -> Tensor(a!)"
  interface: diopiReluInp(ctx, self)

- schema: "relu(Tensor self) -> Tensor"
  custom_code_at_the_beginning: auto out = at::empty_like(self);
  interface: diopiRelu(ctx, out, self)

- schema: "randperm.out(int n, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiRandperm(ctx, out, n, 0)

- schema: "randperm.generator_out(int n, *, Generator? generator, Tensor(a!) out) -> Tensor(a!)"
  custom_code_at_the_beginning: >
    const int64_t seed = (generator.has_value() && generator.value().defined()) ? generator.value().seed() : 0;
  interface: diopiRandperm(ctx, out, n, seed)

- schema: "aten::sum.IntList_out(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)"
  custom_code_at_the_beginning: >
    ::diopiSize_t diopi_size = toDiopiSize(dim);
  interface: diopiSum(ctx, out, self, diopi_size)

- schema: "addmm.out(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)"
  custom_code_at_the_beginning: >
  interface: diopiAddmm(&context, out, self, mat1, mat2, beta, alpha)

- schema: "cross_entropy_loss_impl(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, SymInt ignore_index=-100, float label_smoothing=0.0) -> Tensor"
  register_op: False
  custom_code_at_the_beginning: >
    const int64_t ignore_index_int = ignore_index.expect_int();
    const auto reductionDiopi = static_cast<::diopiReduction_t>(reduction);
    at::Tensor out;
    auto options = self.options();
    if (reductionDiopi == ReductionNone) {
      out = at::empty(target.sizes(), options);
    } else {
      out = at::empty({}, options);
    }
  interface: diopiCrossEntropyLoss(ctx, out, self, target, weight, reductionDiopi, ignore_index_int, label_smoothing)

- schema: "cross_entropy_loss_backward_impl(Tensor grad_output, Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, SymInt ignore_index=-100, float label_smoothing=0.0) -> Tensor"
  register_op: False
  custom_code_at_the_beginning: >
    const auto reductionDiopi = static_cast<::diopiReduction_t>(reduction);
    const int64_t ignore_index_int = ignore_index.expect_int();
    at::Tensor out = at::empty_like(self);
  autograd: True
  interface: diopiCrossEntropyLossBackward(ctx, out, grad_output, self, target, weight, reductionDiopi, ignore_index_int, label_smoothing)

- schema: "log.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiLog(ctx, out, self)

- schema: "log_(Tensor(a!) self) -> Tensor(a!)"
  interface: diopiLogInp(ctx, self)

- schema: "log2.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiLog2(ctx, out, self)

- schema: "log2_(Tensor(a!) self) -> Tensor(a!)"
  interface: diopiLog2Inp(ctx, self)

- schema: "abs(Tensor self) -> Tensor"
  custom_code_at_the_beginning: >
    auto out = at::empty_like(self);
  interface: diopiAbs(ctx, out, self)

- schema: "abs_(Tensor(a!) self) -> Tensor(a!)"
  interface: diopiAbsInp(ctx, self)

- schema: "abs.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiAbs(ctx, out, self)

- schema: "neg.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiNeg(ctx, out, self)

- schema: "neg_(Tensor(a!) self) -> Tensor(a!)"
  interface: diopiNegInp(ctx, self)

- schema: "sqrt.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiSqrt(ctx, out, self)

- schema: "sqrt_(Tensor(a!) self) -> Tensor(a!)"
  interface: diopiSqrtInp(ctx, self)

- schema: "all.out(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiAll(ctx, out, self, &dim)

- schema: "all.all_out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiAll(ctx, out, self, nullptr)

- schema: "any.all_out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiAny(ctx, out, self, nullptr)

- schema: "any.out(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiAny(ctx, out, self, &dim)

- schema: "topk.values(Tensor self, int k, int dim=-1, bool largest=True, bool sorted=True, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)"
  interface: diopiTopk(ctx, values, indices, self, k, dim, largest, sorted)

- schema: "topk(Tensor self, int k, int dim=-1, bool largest=True, bool sorted=True) -> (Tensor values, Tensor indices)"
  custom_code_at_the_beginning: >
    std::vector<int64_t> output_size(self.sizes().begin(), self.sizes().end());
    dim = dim < 0 ? (dim + output_size.size()) : dim;
    output_size[dim] = k;
    auto values = at::empty(output_size, self.options());
    auto indices = at::empty(output_size, self.options().dtype(at::kLong));
  interface: diopiTopk(ctx, values, indices, self, k, dim, largest, sorted)

- schema: "dropout_impl(Tensor input, float p, bool train, *, Tensor(a!) out, Tensor(b!) mask) -> (Tensor(a!), Tensor(b!))"
  register_op: False
  print_func_call_info: True
  interface: diopiDropout(ctx, out, mask, input, p, train)

- schema: "dropout__impl(Tensor(a!) input, Tensor(b!) mask, float p, bool train) -> (Tensor(a!), Tensor(b!))"
  print_func_call_info: True
  register_op: False
  interface: diopiDropoutInp(ctx, input, mask, p, train)

- schema: "rsqrt.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiRsqrt(ctx, out, self)

- schema: "uniform_(Tensor(a!) self, float from=0, float to=1, *, Generator? generator=None) -> Tensor(a!)"
  custom_code_at_the_beginning: >
    const int64_t seed = (generator.has_value() && generator.value().defined()) ? generator.value().seed() : 0;
  interface: diopiUniformInp(ctx, self, from, to, seed)

- schema: "tril(Tensor self, int diagonal=0) -> Tensor"
  custom_code_at_the_beginning: >
    auto out = at::empty_like(self);
  interface: diopiTril(ctx, out, self, diagonal)

- schema: "tril.out(Tensor self, int diagonal=0, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiTril(ctx, out, self, diagonal)

- schema: "multinomial(Tensor self, int num_samples, bool replacement=False, *, Generator? generator=None) -> Tensor"
  custom_code_at_the_beginning: >
    auto out = at::empty_like(self);
    if (self.dim() == 2){
      out = at::empty({self.size(0), num_samples}, self.options().dtype(at::kLong));
    }
    else if (self.dim() == 1) {
      out = at::empty({num_samples,}, self.options().dtype(at::kLong));
    }
  interface: diopiMultinomial(ctx, out, self, num_samples, replacement)

- schema: "multinomial.out(Tensor self, int num_samples, bool replacement=False, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiMultinomial(ctx, out, self, num_samples, replacement)