- schema: "exampleop.overloadname(Tensor self, Scalar other, Scalar alpha=1, *, Tensor(a!) out) -> Tensor(a!)"
  register_op: False  # Whether generate registe code for this op, default value is True
  print_func_call_info: False # whether generate code that prints function call information
  dummy_call_diopi: False # Does not generate code that actually calls the diopi function, defalut value is False
  custom_code_at_the_beginning: "/* Here can be a piece of c++ code at the begining*/"
  custom_code_before_call_diopi: >
    std::cout << "self:" << self << std::endl;
    std::cout << "other:" << other << std::endl;
  custom_code_before_return: >
    dipu::getCurrentDIPUStream().synchronize();
    std::cout << "out:" << out << std::endl;
  interface: diopiAddScalar(ctx, out, self, other, alpha)

- schema: "aten::add.Scalar_out(Tensor self, Scalar other, Scalar alpha=1, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiAddScalar(ctx, out, self, other, alpha)

- schema: "aten::add.out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)"
  custom_code_at_the_beginning: >
    if (other.numel() == 1 && other.is_cpu()) {
        return dipu_add_scalar_out(self, other.item(), alpha, out);
    } else if (self.numel() == 1 && self.is_cpu()) {
        return dipu_add_scalar_out(other, self.item(), alpha, out);
    }
  interface: diopiAdd(ctx, out, self, other, alpha)

- schema: "aten::sub.Scalar_out(Tensor self, Scalar other, Scalar alpha=1, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiSubScalar(ctx, out, self, other, alpha)

- schema: "aten::sub.out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)"
  custom_code_at_the_beginning: >
    if (other.numel() == 1 && other.is_cpu()) {
        return dipu_sub_scalar_out(self, other.item(), alpha, out);
    } else if (self.numel() == 1 && self.is_cpu()) {
        return dipu_sub_scalar_out(other, self.item(), alpha, out);
    }
  interface: diopiSub(ctx, out, self, other, alpha)

- schema: "div.Scalar(Tensor self, Scalar other) -> Tensor"
  custom_code_at_the_beginning: >
    auto out = at::empty_like(self);
  interface: diopiDivScalar(ctx, out, self, other, RoundModeNone)

- schema: "div_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)"
  interface: diopiDivInpScalar(ctx, self, other, RoundModeNone)

- schema: "div_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)"
  custom_code_at_the_beginning: >
    if (other.numel() == 1 && other.is_cpu()) {
        return dipu_div__scalar(self, other.item());
    }
  interface: diopiDivInp(ctx, self, other, RoundModeNone)

- schema: "aten::div.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  custom_code_at_the_beginning: >
    if (other.numel() == 1 && other.is_cpu()) {
        out = dipu_div_scalar(self, other.item());
        return out;
    } else if (self.numel() == 1 && self.is_cpu()) {
        out = dipu_div_scalar(other, self.item());
        return out;
    }
  interface: diopiDiv(ctx, out, self, other, RoundModeNone)

- schema: "aten::div.Scalar_mode_out(Tensor self, Scalar other, *, str? rounding_mode, Tensor(a!) out) -> Tensor(a!)"
  custom_code_at_the_beginning: >
    const auto mode = toDiopiRoundMode(rounding_mode.has_value() ? rounding_mode.value().data():"none");
  interface: diopiDivScalar(ctx, out, self, other, mode)

- schema: "aten::div.out_mode(Tensor self, Tensor other, *, str? rounding_mode, Tensor(a!) out) -> Tensor(a!)"
  custom_code_at_the_beginning: >
    if (other.numel() == 1 && other.is_cpu()) {
        return dipu_div_scalar_mode_out(self, other.item(), rounding_mode, out);
    } else if (self.numel() == 1 && self.is_cpu()) {
        return dipu_div_scalar_mode_out(other, self.item(), rounding_mode, out);
    }
    const auto mode = toDiopiRoundMode(rounding_mode.has_value() ? rounding_mode.value().data():"none");
  interface: diopiDiv(ctx, out, self, other, mode)

- schema: "aten::fill_.Scalar(Tensor(a!) self, Scalar value) -> Tensor(a!)"
  interface: diopiFill(ctx, self, value)

- schema: "mul.Scalar(Tensor self, Scalar other) -> Tensor"
  custom_code_at_the_beginning: auto out = at::empty_like(self);
  interface: diopiMulScalar(ctx, out, self, other)

- schema: "mul_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)"
  interface: diopiMulInpScalar(ctx, self, other)

- schema: "mul_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)"
  custom_code_at_the_beginning: >
    if (other.numel() == 1 && other.is_cpu()) {
        return dipu_mul__scalar(self, other.item());
    }
  interface: diopiMulInp(ctx, self, other)

- schema: "mul.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  custom_code_at_the_beginning: >
    if (other.numel() == 1 && other.is_cpu()) {
        out = dipu_mul_scalar(self, other.item());
        return out;
    } else if (self.numel() == 1 && self.is_cpu()) {
        out = dipu_mul_scalar(other, self.item());
        return out;
    }
  interface: diopiMul(ctx, out, self, other)

- schema: "aten::native_batch_norm.out(Tensor input, Tensor? weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float momentum, float eps, *, Tensor(a!) out, Tensor(b!) save_mean, Tensor(c!) save_invstd) -> (Tensor(a!), Tensor(b!), Tensor(c!))"
  interface: diopiBatchNorm(ctx, out, save_mean, save_invstd, input, weight, bias, const_cast<diopiTensorHandle_t>(running_mean), const_cast<diopiTensorHandle_t>(running_var), training, momentum, eps);

- schema: "aten::native_batch_norm(Tensor input, Tensor? weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float momentum, float eps) -> (Tensor, Tensor, Tensor)"
  custom_code_at_the_beginning: >
    const int64_t dim_c = input.size(1);
    auto out0 = at::empty_like(input);
    auto options = input.options().dtype(at::kFloat);
    auto out1 = at::empty({dim_c}, options);
    auto out2 = at::empty({dim_c}, options);
  interface: diopiBatchNorm(ctx, out0, out1, out2, input, weight, bias, const_cast<diopiTensorHandle_t>(running_mean), const_cast<diopiTensorHandle_t>(running_var), training, momentum, eps);

- schema: "native_batch_norm_backward(Tensor grad_out, Tensor input, Tensor? weight, Tensor? running_mean, Tensor? running_var, Tensor? save_mean, Tensor? save_invstd, bool train, float eps, bool[3] output_mask) -> (Tensor, Tensor, Tensor)"
  custom_code_at_the_beginning: >
    int64_t dim_c = input.size(1);
    auto options = input.options().dtype(at::kFloat);
    at::Tensor out0 = at::empty(input.sizes(), input.options());
    at::Tensor out1 = at::empty({dim_c}, options);
    at::Tensor out2 = at::empty({dim_c}, options);
  interface: diopiBatchNormBackward(ctx, out0, out1, out2, grad_out, input, weight, running_mean, running_var, save_mean, save_invstd, train, eps)

- schema: "adaptive_avg_pool2d.out(Tensor self, SymInt[2] output_size, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiAdaptiveAvgPool2d(ctx, out, self, output_size)

- schema: "_adaptive_avg_pool2d(Tensor self, SymInt[2] output_size) -> Tensor"
  custom_code_at_the_beginning: >
    TORCH_CHECK(output_size.size() == 2, __func__, ":", __FILE__, ":", __LINE__,
        " output_size should equal 2, size is ", output_size.size());
    auto out_tensor_size = self.sizes().vec();
    out_tensor_size[self.dim() - 2] = output_size[0].expect_int();
    out_tensor_size[self.dim() - 1] = output_size[1].expect_int();
    at::Tensor out = at::empty(out_tensor_size, self.options());
  interface: diopiAdaptiveAvgPool2d(ctx, out, self, output_size)

- schema: "_adaptive_avg_pool2d_backward(Tensor grad_output, Tensor self) -> Tensor"
  custom_code_at_the_beginning: >
    auto out = at::empty_like(self);
  interface: diopiAdaptiveAvgPool2dBackward(ctx, out, grad_output, self);

- schema: "eq.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiEqScalar(ctx, out, self, other)

- schema: "eq.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  custom_code_at_the_beginning: >
    if (other.numel() == 1 && other.is_cpu()) {
        out = dipu_eq_scalar_out(self, other.item(), out);
        return out;
    } else if (self.numel() == 1 && self.is_cpu()) {
        out = dipu_eq_scalar_out(other, self.item(), out);
        return out;
    }
  interface: diopiEq(ctx, out, self, other)

- schema: "eq_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)"
  interface: diopiEqInpScalar(ctx, self, other)

- schema: "eq_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)"
  custom_code_at_the_beginning: >
    if (other.numel() == 1 && other.is_cpu()) {
        return dipu_eq__scalar(self, other.item());
    }
  interface: diopiEqInp(ctx, self, other)

- schema: "lt.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiLtScalar(ctx, out, self, other)

- schema: "lt.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  custom_code_at_the_beginning: >
    if (other.numel() == 1 && other.is_cpu()) {
        out = dipu_lt_scalar_out(self, other.item(), out);
        return out;
    } else if (self.numel() == 1 && self.is_cpu()) {
        out = dipu_lt_scalar_out(other, self.item(), out);
        return out;
    }
  interface: diopiLt(ctx, out, self, other)

- schema: "lt_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)"
  interface: diopiLtInpScalar(ctx, self, other)

- schema: "lt_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)"
  custom_code_at_the_beginning: >
    if (other.numel() == 1 && other.is_cpu()) {
        return dipu_lt__scalar(self, other.item());
    }
  interface: diopiLtInp(ctx, self, other)

- schema: "ne.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiNeScalar(ctx, out, self, other)

- schema: "ne.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  custom_code_at_the_beginning: >
    if (other.numel() == 1 && other.is_cpu()) {
        out = dipu_ne_scalar_out(self, other.item(), out);
        return out;
    } else if (self.numel() == 1 && self.is_cpu()) {
        out = dipu_ne_scalar_out(other, self.item(), out);
        return out;
    }
  interface: diopiNe(ctx, out, self, other)

- schema: "ne_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)"
  interface: diopiNeInpScalar(ctx, self, other)

- schema: "ne_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)"
  custom_code_at_the_beginning: >
    if (other.numel() == 1 && other.is_cpu()) {
        return dipu_ne__scalar(self, other.item());
    }
  interface: diopiNeInp(ctx, self, other)

- schema: "ge.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiGeScalar(ctx, out, self, other)

- schema: "ge.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  custom_code_at_the_beginning: >
    if (other.numel() == 1 && other.is_cpu()) {
        out = dipu_ge_scalar_out(self, other.item(), out);
        return out;
    } else if (self.numel() == 1 && self.is_cpu()) {
        out = dipu_ge_scalar_out(other, self.item(), out);
        return out;
    }
  interface: diopiGe(ctx, out, self, other)

- schema: "ge_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)"
  interface: diopiGeInpScalar(ctx, self, other)

- schema: "ge_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)"
  custom_code_at_the_beginning: >
    if (other.numel() == 1 && other.is_cpu()) {
        return dipu_ge__scalar(self, other.item());
    }
  interface: diopiGeInp(ctx, self, other)

- schema: "gt.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiGtScalar(ctx, out, self, other)

- schema: "gt.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  custom_code_at_the_beginning: >
    if (other.numel() == 1 && other.is_cpu()) {
        out = dipu_gt_scalar_out(self, other.item(), out);
        return out;
    } else if (self.numel() == 1 && self.is_cpu()) {
        out = dipu_gt_scalar_out(other, self.item(), out);
        return out;
    }
  interface: diopiGt(ctx, out, self, other)

- schema: "gt_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)"
  interface: diopiGtInpScalar(ctx, self, other)

- schema: "gt_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)"
  custom_code_at_the_beginning: >
    if (other.numel() == 1 && other.is_cpu()) {
        return dipu_gt__scalar(self, other.item());
    }
  interface: diopiGtInp(ctx, self, other)

- schema: "le.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiLeScalar(ctx, out, self, other)

- schema: "le.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  custom_code_at_the_beginning: >
    if (other.numel() == 1 && other.is_cpu()) {
        out = dipu_le_scalar_out(self, other.item(), out);
        return out;
    } else if (self.numel() == 1 && self.is_cpu()) {
        out = dipu_le_scalar_out(other, self.item(), out);
        return out;
    }
  interface: diopiLe(ctx, out, self, other)

- schema: "le_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)"
  interface: diopiLeInpScalar(ctx, self, other)

- schema: "le_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)"
  custom_code_at_the_beginning: >
    if (other.numel() == 1 && other.is_cpu()) {
        return dipu_le__scalar(self, other.item());
    }
  interface: diopiLeInp(ctx, self, other)

- schema: "relu_(Tensor(a!) self) -> Tensor(a!)"
  interface: diopiReluInp(ctx, self)

- schema: "relu(Tensor self) -> Tensor"
  custom_code_at_the_beginning: auto out = at::empty_like(self);
  interface: diopiRelu(ctx, out, self)

- schema: "randperm.out(int n, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiRandperm(ctx, out, n, 0)

- schema: "randperm.generator_out(int n, *, Generator? generator, Tensor(a!) out) -> Tensor(a!)"
  custom_code_at_the_beginning: >
    const int64_t seed = (generator.has_value() && generator.value().defined()) ? generator.value().seed() : 0;
  interface: diopiRandperm(ctx, out, n, seed)

- schema: "aten::sum.IntList_out(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)"
  custom_code_at_the_beginning: >
    ::diopiSize_t diopi_size = toDiopiSize(dim);
  interface: diopiSum(ctx, out, self, diopi_size)

- schema: "addmm.out(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)"
  custom_code_at_the_beginning: >
  interface: diopiAddmm(&context, out, self, mat1, mat2, beta, alpha)

- schema: "cross_entropy_loss_backward(Tensor grad_output, Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, SymInt ignore_index=-100, float label_smoothing=0.0) -> Tensor"
  register_op: False
  custom_code_at_the_beginning: >
    const auto reductionDiopi = static_cast<::diopiReduction_t>(reduction);
    at::Tensor out = at::empty_like(self);
  interface: diopiCrossEntropyLossBackward(ctx, out, grad_output, self, target, weight, reductionDiopi, ignore_index.expect_int(), label_smoothing)


- schema: "cross_entropy_loss(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, SymInt ignore_index=-100, float label_smoothing=0.0) -> Tensor"
  register_op: True
  autograd: True
  custom_code_at_the_beginning: >
    const int64_t ignore_index_int = ignore_index.expect_int();
    const auto reductionDiopi = static_cast<::diopiReduction_t>(reduction);
    at::Tensor out;
    auto options = self.options();
    if (reductionDiopi == ReductionNone) {
      out = at::empty(target.sizes(), options);
    } else {
      out = at::empty({}, options);
    }
  interface: diopiCrossEntropyLoss(ctx, out, self, target, weight, reductionDiopi, ignore_index_int, label_smoothing)
  backward_schema: "cross_entropy_loss_backward(Tensor grad_output, Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, SymInt ignore_index=-100, float label_smoothing=0.0) -> Tensor"
  saved_data: [reduction, ignore_index, label_smoothing, weight, self, target]
  cal_grad_code: >
    auto grad_output = grad_outputs.at(0);
    auto reduction = reduction_.toInt();
    auto ignore_index = ignore_index_.toInt();
    auto label_smoothing = label_smoothing_.toDouble();
    auto weight = weight_.toOptional<at::Tensor>();
    auto self = self_.toTensor();
    auto target = target_.toTensor();
  backward_return_code: >
    std::vector<at::Tensor> outputs(6);
    outputs[0] = result;
    return outputs;


- schema: "convolution_backward(Tensor grad_output, Tensor input, Tensor weight, int[] bias_sizes, int[] stride, int[] padding, int[] dilation, bool transposed, int[] output_padding, int groups, bool[3] output_mask) -> (Tensor grad_input, Tensor grad_weight, Tensor grad_bias)"
  register_op: False
  size_attr: [stride, padding, dilation, bias_sizes]
  custom_code_at_the_beginning: >
    at::Tensor grad_input;
    at::Tensor grad_weight;
    at::Tensor grad_bias;
    grad_input = at::empty(input.sizes(), input.options());
    grad_weight = at::empty(weight.sizes(), weight.options().dtype(at::kFloat));
    if (output_mask[2]) {
        c10::IntArrayRef bias_size = { grad_output.size(1) };
        grad_bias = at::empty(bias_size, grad_output.options());
    }
    ::diopiSize_t output_paddingDiopiSize;
  custom_code_before_call_diopi: >
    ::diopiSize_t* bias_sizes_ptr = output_mask[2] ? &bias_sizesDiopiSize : nullptr;
  interface: diopiConvolution2dBackward(ctx, grad_input, grad_weight, grad_bias, grad_output, input, weight, bias_sizes_ptr, stride, padding, dilation, false, output_paddingDiopiSize, groups);

- schema: conv2d(Tensor input, Tensor weight, Tensor? bias=None, int[2] stride=1, int[2] padding=0, int[2] dilation=1, int groups=1) -> Tensor
  size_attr: [stride, padding, dilation]
  custom_code_at_the_beginning: >
    int64_t batch_size = input.size(0);
    int64_t height = input.size(2);
    int64_t width = input.size(3);
    int64_t out_channel = weight.size(0);
    auto kernel_size = weight.sizes().slice(2);
    int64_t out_height = (height + 2 * padding[0] - dilation[0] * (kernel_size[0] - 1) - 1) / stride[0] + 1;
    int64_t out_width = (width + 2 * padding[1] - dilation[1] * (kernel_size[1] - 1) - 1) / stride[1] + 1;
    c10::SmallVector<int64_t, 8> output_size = {batch_size, out_channel, out_height, out_width};
    at::Tensor out = at::empty(output_size, input.options());
  interface: diopiConvolution2d(&context, out, input, weight, bias, stride, padding, dilation, groups)
  autograd: True
  forward_process_code: >
    bool bias_has_value = (bias.has_value() == true) ? bias.value().requires_grad() : false;
  saved_data: [stride, padding, dilation, groups, bias_has_value, input, weight]
  backward_schema: "convolution_backward(Tensor grad_output, Tensor input, Tensor weight, int[] bias_sizes, int[] stride, int[] padding, int[] dilation, bool transposed, int[] output_padding, int groups, bool[3] output_mask) -> (Tensor grad_input, Tensor grad_weight, Tensor grad_bias)"
  cal_grad_code: >
      auto grad_output = grad_outputs.at(0);
      auto input = input_.toTensor();
      auto weight = weight_.toTensor();
      auto padding = padding_.toIntVector();
      auto stride = stride_.toIntVector();
      auto dilation = dilation_.toIntVector();
      bool bias_has_value = bias_has_value_.toBool();
      auto groups = groups_.toInt();
      std::vector<int64_t> bias_sizes;
      if (bias_has_value) {
        bias_sizes.push_back(grad_output.size(1));
      }
      std::array<bool, 3> output_mask;
      output_mask[0] = input.requires_grad();
      output_mask[1] = weight.requires_grad();
      output_mask[2] = bias_has_value;
      bool transposed = false;
      at::IntArrayRef output_padding{};
  backward_return_code: >
    std::vector<at::Tensor>  outputs = {
          std::get<0>(result), std::get<1>(result), std::get<2>(result),
          at::Tensor(), at::Tensor(), at::Tensor(), at::Tensor()};
    return outputs;

- schema: "dropout_impl(Tensor input, float p, bool train, *, Tensor(a!) mask) -> Tensor"
  custom_code_at_the_beginning: >
    at::Tensor out = at::empty_like(input);
  register_op: False
  interface: diopiDropout(ctx, out, mask, input, p, train)

- schema: "dropout(Tensor input, float p, bool train) -> Tensor"
  register_op: True
  custom_code_at_the_beginning: >
    auto mask = at::empty(input.sizes(), input.options().dtype(at::kByte));
    at::Tensor out = at::empty_like(input);
  interface: diopiDropout(ctx, out, mask, input, p, train)
  outs: [mask]
  autograd: True
  saved_data: [p, mask]
  forward_schema: "dropout_impl(Tensor input, float p, bool train, *, Tensor(a!) mask) -> Tensor"
  forward_process_code: >
    auto mask = at::empty(input.sizes(), input.options().dtype(at::kByte));
    at::Tensor out = at::empty_like(input);
  cal_grad_code: >
        auto p = p_.toDouble();
        double p1m = 1. - p;
        double scale = p1m == 0 ? 0. : 1. / p1m;
        auto mask = mask_.toTensor();
        at::Tensor out = grad_outputs[0] * mask * scale;
  backward_return_code: >
    std::vector<at::Tensor> outputs(6);
    outputs[0] = out;
    return outputs;

- schema: "dropout__impl(Tensor(a!) self, Tensor(b!) mask, float p, bool train) -> Tensor(a!)"
  register_op: False
  interface: diopiDropoutInp(ctx, self, mask, p, train)

- schema: "dropout_(Tensor(a!) self, float p, bool train) -> Tensor(a!)"
  custom_code_at_the_beginning: >
    auto mask = at::empty(self.sizes(), self.options().dtype(at::kByte));
  outs: [mask]
  interface: diopiDropoutInp(ctx, self, mask, p, train)
  autograd: True
  forward_process_code: >
    auto mask = at::empty(self.sizes(), self.options().dtype(at::kByte));
  saved_data: [p, mask]
  forward_schema: "dropout__impl(Tensor(a!) self, Tensor(b!) mask, float p, bool train) -> Tensor(a!)"
  cal_grad_code: >
    auto p = p_.toDouble();
    double p1m = 1. - p;
    double scale = p1m == 0 ? 0. : 1. / p1m;
    auto mask = mask_.toTensor();
    at::Tensor out = grad_outputs[0] * mask * scale;
  backward_return_code: >
    std::vector<at::Tensor> outputs(6);
    outputs[0] = out;
    return outputs;
  wrappter_custom_return: return self;



- schema: "log.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiLog(ctx, out, self)

- schema: "log_(Tensor(a!) self) -> Tensor(a!)"
  interface: diopiLogInp(ctx, self)

- schema: "log2.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiLog2(ctx, out, self)

- schema: "log2_(Tensor(a!) self) -> Tensor(a!)"
  interface: diopiLog2Inp(ctx, self)

- schema: "abs(Tensor self) -> Tensor"
  custom_code_at_the_beginning: >
    auto out = at::empty_like(self);
  interface: diopiAbs(ctx, out, self)

- schema: "abs_(Tensor(a!) self) -> Tensor(a!)"
  interface: diopiAbsInp(ctx, self)

- schema: "abs.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiAbs(ctx, out, self)

- schema: "neg.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiNeg(ctx, out, self)

- schema: "neg_(Tensor(a!) self) -> Tensor(a!)"
  interface: diopiNegInp(ctx, self)

- schema: "sqrt.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiSqrt(ctx, out, self)

- schema: "sqrt_(Tensor(a!) self) -> Tensor(a!)"
  interface: diopiSqrtInp(ctx, self)

- schema: "all.out(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiAll(ctx, out, self, &dim)

- schema: "all.all_out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiAll(ctx, out, self, nullptr)

- schema: "any.all_out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiAny(ctx, out, self, nullptr)

- schema: "any.out(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiAny(ctx, out, self, &dim)

- schema: "topk.values(Tensor self, int k, int dim=-1, bool largest=True, bool sorted=True, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)"
  interface: diopiTopk(ctx, values, indices, self, k, dim, largest, sorted)

- schema: "topk(Tensor self, int k, int dim=-1, bool largest=True, bool sorted=True) -> (Tensor values, Tensor indices)"
  custom_code_at_the_beginning: >
    std::vector<int64_t> output_size(self.sizes().begin(), self.sizes().end());
    dim = dim < 0 ? (dim + output_size.size()) : dim;
    output_size[dim] = k;
    auto values = at::empty(output_size, self.options());
    auto indices = at::empty(output_size, self.options().dtype(at::kLong));
  interface: diopiTopk(ctx, values, indices, self, k, dim, largest, sorted)

- schema: "mean.out(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)"
  register_op: False
  print_func_call_info: True
  custom_code_at_the_beginning: >
    ::diopiSize_t diopi_size = toDiopiSize(dim);
  interface: diopiMean(ctx, out, self, diopi_size);

- schema: "linear_backward(Tensor input, Tensor grad_output, Tensor weight, bool[3] output_mask) -> (Tensor grad_input, Tensor grad_weight, Tensor grad_bias)"
  register_op: False
  custom_code_at_the_beginning: >
    auto grad_input = at::empty(input.sizes(), input.options());
    auto grad_weight = at::empty(weight.sizes(), weight.options().dtype(at::kFloat));
    at::Tensor grad_bias;
    bool bias_has_value = output_mask[2];
    if (bias_has_value) {
      grad_bias = at::empty({grad_output.size(1)}, grad_output.options());
    }
  interface: diopiLinearBackward(ctx, grad_input, grad_weight, grad_bias, grad_output, input, weight)

- schema: "linear(Tensor input, Tensor weight, Tensor? bias=None) -> Tensor"
  register_op: True
  autograd: True
  custom_code_at_the_beginning: >
    std::vector<int64_t> output_size(input.sizes().begin(), input.sizes().end());
    output_size.back() = weight.sizes()[0];
    auto out = at::empty(output_size, input.options());
  interface: diopiLinear(ctx, out, input, weight, bias)
  forward_process_code: >
    bool bias_has_value = (bias.has_value() == true) ? bias.value().requires_grad() : false;
    std::array<bool, 3> output_mask{input.requires_grad(), weight.requires_grad(), bias_has_value};
  saved_data: [output_mask, input, weight, bias]
  cal_grad_code: >
    auto output_mask = output_mask_.to<std::array<bool, 3>>();
    auto input = input_.toTensor();
    auto weight = weight_.toTensor();
    auto bias = bias_.toTensor();
    auto grad_output = grad_outputs[0];
  backward_schema: "linear_backward(Tensor input, Tensor grad_output, Tensor weight, bool[3] output_mask) -> (Tensor grad_input, Tensor grad_weight, Tensor grad_bias)"
  backward_return_code: >
    return {std::get<0>(result), std::get<1>(result), std::get<2>(result)};
- schema: "_log_softmax_backward_data.out(Tensor grad_output, Tensor output, int dim, ScalarType input_dtype, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiLogSoftmaxBackward(ctx, out, grad_output, output, dim)

- schema: "_log_softmax.out(Tensor self, int dim, bool half_to_float, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiLogSoftmax(ctx, out, self, dim)

- schema: "aten::log_softmax.int_out(Tensor self, int dim, ScalarType? dtype=None, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiLogSoftmax(ctx, out, self, dim)

- schema: "max_pool2d_with_indices.out(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False, *, Tensor(a!) out, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))"
  size_attr: [kernel_size, stride, padding, dilation]
  interface: diopiMaxPool2dWithIndices(&context, out, indices, self, kernel_size, stride, padding, dilation, ceil_mode)

- schema: max_pool2d_with_indices_backward.grad_input(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] stride, int[2] padding, int[2] dilation, bool ceil_mode, Tensor indices, *, Tensor(a!) grad_input) -> Tensor(a!)
  size_attr: [kernel_size, stride, padding, dilation]
  interface: diopiMaxPool2dBackward(ctx, grad_input, grad_output, self, kernel_size, stride, padding, dilation, ceil_mode, indices)

- schema: nll_loss.out(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, SymInt ignore_index=-100, *, Tensor(a!) out) -> Tensor(a!)
  interface: diopiNLLLoss(ctx, out, self, target, weight, static_cast<diopiReduction_t>(reduction), ignore_index.expect_int())

- schema: nll_loss_forward.output(Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, *, Tensor(a!) output, Tensor(b!) total_weight) -> (Tensor(a!), Tensor(b!))
  interface: diopiNLLLoss(ctx, output, self, target, weight, static_cast<diopiReduction_t>(reduction), ignore_index.expect_int())

- schema: nll_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, Tensor total_weight, *, Tensor(a!) grad_input) -> Tensor(a!)
  interface: diopiNLLLossBackward(&context, grad_input, grad_output, self, target, weight, static_cast<diopiReduction_t>(reduction), ignore_index.expect_int());

- schema: "threshold_backward.grad_input(Tensor grad_output, Tensor self, Scalar threshold, *, Tensor(a!) grad_input) -> Tensor(a!)"
  interface: diopiThresholdBackward(ctx, grad_input, grad_output, self, &threshold)

- schema: transpose_(Tensor(a!) self, int dim0, int dim1) -> Tensor(a!)
  register_op: False # TODO: have problem on camb
  custom_code_at_the_beginning: >
    std::vector<int64_t> output_size(self.sizes().cbegin(), self.sizes().cend());
    std::vector<int64_t> output_stride(self.strides().cbegin(), self.strides().cend());
    auto tmp1 = output_size.at(dim0);
    auto tmp2 = output_stride.at(dim0);
    output_size[dim0] = output_size.at(dim1);
    output_stride[dim0] = output_stride.at(dim1);
    output_size[dim1] = tmp1;
    output_stride[dim1] = tmp1;
    ::diopiConstTensorHandle_t self_ = dipu::diopi_helper::toDiopiTensorHandle(self);
    self.sizes() = output_size;
    self.strides() = output_stride;
  interface: diopiTranspose(ctx, self, self_, dim0, dim1)

- schema: "bitwise_and.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiBitwiseAnd(ctx, out, self, other)

- schema: "bitwise_and_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)"
  interface: diopiBitwiseAndInp(ctx, self, other)

- schema: "bitwise_and.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiBitwiseAndScalar(ctx, out, self, other)

- schema: "bitwise_and_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)"
  interface: diopiBitwiseAndInpScalar(ctx, self, other)

- schema: "bitwise_not.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiBitwiseNot(ctx, out, self)

- schema: "bitwise_not_(Tensor(a!) self) -> Tensor(a!)"
  interface: diopiBitwiseNotInp(ctx, self)

- schema: "stack(Tensor[] tensors, int dim=0) -> Tensor"
  custom_code_at_the_beginning: >
    auto num_tensors = tensors.size();
    auto shape = tensors[0].sizes();
    std::vector<long int> tmp;
    for (int i = 0; i < dim; i++) {
        tmp.push_back(shape[i]);
    }
    tmp.push_back(num_tensors);
    for (int i = dim; i < shape.size(); i++) {
        tmp.push_back(shape[i]);
    }
    const std::vector<long int>& const_tmp = tmp;
    shape = at::ArrayRef<long int>(const_tmp);
    auto out = at::empty({shape}, tensors[0].options());
    
    std::vector<diopiConstTensorHandle_t> diopiTensorHandles(tensors.size());
    for (size_t i = 0; i < tensors.size(); ++i) {
      diopiTensorHandles[i] = dipu::diopi_helper::toDiopiTensorHandle(tensors.at(i));
    }
  interface: diopiStack(ctx, out, diopiTensorHandles.data(), num_tensors, dim)

- schema: "stack.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -> Tensor(a!)"
  custom_code_at_the_beginning: >
    std::vector<diopiConstTensorHandle_t> diopiTensorHandles(tensors.size());
    for (size_t i = 0; i < tensors.size(); ++i) {
      diopiTensorHandles[i] = dipu::diopi_helper::toDiopiTensorHandle(tensors.at(i));
    }
  interface: diopiStack(ctx, out, diopiTensorHandles.data(), tensors.size(), dim)

- schema: "sort(Tensor self, int dim=-1, bool descending=False) -> (Tensor values, Tensor indices)"
  custom_code_at_the_beginning: >
    auto dim_ = dim < 0 ? (dim + self.sizes().size()) : dim;
    auto values = at::empty(self.sizes(), self.options().dtype(at::kLong));
    auto indices = at::empty(self.sizes(), self.options().dtype(at::kLong));
  interface: diopiSort(ctx, values, indices, self, dim_, descending, nullptr)

- schema: "sort.values(Tensor self, int dim=-1, bool descending=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)"
  custom_code_at_the_beginning: >
    auto dim_ = dim < 0 ? (dim + self.sizes().size()) : dim;
  interface: diopiSort(ctx, values, indices, self, dim_, descending, nullptr)

- schema: "sort.values_stable(Tensor self, *, bool? stable, int dim=-1, bool descending=False, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)"
  custom_code_at_the_beginning: >
    auto dim_ = dim < 0 ? (dim + self.sizes().size()) : dim;
    bool stable_ = stable.has_value() ? stable.value() : false;
    const bool *p = &stable_;
  interface: diopiSort(ctx, values, indices, self, dim_, descending, p)

- schema: "rsqrt.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiRsqrt(ctx, out, self)

- schema: "uniform_(Tensor(a!) self, float from=0, float to=1, *, Generator? generator=None) -> Tensor(a!)"
  custom_code_at_the_beginning: >
    const int64_t seed = (generator.has_value() && generator.value().defined()) ? generator.value().seed() : 0;
  interface: diopiUniformInp(ctx, self, from, to, seed)

- schema: "tril(Tensor self, int diagonal=0) -> Tensor"
  custom_code_at_the_beginning: >
    auto out = at::empty_like(self);
  interface: diopiTril(ctx, out, self, diagonal)

- schema: "tril.out(Tensor self, int diagonal=0, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiTril(ctx, out, self, diagonal)

- schema: "multinomial(Tensor self, int num_samples, bool replacement=False, *, Generator? generator=None) -> Tensor"
  custom_code_at_the_beginning: >
    auto out = at::empty_like(self);
    if (self.dim() == 2){
      out = at::empty({self.size(0), num_samples}, self.options().dtype(at::kLong));
    }
    else if (self.dim() == 1) {
      out = at::empty({num_samples,}, self.options().dtype(at::kLong));
    }
  interface: diopiMultinomial(ctx, out, self, num_samples, replacement)

- schema: "multinomial.out(Tensor self, int num_samples, bool replacement=False, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiMultinomial(ctx, out, self, num_samples, replacement)

- schema: "random_(Tensor(a!) self, *, Generator? generator=None) -> Tensor(a!)"
  print_func_call_info: True
  custom_code_at_the_beginning: >
    const int64_t seed = (generator.has_value() && generator.value().defined()) ? generator.value().seed() : 0;
  interface: diopiRandomInp(ctx, self, 0, nullptr, seed)

- schema: "random_.to(Tensor(a!) self, int to, *, Generator? generator=None) -> Tensor(a!)"
  print_func_call_info: True
  custom_code_at_the_beginning: >
    const int64_t seed = (generator.has_value() && generator.value().defined()) ? generator.value().seed() : 0;
  interface: diopiRandomInp(ctx, self, 0, &to, seed)

# random_.from(Tensor(a!) self, int from, int? to, *, Generator? generator=None) -> Tensor(a!)

- schema: "matmul(Tensor self, Tensor other) -> Tensor"
  print_func_call_info: True
  custom_code_at_the_beginning: >
    auto out = at::empty_like(self);
  interface: diopiMatmul(ctx, out, self, other)

- schema: "matmul.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  print_func_call_info: True
  register_op: False
  custom_code_at_the_beginning: >
    auto grad_input = at::empty(input.sizes(), input.options());
    auto grad_weight = at::empty(weight.sizes(), weight.options().dtype(at::kFloat));
    at::Tensor grad_bias;
    bool bias_has_value = output_mask[2];
    if (bias_has_value) {
      grad_bias = at::empty({grad_output.size(1)}, grad_output.options());
    }
  interface: diopiLinearBackward(ctx, grad_input, grad_weight, grad_bias, grad_output, input, weight)

- schema: "linear(Tensor input, Tensor weight, Tensor? bias=None) -> Tensor"
  register_op: True
  autograd: True
  custom_code_at_the_beginning: >
    std::vector<int64_t> output_size(input.sizes().begin(), input.sizes().end());
    output_size.back() = weight.sizes()[0];
    auto out = at::empty(output_size, input.options());
  interface: diopiLinear(ctx, out, input, weight, bias)
  forward_process_code: >
    bool bias_has_value = (bias.has_value() == true) ? bias.value().requires_grad() : false;
    std::array<bool, 3> output_mask{input.requires_grad(), weight.requires_grad(), bias_has_value};
  saved_data: [output_mask, input, weight, bias]
  cal_grad_code: >
    auto output_mask = output_mask_.to<std::array<bool, 3>>();
    auto input = input_.toTensor();
    auto weight = weight_.toTensor();
    auto bias = bias_.toTensor();
    auto grad_output = grad_outputs[0];
  backward_schema: "linear_backward(Tensor input, Tensor grad_output, Tensor weight, bool[3] output_mask) -> (Tensor grad_input, Tensor grad_weight, Tensor grad_bias)"
  backward_return_code: >
    return {std::get<0>(result), std::get<1>(result), std::get<2>(result)};
- schema: "_log_softmax_backward_data.out(Tensor grad_output, Tensor output, int dim, ScalarType input_dtype, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiLogSoftmaxBackward(ctx, out, grad_output, output, dim)

- schema: "_log_softmax.out(Tensor self, int dim, bool half_to_float, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiLogSoftmax(ctx, out, self, dim)

- schema: "aten::log_softmax.int_out(Tensor self, int dim, ScalarType? dtype=None, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiLogSoftmax(ctx, out, self, dim)

- schema: "max_pool2d_with_indices.out(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False, *, Tensor(a!) out, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))"
  size_attr: [kernel_size, stride, padding, dilation]
  interface: diopiMaxPool2dWithIndices(&context, out, indices, self, kernel_size, stride, padding, dilation, ceil_mode)

- schema: max_pool2d_with_indices_backward.grad_input(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] stride, int[2] padding, int[2] dilation, bool ceil_mode, Tensor indices, *, Tensor(a!) grad_input) -> Tensor(a!)
  size_attr: [kernel_size, stride, padding, dilation]
  interface: diopiMaxPool2dBackward(ctx, grad_input, grad_output, self, kernel_size, stride, padding, dilation, ceil_mode, indices)

- schema: nll_loss.out(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, SymInt ignore_index=-100, *, Tensor(a!) out) -> Tensor(a!)
  interface: diopiNLLLoss(ctx, out, self, target, weight, static_cast<diopiReduction_t>(reduction), ignore_index.expect_int())

- schema: nll_loss_forward.output(Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, *, Tensor(a!) output, Tensor(b!) total_weight) -> (Tensor(a!), Tensor(b!))
  interface: diopiNLLLoss(ctx, output, self, target, weight, static_cast<diopiReduction_t>(reduction), ignore_index.expect_int())

- schema: nll_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, Tensor total_weight, *, Tensor(a!) grad_input) -> Tensor(a!)
  interface: diopiNLLLossBackward(&context, grad_input, grad_output, self, target, weight, static_cast<diopiReduction_t>(reduction), ignore_index.expect_int());

- schema: "threshold_backward.grad_input(Tensor grad_output, Tensor self, Scalar threshold, *, Tensor(a!) grad_input) -> Tensor(a!)"
  interface: diopiThresholdBackward(ctx, grad_input, grad_output, self, &threshold)

- schema: transpose_(Tensor(a!) self, int dim0, int dim1) -> Tensor(a!)
  register_op: False # TODO: have problem on camb
  custom_code_at_the_beginning: >
    std::vector<int64_t> output_size(self.sizes().cbegin(), self.sizes().cend());
    std::vector<int64_t> output_stride(self.strides().cbegin(), self.strides().cend());
    auto tmp1 = output_size.at(dim0);
    auto tmp2 = output_stride.at(dim0);
    output_size[dim0] = output_size.at(dim1);
    output_stride[dim0] = output_stride.at(dim1);
    output_size[dim1] = tmp1;
    output_stride[dim1] = tmp1;
    ::diopiConstTensorHandle_t self_ = dipu::diopi_helper::toDiopiTensorHandle(self);
    self.sizes() = output_size;
    self.strides() = output_stride;
  interface: diopiTranspose(ctx, self, self_, dim0, dim1)

- schema: "bitwise_and.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiBitwiseAnd(ctx, out, self, other)

- schema: "bitwise_and_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)"
  interface: diopiBitwiseAndInp(ctx, self, other)

- schema: "bitwise_and.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiBitwiseAndScalar(ctx, out, self, other)

- schema: "bitwise_and_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)"
  interface: diopiBitwiseAndInpScalar(ctx, self, other)

- schema: "bitwise_not.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiBitwiseNot(ctx, out, self)

- schema: "bitwise_not_(Tensor(a!) self) -> Tensor(a!)"
  interface: diopiBitwiseNotInp(ctx, self)

- schema: "stack(Tensor[] tensors, int dim=0) -> Tensor"
  custom_code_at_the_beginning: >
    auto num_tensors = tensors.size();
    auto shape = tensors[0].sizes();
    std::vector<long int> tmp;
    for (int i = 0; i < dim; i++) {
        tmp.push_back(shape[i]);
    }
    tmp.push_back(num_tensors);
    for (int i = dim; i < shape.size(); i++) {
        tmp.push_back(shape[i]);
    }
    const std::vector<long int>& const_tmp = tmp;
    shape = at::ArrayRef<long int>(const_tmp);
    auto out = at::empty({shape}, tensors[0].options());
    
    std::vector<diopiConstTensorHandle_t> diopiTensorHandles(tensors.size());
    for (size_t i = 0; i < tensors.size(); ++i) {
      diopiTensorHandles[i] = dipu::diopi_helper::toDiopiTensorHandle(tensors.at(i));
    }
  interface: diopiStack(ctx, out, diopiTensorHandles.data(), num_tensors, dim)

- schema: "stack.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -> Tensor(a!)"
  custom_code_at_the_beginning: >
    std::vector<diopiConstTensorHandle_t> diopiTensorHandles(tensors.size());
    for (size_t i = 0; i < tensors.size(); ++i) {
      diopiTensorHandles[i] = dipu::diopi_helper::toDiopiTensorHandle(tensors.at(i));
    }
  interface: diopiStack(ctx, out, diopiTensorHandles.data(), tensors.size(), dim)

- schema: "sort(Tensor self, int dim=-1, bool descending=False) -> (Tensor values, Tensor indices)"
  custom_code_at_the_beginning: >
    auto dim_ = dim < 0 ? (dim + self.sizes().size()) : dim;
    auto values = at::empty(self.sizes(), self.options().dtype(at::kLong));
    auto indices = at::empty(self.sizes(), self.options().dtype(at::kLong));
  interface: diopiSort(ctx, values, indices, self, dim_, descending, nullptr)

- schema: "sort.values(Tensor self, int dim=-1, bool descending=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)"
  custom_code_at_the_beginning: >
    auto dim_ = dim < 0 ? (dim + self.sizes().size()) : dim;
  interface: diopiSort(ctx, values, indices, self, dim_, descending, nullptr)

- schema: "sort.values_stable(Tensor self, *, bool? stable, int dim=-1, bool descending=False, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)"
  custom_code_at_the_beginning: >
    auto dim_ = dim < 0 ? (dim + self.sizes().size()) : dim;
    bool stable_ = stable.has_value() ? stable.value() : false;
    const bool *p = &stable_;
  interface: diopiSort(ctx, values, indices, self, dim_, descending, p)

- schema: "rsqrt.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiRsqrt(ctx, out, self)

- schema: "uniform_(Tensor(a!) self, float from=0, float to=1, *, Generator? generator=None) -> Tensor(a!)"
  custom_code_at_the_beginning: >
    const int64_t seed = (generator.has_value() && generator.value().defined()) ? generator.value().seed() : 0;
  interface: diopiUniformInp(ctx, self, from, to, seed)

- schema: "tril(Tensor self, int diagonal=0) -> Tensor"
  custom_code_at_the_beginning: >
    auto out = at::empty_like(self);
  interface: diopiTril(ctx, out, self, diagonal)

- schema: "tril.out(Tensor self, int diagonal=0, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiTril(ctx, out, self, diagonal)

- schema: "multinomial(Tensor self, int num_samples, bool replacement=False, *, Generator? generator=None) -> Tensor"
  custom_code_at_the_beginning: >
    auto out = at::empty_like(self);
    if (self.dim() == 2){
      out = at::empty({self.size(0), num_samples}, self.options().dtype(at::kLong));
    }
    else if (self.dim() == 1) {
      out = at::empty({num_samples,}, self.options().dtype(at::kLong));
    }
  interface: diopiMultinomial(ctx, out, self, num_samples, replacement)

- schema: "multinomial.out(Tensor self, int num_samples, bool replacement=False, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiMultinomial(ctx, out, self, num_samples, replacement)

- schema: "matmul(Tensor self, Tensor other) -> Tensor"
  custom_code_at_the_beginning: >
    auto out = at::empty_like(self);
  interface: diopiMatmul(ctx, out, self, other)

- schema: "matmul.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)"
  interface: diopiMatmul(ctx, out, self, other)
