cuda:
    # alpaca-lora
    # - model_cfg: "alpaca-lora run_llama_finetune.py workdirs_alpaca_lora_llama_finetune"
    # transformers
    - model_cfg: "transformers examples/pytorch/language-modeling/llama_7b_infer.py workdirs_transformers_llama_infer"
    - model_cfg: "transformers examples/pytorch/language-modeling/internlm_7b_infer.py workdirs_transformers_internlm_infer"
    # lightllm lightllm's inference service is temporarily unable to properly recycle resources, skipping ci testing for now
    # - model_cfg: "lightllm llama_7b_via_lightllm_infer.py workdirs_lightllm_llama_infer"


camb:
    # alpaca-lora
    # - model_cfg: "alpaca-lora run_llama_finetune.py workdirs_alpaca_lora_llama_finetune"
    # transformers
    - model_cfg: "transformers examples/pytorch/language-modeling/llama_7b_infer.py workdirs_transformers_llama_infer"
    # - model_cfg: "transformers examples/pytorch/language-modeling/internlm_7b_infer.py workdirs_transformers_internlm_infer"


ascend:
    # # alpaca-lora
    # - model_cfg: "alpaca-lora run_llama_finetune.py workdirs_alpaca_lora_llama_finetune"
    # # transformers
    - model_cfg: "transformers examples/pytorch/language-modeling/llama_7b_infer.py workdirs_transformers_llama_infer"
